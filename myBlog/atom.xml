<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SSRVodka&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/516ce0f042c3ff5641e94abb796951aa</icon>
  <subtitle>It&#39;s better to burn out than to fade away.</subtitle>
  <link href="https://blog.sjtuxhw.top/atom.xml" rel="self"/>
  
  <link href="https://blog.sjtuxhw.top/"/>
  <updated>2026-01-17T10:54:07.768Z</updated>
  <id>https://blog.sjtuxhw.top/</id>
  
  <author>
    <name>SSRVodka</name>
    <email>xhwpro@gmail.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>简单的 OS 性能分析速查（运维场景）</title>
    <link href="https://blog.sjtuxhw.top/technical/simple-os-profiling/"/>
    <id>https://blog.sjtuxhw.top/technical/simple-os-profiling/</id>
    <published>2026-01-08T03:16:24.000Z</published>
    <updated>2026-01-17T10:54:07.768Z</updated>
    
    <content type="html"><![CDATA[<p>预备知识库：计算机组成原理（如内存 Hierarchy、CPU 基本组成），操作系统原理（kernel/user space、资源抽象、进程状态抽象等）。</p><p>以下是很零碎、经验化的思路，总结下来方便回顾。</p><h2 id="1-CPU-分析"><a href="#1-CPU-分析" class="headerlink" title="1. CPU 分析"></a>1. CPU 分析</h2><p>业界主流的对于 CPU 忙碌情况的指标是 “<strong>Load Average</strong>”，定义为 <strong><u>处于可运行状态（running）和不可中断睡眠（uninterruptible sleep，通常在等待 I/O）的进程数平均值</u></strong>。相关知识参见 <a href="#Appendix II. Review: About Process States">附录 II</a>。</p><p>Linux 上使用 <code>top</code> 类指令查看到的 load average 通常有 3 个数字，分别代表在过去 1 分钟、5 分钟、15 分钟内的 load average。</p><p>例：<code>load average: 2.35, 1.87, 1.25</code> 表示 1 分钟平均有 2.35 个进程在等待 CPU 或 I/O 资源（粗略认为）。</p><p>一般情况正常工作的理想状况 load average 约等于 CPU 核心数。load average 两倍于 CPU 核心数超过 5 分钟认为过载预警，4 倍于 CPU 核心数认为严重过载。</p><p>另一个指标是 CPU 利用率（CPU 时间分配比例）。也可通过 <code>top</code> 类指令观察到：</p><ul><li><code>%usr</code>（<code>us</code>）：用户进程占用 CPU 时间百分比（应用程序）</li><li><code>%nice</code>（<code>ni</code>）：手动调整过优先级的用户进程占 CPU 时间百分比</li><li><code>%sys</code>（<code>sy</code>）：内核进程占用 CPU 时间百分比（系统调用、中断处理）</li><li><code>%idle</code>（<code>id</code>）：CPU 空闲时间百分比</li><li><code>%iowait</code>（<code>wa</code>）：CPU 等待 I/O 完成的时间百分比（<strong>不是 CPU 被占用，而是空闲但有未完成的 I/O 请求</strong>）</li><li><code>%softit / %hardit</code>（<code>si / hi</code>）：处理软中断/硬中断的 CPU 时间占比</li><li><code>%steal</code>（<code>st</code>）：虚拟机等待物理 CPU 的时间百分比（仅虚拟化环境）</li></ul><p>此外 <code>vmstat</code> 指令能提供更多全局数据，如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vmstat 1</span></span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 8  0      0 123456  78901 234567    0    0     0     0  123 4567 85 10  5  0  0</span><br></pre></td></tr></table></figure><ul><li><strong>r (running)</strong>：等待 CPU 的进程数，<strong>持续 &gt;&gt; CPU 核心数</strong>表示 CPU 瓶颈</li><li><strong>b (blocked)</strong>：等待 I/O 的进程数，<strong>持续 &gt; 0</strong> 可能存在 I/O 瓶颈</li><li><strong>us + sy</strong>：<strong>持续 &gt; 90%</strong> 表示 CPU 资源紧张</li><li><strong>cs (context switch)</strong>：context switch 次数，<strong>&gt; 10 万/秒</strong> 可能存在资源竞争问题</li></ul><hr><p>排除 CPU 瓶颈的手段：</p><ol><li><p>确认 CPU 大致状况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">top -H  # -H 线程级别统计</span><br><span class="line">htop</span><br><span class="line">vmstat 1  # interval 1s, procs/memory/swap/io/cpu 统计</span><br></pre></td></tr></table></figure></li><li><p>确认哪个进程、哪些 CPU：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpstat -P ALL 1  # 按 CPU 核心统计</span><br><span class="line">pidstat -u 1     # -u 按进程 CPU 使用</span><br></pre></td></tr></table></figure></li><li><p>追踪进程内的瓶颈，需使用 <code>perf</code> 生成 flame graph；</p></li><li><p>想知道某些进程的 CPU 亲和性，或者处于性能测试的原因想将进程绑定到特定 CPU 上，可以使用 <code>taskset -c 0-3 ./app</code> 这样的方法；</p></li><li><p>想手动调整特定进程的优先级，可以使用这样的指令来告诉调度器：<code>nice -n -20 ./critical_app</code>（nice 值越小意味着越倾向于让出 CPU 资源，因此优先级越高，范围 -20 到 19，默认 0）；</p></li></ol><h2 id="2-内存分析"><a href="#2-内存分析" class="headerlink" title="2. 内存分析"></a>2. 内存分析</h2><p>内存是否到达“极限”的核心指标：<strong>可用内存</strong>（available）和 <strong>swap 使用率</strong>。</p><p>一般可以通过 <code>free</code> 指令查看全局的粗略信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">free -h</span></span><br><span class="line">              total    used    free  shared  buff/cache   available</span><br><span class="line">Mem:           62Gi    15Gi   1.2Gi   1.2Gi        45Gi        45Gi</span><br><span class="line">Swap:          16Gi   166Mi    16Gi</span><br></pre></td></tr></table></figure><p>注：<code>shared</code> 表示 tmpfs 等共享内存使用量</p><blockquote><p>[!IMPORTANT]</p><p>注意可用内存：<strong><u><code>avail = free + buff/cache - reclaimable</code></u></strong>。</p><p>因此在判断内存瓶颈时，<strong>不能只看 <code>free</code> 命令的 free 列</strong>，因为 Linux 会充分利用空闲内存作为缓存。</p></blockquote><p>除了 <code>free</code>，<code>vmstat</code> 也能查看内存相关稍详细一点的全局信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vmstat 1</span></span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 1  0 166400 123456  78901 234567    0    0    12    34   56   78 10  5 85  0  0</span><br></pre></td></tr></table></figure><ul><li><strong>swpd</strong>：已使用的交换分区大小（KB）</li><li><strong>si</strong>（swap in）：每秒从交换分区读入内存的数据量（KB/s）</li><li><strong>so</strong>（swap out）：每秒从内存写入交换分区的数据量（KB/s）</li><li><strong>in</strong>（interrupt）：每秒发生的中断总数（page fault 中断包含其中）</li><li><strong>关键阈值</strong>：<strong>si + so &gt; 10MB/s 持续 5 秒以上</strong>，表示存在严重的 thrashing，working set 大于现有内存资源。</li></ul><p>如果想了解更详细关于 page fault 的情况来掌握内存使用情况，不止需要了解 swap，还需要了解 page fault 频率等等。</p><p>通过 <code>sar</code> 指令查看 page 的交换和缺页异常的情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sar -W 1</span></span><br><span class="line">Linux 5.4.0-91-generic (server)     01/17/2026     _x86_64_    (8 CPU)</span><br><span class="line"></span><br><span class="line">02:30:01 PM  pswpin/s pswpout/s</span><br><span class="line">02:30:02 PM      0.00      0.00</span><br><span class="line"></span><br><span class="line">02:30:01 PM pgpgin/s pgpgout/s   fault/s  majflt/s</span><br><span class="line">02:30:02 PM    123.00    456.00   789.00      0.00</span><br></pre></td></tr></table></figure><ul><li><strong>pswpin/s, pswpout/s</strong>：每秒交换入/出的页数</li><li><strong>fault/s</strong>：每秒 Page Fault 总数</li><li><strong>majflt/s</strong>：每秒 Major Page Fault 数量</li></ul><p><img src="imgs/pf.png" width="450px" /></p><hr><p>排除内存瓶颈的手段：</p><ol><li><p>使用 <code>top</code> / <code>free</code> / <code>vmstat</code> / <code>sar</code> 类型的指令查看全局的内存主存、swap 使用情况，包括占用/增速；</p></li><li><p>定位哪个进程正在造成这个问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">按 RSS(Resident Set Size) 排序</span></span><br><span class="line">ps -eo pid,user,rss,cmd --sort=-rss | head -10</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-r 显示内存统计信息</span></span><br><span class="line">pidstat -r 1</span><br></pre></td></tr></table></figure></li><li><p>针对这个进程了解内存使用情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">监控特定进程的内存增长</span></span><br><span class="line">watch -n 1 &quot;ps -p &lt;pid&gt; -o rss,vsz,cmd&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看进程的详细内存映射</span></span><br><span class="line">cat /proc/&lt;pid&gt;/smaps | less</span><br></pre></td></tr></table></figure></li><li><p>更细粒度（进程甚至线程内部）的判断则需应用相关的调优手段。例如对 Java 应用可以通过 JVM 提供的 perf 工具（如 <code>jstat/jstack/jmap</code> 等）。</p></li></ol><blockquote><p>[!NOTE]</p><p>值得关注的是，在粒度细化到针对特定进程时，我们还有一些指标来衡量内存使用情况（下面表格为 LLM 生成内容）：</p><div class="table-container"><table><thead><tr><th><strong>内存指标</strong></th><th><strong>定义</strong></th><th><strong>是否包含共享内存</strong></th><th><strong>是否包含 Swap</strong></th><th><strong>使用场景</strong></th></tr></thead><tbody><tr><td><strong>RSS</strong> (Resident Set Size)</td><td>驻留在物理内存中的进程内存大小</td><td>✅ 完整计入</td><td>❌ 仅物理内存</td><td>快速查看进程内存占用</td></tr><tr><td><strong>VSZ</strong> (Virtual Memory Size)</td><td>进程虚拟地址空间总大小</td><td>✅ 完整计入</td><td>✅ 包含</td><td>了解进程地址空间需求</td></tr><tr><td><strong>PSS</strong> (Proportional Set Size)</td><td>RSS + 按比例分摊的共享内存</td><td>✅ 按进程数均摊</td><td>❌ 仅物理内存</td><td><strong>精确计算进程真实内存开销</strong></td></tr><tr><td><strong>USS</strong> (Unique Set Size)</td><td>进程独占的物理内存大小</td><td>❌ 仅独占部分</td><td>❌ 仅物理内存</td><td>分析内存泄漏、优化独占内存</td></tr></tbody></table></div></blockquote><h2 id="3-磁盘-I-O"><a href="#3-磁盘-I-O" class="headerlink" title="3. 磁盘 I/O"></a>3. 磁盘 I/O</h2><p>硬盘是否到达“极限”的指标：<strong><u>IOPS</u></strong> 和 <strong><u>吞吐量</u></strong>；</p><ul><li><strong>IOPS 极限</strong>：每秒能处理的请求数（一般小文件随机读写瓶颈）；</li><li><strong>Throughput 极限</strong>：每秒能传输的数据量（一般大文件顺序读写瓶颈）；</li></ul><blockquote><p>[!IMPORTANT]</p><p>在判断一个硬盘到达极限时应该遵循指标分析，方便后续优化工作。</p><p>另外 IOPS 还受制于 CPU 性能、Throughput 还受制于内存性能（看具体场景）。</p><p>这套指标不仅适用于存储 I/O，还适用于网络 I/O。</p></blockquote><p>使用 <code>iostat -x [interval]</code> 观察硬盘的指标（<code>-x</code> 打印 extended 信息，<code>-z</code> 不打印为 0 的数据）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Device      r/s     w/s   rkB/s    wkB/s  %util</span><br><span class="line">nvme0n1   48000   12000  192000   48000   85.00</span><br></pre></td></tr></table></figure><p><code>r/s</code>（读 IOPS）、<code>w/s</code>（写 IOPS）之和是 IOPS 数据；</p><p><code>rkB/s</code>（读吞吐量）、<code>wkB/s</code>（写吞吐量）之和是 Throughput 数据；</p><hr><ul><li><p><code>iostat</code> 的 <code>%util</code> 指标无法直接判断硬盘 I/O 极限。因为 <code>%util</code> 的定义是 “磁盘忙碌时间占比”。</p><ul><li><p>例：该磁盘 1 秒内 0.98 秒在处理请求，则当前 <code>%util</code> 值为 98%；</p></li><li><p>对机械硬盘（HDD）：每次只能处理一个请求（磁头只有一个），<code>%util</code> 为 100% 意味着满负荷（即到达 I/O 极限）；</p></li><li><p>对现代 SSD / NVMe：多 channels 设计，可并行处理上百请求，<code>%util</code> 为 100% 代表每时每刻都不是完全空闲的；</p><blockquote><p>一个 NVMe 盘能同时处理 128 个请求，现在有 10 个请求一直在队列中处理，<code>%util</code> 为 100%，但是 90% I/O 能力 not fully utilized；</p></blockquote></li></ul></li><li><p><code>iostat</code> 的 <code>await</code> 指标能间接判断硬盘的 I/O 极限。因为 <code>await</code> 的定义是请求从发出到完成的平均时间（即平均周转时间 ATT），单位毫秒；</p><p><strong><u><code>await</code> 异常升高（相对于同型号正常值）证明当前 workload 接近 I/O 极限</u></strong>。注意不同磁盘的 baseline（正常值基准）不一样。</p><p>可以使用 <code>fio</code> 工具测试磁盘的 IOPS /  吞吐量极限：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">测随机读 IOPS</span></span><br><span class="line">fio --name=randread --ioengine=libaio --direct=1 --bs=4k \</span><br><span class="line">    --iodepth=64 --rw=randread --size=1G --runtime=30</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">测顺序写吞吐量</span></span><br><span class="line">fio --name=seqwrite --ioengine=libaio --direct=1 --bs=1m \</span><br><span class="line">    --iodepth=32 --rw=write --size=1G --runtime=30</span><br></pre></td></tr></table></figure></li></ul><hr><p>排除硬盘压力/瓶颈的手段：</p><ol><li><p>确认是否是 I/O 问题：<code>top</code> 选项的 <code>wa%</code>（<strong><u>iowait percentage</u></strong>）指标：15%-20% 记为高占用。</p></li><li><p>定位硬盘的具体问题，哪块设备、什么瓶颈：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iostat -x 1</span><br></pre></td></tr></table></figure></li><li><p>哪个进程正在造成这个问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">通过检查 kernel 提供的 /proc 中的 I/O 统计信息</span></span><br><span class="line">sudo iotop -oP    # -o 仅正在执行 I/O 操作的进程；-P 仅进程（不包括默认显示的线程）</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sysstat 工具包一部分</span></span><br><span class="line">pidstat -d 1    # -d I/O 统计信息，interval 1s 采样间隔</span><br></pre></td></tr></table></figure></li><li><p>这个进程正在读写什么文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lsof -p &lt;pid&gt;</span><br><span class="line">strace -p &lt;pid&gt; -e trace=read,write,open -f</span><br></pre></td></tr></table></figure></li></ol><h2 id="Appendix-I-场景实战"><a href="#Appendix-I-场景实战" class="headerlink" title="Appendix I. 场景实战"></a>Appendix I. 场景实战</h2><h3 id="Scene-A-不可中断睡眠"><a href="#Scene-A-不可中断睡眠" class="headerlink" title="Scene A. 不可中断睡眠"></a>Scene A. 不可中断睡眠</h3><p>关于是否可中断睡眠的辨析：</p><ul><li>块设备操作，如磁盘读写操作涉及复杂的硬件交互，必须保证操作的完整性，所以进程操作时一定位于不可中断睡眠状态；<ul><li>错误。D/S 区分与“是不是磁盘 I/O” 无关。有些 I/O 操作需要 D 状态是因为 Kernel 打断后没法恢复状态/ roll back，与硬件性质也无关。</li></ul></li><li>观察到与 NFS 相关的大量进程长期进入 D 状态 是没关系的，因为 NFS 锁需要不可中断睡眠来保证操作完整性。<ul><li>半对，应该检查具体原因。NFS 历史上会导致很多 D 状态是设计缺陷，现在的 kernel 可以尽量避免出现 D 状态；</li></ul></li><li>使用不可中断睡眠的其中一个原因就是，kernel 在执行 nested signal handlers 时会出问题。<ul><li>错误。不可中断睡眠与内核处理嵌套信号句柄无关。本质上就是 Signals 默认会强制退出进程睡眠状态，如果 kernel 不能在此时 abort 操作（回到用户态为时尚早），那么使用 S 就是不安全的，这个进程需要 D 状态。</li></ul></li></ul><h3 id="Scene-B-CPU-瓶颈"><a href="#Scene-B-CPU-瓶颈" class="headerlink" title="Scene B. CPU 瓶颈"></a>Scene B. CPU 瓶颈</h3><p>下面的情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">uptime</span></span></span><br><span class="line"> 14:30:01 up 10 days,  3:22,  2 users,  load average: 12.45, 8.76, 4.32</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">mpstat -P ALL 1</span></span><br><span class="line">Linux 5.4.0-91-generic (server)   01/17/2026  _x86_64_  (8 CPU)</span><br><span class="line">02:30:02 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">02:30:03 PM  all   85.21    0.00    9.87    0.00    0.00    4.92    0.00    0.00    0.00    0.00</span><br><span class="line">02:30:03 PM    0   99.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">02:30:03 PM    1   98.00    0.00    2.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>注意到：</p><ul><li>这是一个 8 核机器，一分钟内 load average 说明存在 CPU 资源压力；</li><li><code>%usr = 85.21%</code> 表明是<strong>用户进程</strong>导致的 CPU 压力；</li><li>接下来需要使用 <code>pidstat -u 1</code> 或其他指令查看具体进程使用情况；</li></ul><h3 id="Scene-C-I-O-等待会影响-Load-Average"><a href="#Scene-C-I-O-等待会影响-Load-Average" class="headerlink" title="Scene C. I/O 等待会影响 Load Average"></a>Scene C. I/O 等待会影响 Load Average</h3><p>下面的场景：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vmstat 1</span></span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 0  5      0  45678  12345 678901    0    0  2345  6789  100  200  5 10  0 85  0</span><br></pre></td></tr></table></figure><p>看起来 <code>id=0%</code> 是 CPU 资源受限吗？并非，<strong><code>wa=85%</code></strong> 说明 CPU 有 85% 时间在等待 I/O 完成，很可能是 I/O 瓶颈。</p><p>注意 <strong><code>bi=2345</code>、<code>bo=6789</code></strong>：块设备每秒读写次数高，需要结合 <code>iostat</code> 进一步分析 I/O 瓶颈问题。</p><h3 id="Scene-D-内存是否存在压力"><a href="#Scene-D-内存是否存在压力" class="headerlink" title="Scene D. 内存是否存在压力"></a>Scene D. 内存是否存在压力</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">free -h</span></span><br><span class="line">              total    used    free  shared  buff/cache   available</span><br><span class="line">Mem:           62Gi    58Gi   1.2Gi   1.5Gi        3.2Gi        2.1Gi</span><br><span class="line">Swap:          16Gi    12Gi    4.0Gi</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vmstat 1</span></span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 4  2 12582912 123456  78901 234567  8192 12288   123   456  789 1011 25 15 40 20  0</span><br></pre></td></tr></table></figure><p><strong>available 仅 2.1Gi（总内存 62Gi 的 3.4%）</strong>，远低于 10% 的健康阈值，存在内存问题。</p><p><code>wa=20%</code> 说明 CPU 有 20% 时间在等待 I/O 完成，但另外的数据证明了并非用户态程序的 I/O：<strong><code>si=8192KB/s</code>, <code>so=12288KB/s</code></strong>，持续的高 swap I/O 活动证明可能存在大规模的 Thrashing 现象，说明系统存在严重物理内存短缺问题，需要立即定位进程并查找原因：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pidstat -r 1</span></span><br><span class="line">Linux 5.4.0-91-generic (server)     01/17/2026     _x86_64_    (8 CPU)</span><br><span class="line"></span><br><span class="line">02:35:01 PM   UID       PID  minflt/s  majflt/s     VSZ    RSS   %MEM  Command</span><br><span class="line">02:35:02 PM  1000      1234    150.00      5.00 4500000 3200000   5.00  java</span><br><span class="line">02:35:02 PM  1000      5678    200.00     15.00 2000000 1800000   2.80  python</span><br><span class="line">02:35:02 PM     0       999     50.00     20.00 1500000 1200000   1.90  mysqld</span><br></pre></td></tr></table></figure><p>发现 PID 为 1234 的 Java 应用存在持续的 major page fault，应使用 JVM 相关工具对业务逻辑进行进一步排查。</p><h3 id="Scene-E-磁盘-I-O-检查"><a href="#Scene-E-磁盘-I-O-检查" class="headerlink" title="Scene E. 磁盘 I/O 检查"></a>Scene E. 磁盘 I/O 检查</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">iostat -x 1</span></span><br><span class="line">Device      r/s     w/s  r_await w_await  aqu-sz  %util</span><br><span class="line">nvme0n1   15000    8000     0.08    0.12    1.84  99.80</span><br><span class="line">sda         200     150    45.32   62.18   16.43  98.50</span><br></pre></td></tr></table></figure><p>如上所示，可以发现：</p><ul><li><code>nvme0n1</code> 设备并不存在接近极限的问题：await 约 0.1 ms，队列长度 1.84；</li><li><code>sda</code> 设备达到极限：await 高达 45-62 ms，队列积压 16 个请求；</li></ul><h3 id="Scene-F-磁盘-I-O-瓶颈估算"><a href="#Scene-F-磁盘-I-O-瓶颈估算" class="headerlink" title="Scene F. 磁盘 I/O 瓶颈估算"></a>Scene F. 磁盘 I/O 瓶颈估算</h3><p>某 NVMe 硬盘标称 50 万 IOPS、3 GB/s 吞吐。估计：</p><ul><li><p>“数据库大量随机读写，4KB 小块” 时的场景：先达到 IOPS 极限（50w × 4KB = 2GB/s），此时吞吐量未达上限；</p></li><li><p>“视频转码，顺序读写超大视频文件” 时的场景：大概率先达到吞吐量上限（3 GB/s 时 IOPS 仅数千）；</p></li></ul><h2 id="Appendix-II-Review-About-Process-States"><a href="#Appendix-II-Review-About-Process-States" class="headerlink" title="Appendix II. Review: About Process States"></a>Appendix II. Review: About Process States</h2><p><strong>经典操作系统理论中的进程状态</strong>（Created/Ready/Running/Blocked/Terminated）是一个<strong>抽象模型</strong>，描述了进程生命周期的基本阶段。而 <strong>Linux 进程状态</strong>（如下）是<strong>具体工程实现</strong>，提供了更细致和实际的状态描述。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">S Interruptible sleep (waiting for an event to complete)</span><br><span class="line">D Uninterruptible sleep (usually IO)</span><br><span class="line">R Running or runnable (on run queue)</span><br><span class="line">T Stopped, either by a job control signal or because it is being traced.</span><br><span class="line">W paging (not valid since the 2.6.xx kernel)</span><br><span class="line">X dead (should never be seen)</span><br><span class="line">Z Defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent.</span><br></pre></td></tr></table></figure><p>二者映射关系如下：</p><ul><li><p><strong>Ready + Running -&gt; R</strong></p><ul><li>理论中的 Ready（就绪）和 Running（运行）状态在 Linux 中合并为 R 状态，这是因为在 Linux 调度器实现中，这两个状态在内核数据结构层面难以严格区分；</li><li>R 表示 “Running or runnable (on run queue)”，即要么正在 CPU 上执行，要么在 run queue 中等待 CPU；</li></ul></li><li><p><strong>Blocked -&gt; S + D</strong></p><ul><li><p>理论中的 Blocked（阻塞）状态在 Linux 中细化为两种：</p><ul><li><strong>S (Interruptible sleep)</strong>: 可中断睡眠，等待事件完成，可以被信号唤醒；</li><li><strong>D (Uninterruptible sleep)</strong>: 不可中断睡眠，通常是等待 I/O 操作完成，不能被 OS 的信号中断；</li></ul></li><li><p>这种细分反映了实际系统中阻塞的不同性质和处理需求。</p><p>区分的根本意义辨析：<strong>D state is not primarily about protecting “critical kernel processes.”</strong></p><p>It is about <strong>protecting specific *wait contexts*</strong> where interruption would cause:</p><ul><li>resource leaks;</li><li>inconsistent kernel state;</li><li>or impossible rollback;</li></ul><p>A task enters <strong>D</strong> when it sleeps <strong>while holding resources that cannot be safely released or unwound if a signal arrives</strong>.</p><p><strong><u>即：确保某些关键上下文不能被打断 (D)</u></strong>，同时顺便保证系统性能 (S)，与 I/O 或锁无关，只与 Kernel 有没有能力在 signal 打断后 context switch 原样恢复回去有关。举例场景：</p><ol><li><p>I/O Subsystem 的特殊性：</p><ul><li><p>虽然：Disk I/O is <strong>not inherently uninterruptible</strong>.</p></li><li><p>但是：Many I/O paths <em>used to</em> require D because they <strong>could not be safely rolled back</strong>, <strong><u>NOT</u></strong> because of “hardware complexity”.</p></li></ul></li><li><p>Kernel Signal 的正确性（需要 D）：防止在 kernel 关键路径上处理信号导致内核数据结构不一致、避免在持有重要锁时被中断导致死锁（持锁顺序问题）、确保资源清理工作能够完整执行等。这里需要 <strong>avoids partial execution states</strong>.</p></li><li><p>存在很多可以安全中断的等待（需要 S），如等待用户输入、等待定时器、等待非关键资源；</p></li><li><p>并不是所有持并发资源锁等待 / 等 IO 资源的进程都需要是 D 状态。甚至这么说：绝大多数持锁/或者等 IO 的进程（如 <code>mutex_lock</code>, <code>rwsemaphore</code>, <code>futex waits</code>, <code>pipe reads</code>, <code>socker rw</code>, <code>poll/select/epoll</code>, 绝大多数文件系统操作等）都不需要进入 D 状态。进入 D 状态的评判标准：</p><ul><li><p>The task must sleep (cannot spin).</p></li><li><p><strong>If a signal arrived at that sleep point, the kernel could not safely abort the operation and unwind all held state.</strong></p></li></ul></li></ol></li></ul></li><li><p><strong>Terminated -&gt; Z + X</strong></p><ul><li><strong>Z (Zombie)</strong>: 进程已终止但父进程尚未调用 <code>wait()</code> 回收其资源；</li><li><strong>X (Dead)</strong>: 完全终止状态，通常不会在 ps 这样的命令中看到；</li><li>理论模型中的 Terminated 在 Linux 实现中被细化为资源回收的不同阶段；</li></ul></li><li><p><strong>Linux 特有状态</strong></p><ul><li><strong>T (Stopped)</strong>: “Stopped, either by a job control signal or because it is being traced”，这个状态在经典理论模型中没有直接对应，可以看作是 Blocked 的一种特殊情况，被 shell / 调试工具等控制的进程；</li><li><strong>W (Paging)</strong>: 在 Linux 2.6.xx 内核后已不再有效；</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;预备知识库：计算机组成原理（如内存 Hierarchy、CPU 基本组成），操作系统原理（kernel/user space、资源抽象、进程状态抽象等）。&lt;/p&gt;
&lt;p&gt;以下是很零碎、经验化的思路，总结下来方便回顾。&lt;/p&gt;
&lt;h2 id=&quot;1-CPU-分析&quot;&gt;&lt;a hre</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="OS" scheme="https://blog.sjtuxhw.top/tags/OS/"/>
    
    <category term="IO" scheme="https://blog.sjtuxhw.top/tags/IO/"/>
    
    <category term="Linux" scheme="https://blog.sjtuxhw.top/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>ROS2 迁移到 OpenHarmony 平台后无法加载 Plugins 动态链接库的解决方案</title>
    <link href="https://blog.sjtuxhw.top/technical/ros2ohos-plugins-fail/"/>
    <id>https://blog.sjtuxhw.top/technical/ros2ohos-plugins-fail/</id>
    <published>2025-12-18T12:49:17.000Z</published>
    <updated>2025-12-28T12:53:50.004Z</updated>
    
    <content type="html"><![CDATA[<p>越过交叉编译的重重阻碍，我在将 ROS2 及其生态迁移到原生 OpenHarmony 平台上的过程中遇到了一个比较大的问题：ROS2 似乎无法加载插件形态的动态链接库！就是说，launch 一个 ROS2 应用（需要动态链接库）本身是可以的，但是这个 ROS2 应用如果使用到了动态链接库插件（一般通过 ROS2 的 <code>class_loader</code> 组件间接加载）就出问题了。</p><p>TL; DR:（太长不看版）最终解决方案请参见 “<a href="#Final-Solution">Final Solution</a>” 一节；如果想看问题根源请参见 “<a href="#四、问题根源">问题根源</a>” 一节。</p><p>下文将详细叙述调试经过，供读者思考、相互学习。举个实际迁移调试过程中的例子。下面是我编写的问题描述：</p><hr><p>我在 OpenHarmony 上运行迁移的 Navigation2 框架时遇到一些问题，加载的 navigation2 相关的动态链接库出现加载失败的问题。</p><p>运行的应用环境是任意一个使用 navigation2 的 ROS 应用，例如 B 站鱼香肉丝 UP 开发的 fishbot 机器人的 <code>navigation2.launch.py</code>（删除 rviz2 结点）。</p><p>问题的图片信息如下：</p><p><img src="imgs/1.png" /></p><hr><p>并且不仅仅是一个 <code>DifferentialMotionModel</code>（<code>libmotions_lib.so</code>），其他所有插件，如 StaticLayer（<code>liblayers.so</code>）全部报错。。</p><h3 id="一、猜测：Symbols-Not-Found"><a href="#一、猜测：Symbols-Not-Found" class="headerlink" title="一、猜测：Symbols Not Found?"></a>一、猜测：Symbols Not Found?</h3><p>首先我考虑，会不会是符号缺失问题？在 OpenHarmony 上交叉编译可能有些链接参数和 runpath 设置有问题？</p><p>于是使用 <code>readelf -d</code>、<code>nm -D -C</code>、<code>ldd</code> 工具（从 glibc 上移植过来的），一通查找发现，并不缺少动态链接库，也不缺少符号。</p><p><strong><u>那么排除符号缺失问题，只能是加载问题</u></strong>。为什么相同的动态链接库代码，在 Ubuntu 上正常运行，但在 OpenHarmony 上运行失败呢？它们最大的区别是 musl libc 和 glibc。</p><h3 id="二、临时的解决方案？新问题出现"><a href="#二、临时的解决方案？新问题出现" class="headerlink" title="二、临时的解决方案？新问题出现"></a>二、临时的解决方案？新问题出现</h3><p>这样只能从 ROS2 加载所有插件的源码入手。通过源码层层查找可知，ROS2 主动加载动态链接库（尤其是插件）的逻辑位于 <code>rcutils</code> 包，以及 <code>class_loader</code> 包。 </p><p>查找方法是先找加载 Nav2 插件（其实不仅仅是 Nav2 插件）的函数，按调用链溯源到 <code>rcutils/src/shared_library.c</code> 和 <code>class_loader/include/class_loader/class_loader_core.hpp</code>。</p><p><img src="imgs/3.png" /></p><p>我们发现，<code>class_loader</code> 中，创建插件中对象实例的函数中 <code>dynamic_cast</code> 总是返回空指针，提示转换失败。</p><p>我再次确认，Ubuntu 上并不存在这个问题，所以指针指向的数据区域理应是对的（因为代码逻辑没问题）？</p><p>为了快速确认是否是这里的问题，我临时将 <code>dynamic_cast</code> 改为 <code>static_cast</code> 来规避运行时检查，强制按照目标类型来使用目标内存，结果成功了？看起来没有，因为出现新的问题了：</p><p><img src="imgs/4.jpg" /></p><blockquote><p>注意：中间我们还修复了另一个问题，<code>bad_weak_ptr</code>，这个问题是 Humble 官方问题，和 OH 无关，解决方案 Github 都是有的，这里是 <a href="https://github.com/ros-navigation/navigation2/issues/5335">对应 issue</a> 和 <a href="https://github.com/ros-navigation/navigation2/pull/5341/files">对应 PR</a>，我们照着修改 <code>ros_navigation2/nav2_util/include/nav2_util/lifecycle_node.hpp</code> 和 <code>ros_navigation2/nav2_util/src/lifecycle_node.cpp</code> 就行，这里和讨论无关，不再赘述。</p></blockquote><p>由于新问题 “Exception when loading BT: [Any:convert]: no known safe conversion between…” 在网络上甚至找不到相关求助帖，我们只能继续手动调试。</p><p>这个问题似乎是和 Behavior Tree 解析 XML（<code>.../navigate_through_poses_w_replanning_and_recovery.xml</code>）和实例化有关，并且看起来和前一个问题没关系（暂时的）。</p><p>因此我们继续补充日志，使用 <code>execinfo.h</code> 打印问题堆栈，在递归构造 Behavior Tree 的时候打印节点详细信息等等，一步步溯源。添加的第一版日志内容：</p><p><img src="imgs/5.jpg" /></p><p>以及 XML 文件信息：</p><p><img src="imgs/6.jpg" /></p><p>结合源码（<code>behavior_tree_cpp_v3/src</code> 中的 <code>xml_parsing.cpp</code> 和 <code>bt_factory.cpp</code>）看出，程序解析到 <code>RemovePassedGoals</code> 后，调用 <code>XMLParser::Pimpl::createNodeFromXML</code> 创建 Behavior Tree 节点时抛出上面的异常。</p><p>于是根据这个线索阅读源码、定向加强日志：</p><p><img src="imgs/7.jpg" /></p><p>问题出在 <code>BehaviorTreeFactory::instantiateTreeNode</code> 执行加载 Behavior Tree 的插件 <code>RemovePassedGoals</code> 上！</p><p>阅读 <code>bt_factory.h</code> 和 <code>nav2_behavior_tree/plugins/action/remove_passed_goals_action.cpp</code> 源码可知，插件加载过程中，会从 <code>BehaviorTree::Blackboard</code>（注释描述是在插件间存放 Behavior Tree 有类型数据的数据结构）上读取类型为 <code>std::shared_ptr&lt;tf2_ros::Buffer&gt;</code> 的数据。</p><p>这个数据在插件注册（configure）时被用 <code>Any</code> 的方法写到 Blackboard 中，而插件实例化（activate）时从 Any 中以 <code>safe_any::cast&lt;T&gt;</code> 的方法读取出来。</p><p><strong><u>重点来了：但两种类型相同为什么会出错呢？我猛然想到上一个 <code>dynamic_cast</code> 失败的问题</u></strong>。。</p><h3 id="三、问题的共性：C-Template-RTTI-Run-Time-Type-Information"><a href="#三、问题的共性：C-Template-RTTI-Run-Time-Type-Information" class="headerlink" title="三、问题的共性：C++ Template RTTI (Run-Time Type Information)"></a>三、问题的共性：C++ Template RTTI (Run-Time Type Information)</h3><p>新问题中，<code>std::shared_ptr&lt;tf2_ros::Buffer&gt;</code> 恰好是一个模板实例化出来的类型，而旧问题中，<code>dynamic_cast</code> 映射的 <code>impl::AbstractMetaObject&lt;Base&gt;</code> 也是一个模板实例化的类型。</p><p>这两个问题的共性是，<strong><u>看起来它们的类型名称是一样的，但实际上运行时 libc 认为它们是不同的类型</u></strong>？这个验证起来简单，直接打印类型对应的 type info 和 <code>typeid</code>。</p><p>以旧问题为例，使用打印调试法，检查 <code>factoryMap</code> 中究竟存放了什么，为什么类型不匹配？在 <code>factory</code> 赋值语句后面添加调试代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- DEBUGGING BLOCK START ---</span></span><br><span class="line">impl::AbstractMetaObjectBase* base_ptr = factoryMap[derived_class_name];</span><br><span class="line"><span class="keyword">if</span> (!factory &amp;&amp; base_ptr) &#123;</span><br><span class="line">  <span class="type">const</span> std::type_info&amp; src_type = <span class="built_in">typeid</span>(*base_ptr);</span><br><span class="line">  <span class="type">const</span> std::type_info&amp; dst_type = <span class="built_in">typeid</span>(impl::AbstractMetaObject&lt;Base&gt;);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CONSOLE_BRIDGE_logError</span>(<span class="string">&quot;class_loader.impl: DYNAMIC CAST FAILURE DEBUGGING:&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1. Check raw pointer</span></span><br><span class="line">  <span class="built_in">CONSOLE_BRIDGE_logError</span>(<span class="string">&quot;  Raw Pointer Address: %p&quot;</span>, (<span class="type">void</span>*)base_ptr);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. Compare Type Names</span></span><br><span class="line">  <span class="built_in">CONSOLE_BRIDGE_logError</span>(<span class="string">&quot;  Source Type Name: %s&quot;</span>, src_type.<span class="built_in">name</span>());</span><br><span class="line">  <span class="built_in">CONSOLE_BRIDGE_logError</span>(<span class="string">&quot;  Target Type Name: %s&quot;</span>, dst_type.<span class="built_in">name</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. Compare Type Info Addresses</span></span><br><span class="line">  <span class="built_in">CONSOLE_BRIDGE_logError</span>(<span class="string">&quot;  Source TypeInfo Address: %p&quot;</span>, (<span class="type">void</span>*)&amp;src_type);</span><br><span class="line">  <span class="built_in">CONSOLE_BRIDGE_logError</span>(<span class="string">&quot;  Target TypeInfo Address: %p&quot;</span>, (<span class="type">void</span>*)&amp;dst_type);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4. Check Base Class TypeInfo</span></span><br><span class="line">  <span class="type">const</span> std::type_info&amp; base_param_type = <span class="built_in">typeid</span>(Base);</span><br><span class="line">  <span class="built_in">CONSOLE_BRIDGE_logError</span>(<span class="string">&quot;  Base Template Param TypeInfo Addr: %p&quot;</span>, (<span class="type">void</span>*)&amp;base_param_type);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// --- DEBUGGING BLOCK END ---</span></span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[component_container_isolated-1] Error:   class_loader.impl: DYNAMIC CAST FAILURE DEBUGGING:</span><br><span class="line">[component_container_isolated-1]          at line 295 in .../class_loader_core.hpp </span><br><span class="line">[component_container_isolated-1] Error:     Raw Pointer Address: 0x7f8ac2f430</span><br><span class="line">[component_container_isolated-1]          at line 298 in .../class_loader_core.hpp </span><br><span class="line">[component_container_isolated-1] Error:     Source Type Name: N12class_loader4impl10MetaObjectIN9nav2_amcl23DifferentialMotionModelENS2_11MotionModelEEE</span><br><span class="line">[component_container_isolated-1]          at line 301 in .../class_loader_core.hpp </span><br><span class="line">[component_container_isolated-1] Error:     Target Type Name: N12class_loader4impl18AbstractMetaObjectIN9nav2_amcl11MotionModelEEE</span><br><span class="line">[component_container_isolated-1]          at line 302 in .../class_loader_core.hpp </span><br><span class="line">[component_container_isolated-1] Error:     Source TypeInfo Address: 0x7ef19881a8</span><br><span class="line">[component_container_isolated-1]          at line 306 in .../class_loader_core.hpp </span><br><span class="line">[component_container_isolated-1] Error:     Target TypeInfo Address: 0x7efa22ad28</span><br><span class="line">[component_container_isolated-1]          at line 307 in .../class_loader_core.hpp </span><br><span class="line">[component_container_isolated-1] Error:     Base Template Param TypeInfo Addr: 0x7efa224840</span><br><span class="line">[component_container_isolated-1]          at line 311 in .../class_loader_core.hpp </span><br></pre></td></tr></table></figure><p>关键来了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[component_container_isolated-1] Error:      Source TypeInfo Address: 0x7ef19881a8</span><br><span class="line">[component_container_isolated-1] Error:      Target TypeInfo Address: 0x7efa22ad28</span><br></pre></td></tr></table></figure><p>这两个类型的类型对象（type info object）的地址不同，这意味着 libc 将插件中的 <code>AbstractMetaObject</code>（0x7ef…）与 <code>class_loader</code> 中的 <code>AbstractMetaObject</code>（0x7efa…）视为完全不同的类型，尽管它们名称相同！</p><p><strong><u>这正是典型的 Split RTTI（Run-Time Type Information）问题！</u></strong></p><h3 id="四、问题根源"><a href="#四、问题根源" class="headerlink" title="四、问题根源"></a>四、问题根源</h3><p>为了验证我的想法，我先后执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nm -C -D libclass_loader.so | grep AbstractMetaObject</span><br><span class="line">nm -C -D libmotions_lib.so | grep AbstracMetaObject</span><br></pre></td></tr></table></figure><p>观察到第一条指令输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">... (omit)</span><br><span class="line">0000000000042078 V typeinfo for class_loader::impl::AbstractMetaObjectBase</span><br><span class="line">0000000000016a6c V typeinfo name for class_loader::impl::AbstractMetaObjectBase</span><br><span class="line">0000000000042060 V vtable for class_loader::impl::AbstractMetaObjectBase </span><br></pre></td></tr></table></figure><p>第二条输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> ... (omit)</span><br><span class="line">0000000000008110 V typeinfo for class_loader::impl::AbstractMetaObject&lt;nav2_amcl::MotionModel&gt;</span><br><span class="line">0000000000008100 V typeinfo for class_loader::impl::AbstractMetaObjectBase</span><br><span class="line">00000000000043f9 V typeinfo name for class_loader::impl::AbstractMetaObject&lt;nav2_amcl::MotionModel&gt;</span><br><span class="line">000000000000443e V typeinfo name for class_loader::impl::AbstractMetaObjectBase </span><br></pre></td></tr></table></figure><p>这证明了一件事，在两个动态链接库中，<code>AbstractMetaObjectBase</code> RTTI symbols 全标记为 Weak（<code>V</code>）。</p><p>询问了 GPT 后发现，这个加载行为对于 musl libc 和 glibc 是不同的：</p><div class="table-container"><table><thead><tr><th>Area</th><th>glibc</th><th>musl</th></tr></thead><tbody><tr><td>Typeinfo merging</td><td>forgiving</td><td>strict</td></tr><tr><td>Symbol interposition</td><td>permissive</td><td>limited</td></tr><tr><td><code>RTLD_GLOBAL</code> side effects</td><td>common</td><td>reduced</td></tr><tr><td>Weak RTTI symbols</td><td>often merged</td><td>not merged</td></tr></tbody></table></div><p>于是我判断问题的根源：</p><ol><li>两个库中都将符号 <code>typeinfo for ... AbstractMetaObjectBase</code> 标记为 <code>V</code>（弱引用）。</li><li>musl libc 的动态链接器（<code>ld-musl-xxx.so</code>）对符号作用域边界要求更严格。当 <code>class_loader</code> 使用 dlopen 加载插件时，它<strong><u>不会将 RTTI symbols 加入全局符号表</u></strong>。</li><li>再由于 RTTI symbols 不是全局可见（或插件处于隔离状态）的，插件无法 “看到” 主进程中的现有 TypeInfo；此时 musl libc 会 fallback 到使用 RTTI symbol local copy 模式：<strong><u>对每个没见过的 RTTI symbol，都在自己的内存空间中实例化创建一个新的 type info object</u></strong>；</li></ol><p>正因如此，跨动态链接库（DSOs）的 TypeInfo 对象的指针地址才会不一致，进而导致了两个相同的类型信息错误地存在两个不同的内存地址，<strong><u>进而导致 <code>dynamic_cast&lt;T&gt;</code> 以及 <code>safe_any::cast&lt;T&gt;</code> 这样的逻辑没法正确判断横跨插件传递的类型是否一致</u></strong>。</p><h3 id="五、问题缘由？"><a href="#五、问题缘由？" class="headerlink" title="五、问题缘由？"></a>五、问题缘由？</h3><p>所以为什么会出现这个问题？</p><p>一开始我怀疑 musl libc 将不同动态链接库间共享全局 RTTI symbols 的机制改坏了，导致 RTTI 符号没办法跨动态链接库共享（或者是 HUAWEI 官方为了“安全”有意为之。。）</p><p><img src="imgs/8.png" /></p><p><strong>因为我调试时发现添加 <code>LD_PRELOAD</code> 提前将 <code>libclass_loader</code> 和 <code>liblayers</code> 加载到所有进程的内存中可以避免 RTTI symbols 重复创建，进而解决问题</strong>。</p><p>之后和深开鸿那边的技术人员交流和调试后发现，目前交叉编译工具链中的链接器 flags 是这样设置的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">last 4 is <span class="keyword">for</span> moveit_core</span></span><br><span class="line">export SSRVODKA_APPEND_COMMON_LINK_FLAGS=&quot;-Wl,--no-as-needed -lpthread -ldl -lm -lsframe -fopenmp -lc -lrt -lBulletSoftBody -lBulletDynamics -lBulletCollision -lLinearMath&quot;</span><br></pre></td></tr></table></figure><p>因为 <code>-Wl,--no-as-needed</code> 是一个<strong>状态开关</strong>，会影响其后所有引入库的处理方式，这导致编译链接时由 CMake 添加的其他库会被强制加入被编译库的依赖列表中。</p><p>一般程序可能只是加载时性能差一点，但是它还会改变<strong>库的加载方式和符号可见性</strong>，让我们<strong><u>误以为 RTTI symbols 已经共享</u></strong>（表现出之前 <code>readelf / ldd</code> 看到不缺少符号），但是实际上仍然有 split RTTI symbols 的问题。甚至会干扰正常的加载次序（也就是说滥用 <code>-Wl,--no-as-needed</code> 相当于乱填写 <code>LD_PRELOAD</code>）；</p><blockquote><p>为什么这么判断呢？因为发现加入 <code>-Wl,--as-needed</code> 就不需要 <code>LD_PRELOAD</code> 了，但仍然会出现 split RTTI symbols；</p></blockquote><p>总之，目前是未知原因（判断可能是 OH musl libc 的行为）导致的 split RTTI symbols + <code>-Wl,--no-as-needed</code> 误导，合起来造成了这个错误。</p><h3 id="五、解决方案"><a href="#五、解决方案" class="headerlink" title="五、解决方案"></a>五、解决方案</h3><p>有了清晰的思路，问题就很好解决了。为了避免 RTTI symbols 在各个插件间不可见的问题，</p><ol><li><p><strong><u>首先是要确保 RTTI Symbols 能够正确导出</u></strong>，这个需要对所有可执行文件的链接过程（<code>CMAKE_EXE_LINKER_FLAGS</code>）加上 <code>-rdynamic</code>（或 <code>-Wl,--export-dynamic</code>）；</p></li><li><p>然后需要让所有插件需要的 RTTI Symbols 显式加载到 global symbol table 中，让插件加载前这些 RTTI symbols 就已经位于内存中，这样再加载插件时，musl libc 能从 global symbol table 中找到，就不会错误创建额外的 type info object 了！</p></li></ol><p>到这里，问题就能解决了吗？很可惜，还差一点。我们发现，仅仅是这样还是会出现 type info 不一致的问题。不应该啊？我们确定了问题根源，也找到了非常正确解决方案。。</p><p>最后这一个坑我怀疑是 OpenHarmony 留给我们的。因为我发现执行完上述方案后在 OH 上仍然存在 Split RTTI Symbols 的问题，这个对于正常的 musl / glibc 的 dynamic linker 都是没有的行为。最后还是需要强制将 Behavior Tree 相关的 RTTI symbols 强制加载进去。</p><h3 id="Final-Solution"><a href="#Final-Solution" class="headerlink" title="Final Solution"></a>Final Solution</h3><ol><li><strong><u>确保 RTTI Symbols 能够正确导出</u></strong>，这个需要对所有可执行文件的链接过程（<code>CMAKE_EXE_LINKER_FLAGS</code>）加上 <code>-rdynamic</code>（或 <code>-Wl,--export-dynamic</code>）；</li><li><strong><u>确保交叉编译工具链的链接器 flags 不含有多余的 <code>-Wl,--no-as-needed</code></u></strong>（如非必要，不应该将此选项保持开启，应该用完就关），可以尽可能避免插件错误的依赖和加载次序；</li><li>（OpenHarmony 上的权宜之计）ROS2 源码将 <code>src/ros/class_loader/include/class_loader/class_loader_core.hpp</code> 中的 <code>dynamic_cast</code> 改为 <code>static_cast</code>；</li><li>（OpenHarmony 上的权宜之计）启动前添加 <code>LD_PRELOAD</code> 强制加载一些库（假设安装位置是 <code>/data/install</code>）：<code>export LD_PRELOAD=/data/install/lib/libc++_shared.so:/data/install/lib/libbehaviortree_cpp_v3.so:/data/install/lib/libbehavior_server_core.so:/data/install/lib/libnav2_behavior_tree.so:/data/install/lib/libbt_navigator_core.so</code>；</li></ol><p>如果不是 OpenHarmony musl libc 而是普通 musl libc，则不需要第三、四步就行！</p><p>更多 ROS2 + OpenHarmony 技术解决方案，欢迎关注 <a href="https://gitcode.com/openharmony-robot">OpenHarmony Robot PMC</a> 一同交流！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;越过交叉编译的重重阻碍，我在将 ROS2 及其生态迁移到原生 OpenHarmony 平台上的过程中遇到了一个比较大的问题：ROS2 似乎无法加载插件形态的动态链接库！就是说，launch 一个 ROS2 应用（需要动态链接库）本身是可以的，但是这个 ROS2 应用如果使用</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="HarmonyOS" scheme="https://blog.sjtuxhw.top/tags/HarmonyOS/"/>
    
    <category term="OpenHarmony" scheme="https://blog.sjtuxhw.top/tags/OpenHarmony/"/>
    
    <category term="ROS2" scheme="https://blog.sjtuxhw.top/tags/ROS2/"/>
    
  </entry>
  
  <entry>
    <title>具身智能论文速读 2025年11月</title>
    <link href="https://blog.sjtuxhw.top/technical/embodied-paper-202511/"/>
    <id>https://blog.sjtuxhw.top/technical/embodied-paper-202511/</id>
    <published>2025-11-15T02:25:05.000Z</published>
    <updated>2025-12-18T13:06:16.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification"><a href="#VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification" class="headerlink" title="VLA-0: Building State-of-the-Art VLAs with Zero Modification"></a>VLA-0: Building State-of-the-Art VLAs with Zero Modification</h2><p>NVIDIA 团队提出，探索一种<strong>极简且零修改</strong>的 VLA 模型构建范式，突破现有 VLA 方法的复杂性瓶颈，最终在仿真和真实场景中均实现 state-of-the-art（SOTA）性能。</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>VLA 的核心价值是让机器人同时理解视觉（环境图像）、语言（任务指令）并输出动作，是实现通用机器人操作的关键技术。但当前 VLA 的构建方法仍存在<strong>复杂性过高、性能与简洁性难以兼顾</strong>的问题，具体可通过现有三类主流方法的局限体现：</p><p><img src="imgs/cate.png" width="1000px" /></p><div class="table-container"><table><thead><tr><th>类别</th><th>代表模型</th><th>核心思路</th><th>关键缺陷</th></tr></thead><tbody><tr><td>Discrete Token VLAs（AG）</td><td>RT-2、OpenVLA</td><td>连续动作 → 离散为 “动作 bin” → 映射到 VLM 词汇表</td><td>1. 动作分辨率受限（细粒度控制需数千 bin，与 VLM 词汇冲突）；2. 破坏 VLM 预训练语言理解（复用词汇）</td></tr><tr><td>Generative Action-Head VLAs（AG）</td><td>π₀、SmolVLA</td><td>VLM 输出 latent 向量→新增 “动作生成头”（如扩散模型）解码为动作</td><td>1. 新增网络需额外微调；2. 削弱 VLM 的语言接地能力（语言与动作关联变弱）；3. 泛化性受非预训练动作头影响</td></tr><tr><td>自定义架构 VLAs（Custom Arch）</td><td>OpenVLA-OFT、π-FAST</td><td>修改 VLM 架构（如加专用 ACT 头）或自定义动作令牌器（如 DCT）</td><td>1. 需大规模架构修改与额外参数；2. 训练流程复杂（如自定义令牌器需单独优化）</td></tr></tbody></table></div><p><strong>是否存在一种 “无侵入式” 的 VLA 构建方法：不修改 VLM 的词汇表、不新增网络组件、不改变架构，同时实现 SOTA 性能？</strong></p><p>“将动作直接表示为文本” 这一最简单的策略被长期忽视（因直觉上认为 “文本不适合表示连续动作”），因此这个工作正是为验证该策略的有效性而提出 VLA-0。</p><h3 id="Design-Principle"><a href="#Design-Principle" class="headerlink" title="Design Principle"></a>Design Principle</h3><p>“零修改 VLM，最大化利用其原生文本生成能力”，通过精心设计的 “输入 - 输出范式 + 训练 / 推理策略”，实现动作预测的高性能</p><ul><li><p>Backbone:  Qwen-VL-2.5（3B），<strong>Vision encoder (ViT)</strong> for image features + <strong>Language model</strong> for reasoning and text generation.</p><ul><li><strong>性能均衡</strong>：在同参数规模（3B）下，视觉 - 语言理解能力 competitive；</li><li><strong>计算高效</strong>：相比大模型（如 17B），训练和推理速度更快，降低落地门槛；</li><li><strong>开源可复现</strong>；</li></ul></li><li><p>输入输出：完全遵循 VLM 的 “视觉 + 文本输入 -&gt; 文本输出” 范式</p><ul><li><p>Input：</p><ul><li><p>System Prompt：明确任务规则，示例如下（H = 预测步数，D = 动作维度，B = 整数范围）：</p><p>“分析输入图像，预测未来 H 步的机器人动作。每步动作含 D 个维度。仅输出 H×D 个整数（范围 0-B），用空格分隔，不包含其他内容。”</p><p>该提示强制 VLM 仅生成动作相关的整数序列，避免冗余文本。</p></li><li><p>图像输入：根据场景灵活选择，且两种输入方式性能无差异：</p><ul><li>仿真场景：第三人称相机 + 手腕相机（与基线模型一致）；</li><li>真实场景：左右双目相机（如图 3 所示）；</li><li>可选方案：将多图拼接为单张复合图（简化输入逻辑，性能不变）。</li></ul></li><li><p><strong>任务指令（Task Instruction）</strong>：自然语言描述目标，如 “Put the cupcake in the bowl”（将纸杯蛋糕放进碗里）。</p></li></ul></li><li><p>Output:</p><p><strong>空格分隔的整数序列</strong>（如 “4 12 98 3 0 0 …”），代表连续动作的 “文本化编码”：</p><ul><li>（prompt 编码时）将机器人的连续动作值（如末端执行器坐标、关节角度）归一化到 [0, B] 的整数范围（B 为超参，实验中最优值为 1000）；</li><li>推理时将整数序列反归一化回连续动作，直接控制机器人。</li></ul></li></ul></li></ul><p><img src="imgs/arch.png" width="700px" /></p><h3 id="Core-Tech"><a href="#Core-Tech" class="headerlink" title="Core Tech"></a>Core Tech</h3><p>如果仅依靠 “文本表示动作” 的范式，VLA-0 是没有办法达到 SOTA 水平的，所以重点还是在下面的核心技术中：</p><h4 id="Action-Decoding"><a href="#Action-Decoding" class="headerlink" title="Action-Decoding"></a>Action-Decoding</h4><p>通过 “连续动作 -&gt; 归一化整数 -&gt; 文本输出”，无需依赖 VLM 词汇表的 bin 划分，理论上可通过调整超参数 B（如 1000、4000）实现任意分辨率（实验证明 B=1000 已足够，B=4000 无额外增益）。做法就是将连续动作的文本表示映射到固定范围的 <code>[0, B]</code> 中，<code>_val = round((value - min) / (max - min) * B)</code>；</p><p>做法优点：</p><ul><li>可以克服像 OpenVLA 这样 Discrete Token VLA 的缺陷（encoded bins 数量限制 &amp; token 冲突、语义污染降低理解能力、naive per-dimension binning 表示低效性($\pi$-FAST 尝试解决) ）；</li><li>支持任意分辨率的动作细节，而不需要大规模的 discrete tokens 干扰模型的 vocabulary；</li></ul><h4 id="Ensemble-Prediction"><a href="#Ensemble-Prediction" class="headerlink" title="Ensemble Prediction"></a>Ensemble Prediction</h4><blockquote><p>不是创新点</p></blockquote><p>本质上是通过 action chunking 提升动作稳定性（单步预测噪声很大，结合上模型较低的输出频率(tip: $\pi$-0 基于的 flow matching 的 diffusion head)），借鉴 ACT 模型的 “多步预测集成” 策略：<u>推理时，VLM 每步预测 <strong>n 个未来动作</strong>（如 n=5），当前步 t 的最终动作是 “t 步预测的第 1 个动作、t-1 步预测的第 2 个动作、…、t-n+1 步预测的第 n 个动作” 的平均值</u>；</p><blockquote><p>消融实验验证：通过集成降低单步噪声，实验证明移除该技术后成功率下降 2 个百分点；</p></blockquote><h4 id="Masked-Action-Augmentation"><a href="#Masked-Action-Augmentation" class="headerlink" title="Masked Action Augmentation"></a>Masked Action Augmentation</h4><p>因为 VLM 的文本生成是自回归的，若仅训练生成整数序列，VLM 可能只是把任务误当成了另一种 auto-completion，比如根据时间序列的规律上次数值是 100 下次就会猜测 101，并没有结合到视觉信息，仅依靠语言推理；</p><p>训练时<strong>随机掩码目标动作字符串中的部分字符</strong>（如将 “123 456” 掩码为 “1#3 4##”），迫使 VLM 必须依赖图像（环境信息）和任务指令（目标）推理动作，而非仅依赖前序数值；</p><blockquote><p>消融实验验证：移除该技术后成功率下降 1.2 个百分点，证明其能强化 VLM 的 “视觉 - 语言 - 动作关联”。</p></blockquote><p>缺点：文章并未提及相关掩码策略？</p><blockquote><ul><li>数值操作是具有强局部结构（数字顺序编码大小）和全局语义（微小数字变化可能对应实际单位中的巨大变化，取决于量纲）的多位数字符串。随机遮蔽单个字符会以不可预测的方式破坏结构。</li><li>自回归语言模型基于先前生成的标记进行训练。若在目标侧进行遮蔽，模型仍然可能学会复制未遮蔽的前缀并简单“猜测”后缀；若遮蔽分布不切实际，则可能过度拟合遮蔽模式？</li><li>“动作” 这个东西会不会有依赖项（需要跨标记衡量），如关节向量或(x,y,z)三元组。掩盖动作部分与掩盖完整动作条目会产生不同效果。</li></ul></blockquote><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>VLA-0 仅需对基础 VLM 进行<strong>全量微调</strong>（无新增参数），训练配置：</p><ul><li>损失函数：标准交叉熵损失（与 VLM 预训练一致，无需设计新损失）；</li><li>优化器：Adam；</li><li>训练参数：64 个 epoch，batch size = 192，学习率 = $5\times 10^{-6}$；</li><li>计算资源：8 张 A100 GPU，训练耗时约 32 小时（相比大模型微调更高效）。</li></ul><blockquote><p>[!QUESTION]</p><p>无任何微调（仅通过 prompt engineering 实现动作预测）， VLA-0 的策略是否仍能生效？</p><p>VLA-0 的性能依赖 “VLM 的原生文本生成能力” 还是 “微调后习得的动作 - 视觉 - 语言关联”（只是 Qwen 单纯比较好）？</p></blockquote><h3 id="Tests-amp-Eval"><a href="#Tests-amp-Eval" class="headerlink" title="Tests &amp; Eval"></a>Tests &amp; Eval</h3><ul><li><p>仿真环境实验：LIBERO benchmark</p><p><img src="imgs/libero.png" /></p><p>| 套件     | 测试能力                       | VLA-0 成功率 | 最优 baseline（无预训练）成功率 |<br>| ———— | ——————————————— | —————— | ———————————————- |<br>| Spatial  | 空间定位能力（如精准放置）     | 97.0%        | $\pi_{0.5}$-KI（96.6%）         |<br>| Object   | 物体交互能力（如操作特定物体） | 97.8%        | $\pi_{0.5}$-KI（97.2%）         |<br>| Goal     | 目标理解能力（如匹配指令目标） | 96.2%        | $\pi_{0.5}$-KI（94.6%）         |<br>| Long     | 长序列动作能力（如多步操作）   | 87.6%        | $\pi_{0.5}$-KI（85.8%）         |<br>| <strong>平均</strong> | ——                             | <strong>94.7%</strong>    | $\pi_{0.5}$-KI（93.3%）         |</p></li><li><p>真实世界实验：LeRobot SO-100 dataset，和 SmolVLA 对比</p><p><img src="imgs/smol.png"/></p><blockquote><p>VLA-0 无大规模预训练，仍能在各方面平均领先 12.5 个百分点，证明其在真实场景的鲁棒性。</p></blockquote></li><li><p>消融实验：</p><p>| 实验配置                                   | 平均成功率     | 性能下降幅度 |<br>| ————————————————————— | ——————— | —————— |<br>| 完整 VLA-0（动作集成 + 掩码增强 + B=1000） | 94.7%          | -            |<br>| 移除动作集成                               | 92.7%          | -2.0%        |<br>| 移除掩码增强                               | 93.5%          | -1.2%        |<br>| B=250（降低分辨率）                        | 93.2%          | -1.5%        |<br>| B=4000（提高分辨率）                       | 94.2%          | -0.5%        |<br>| 图像拼接 vs 分开输入                       | 94.5% vs 94.7% | 无差异       |</p></li></ul><p>Code not available for now…</p><h3 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h3><ul><li><p>未探索大规模动作数据预训练：若给 VLA-0 加入大规模机器人数据预训练，性能是否能进一步超越 OpenVLA-OFT？</p></li><li><p>推理速度优化：当前真实场景推理速度为 4Hz，可通过模型蒸馏、量化进一步提升，适配更高实时性需求；</p></li></ul><blockquote><p>当前 VLA-0 仅验证了 “低维动作”（如末端执行器坐标、关节角度）的文本化表示。“高维动作”（如机器人手部 20  个自由度的精细操作）呢？</p><p>整数序列的长度会大幅增加（如 H=10 步 ×D=20 维 = 200 个整数）。会不会导致 “动作序列中前后维度的依赖关系紊乱”？</p><p>Modify the code</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification&quot;&gt;&lt;a href=&quot;#VLA-0-Building-State-of-the-Art-VLAs-with-Zero-Modification&quot; </summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="AI" scheme="https://blog.sjtuxhw.top/tags/AI/"/>
    
    <category term="Robot" scheme="https://blog.sjtuxhw.top/tags/Robot/"/>
    
    <category term="Paper" scheme="https://blog.sjtuxhw.top/tags/Paper/"/>
    
    <category term="VLA" scheme="https://blog.sjtuxhw.top/tags/VLA/"/>
    
  </entry>
  
  <entry>
    <title>Survey: 有哪些好用的用于网络搜索的 MCP Servers?</title>
    <link href="https://blog.sjtuxhw.top/chat/web-search-mcps/"/>
    <id>https://blog.sjtuxhw.top/chat/web-search-mcps/</id>
    <published>2025-08-28T15:56:12.000Z</published>
    <updated>2025-08-28T16:17:19.715Z</updated>
    
    <content type="html"><![CDATA[<p>网络搜索的 MCP servers 由两个部分组成：一个是网页搜索服务，另一个是包装成符合 MCP 规范的 MCP server（供 Agent 使用）。</p><p>考虑到现在 MCP servers 相当繁多，而且大多生命短暂（无人维护），因此想让 AI 用上网络搜索的 MCP 工具，首先考虑的是有公司维护的、性价比高的产品，一般搜索质量会好一点，然后再考虑开发者个人搭建的那些 MCP servers（一般它们就是将一个或多个网页搜索服务包装起来，当然也有很多是“巧用”传统搜索引擎的）。</p><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL; DR"></a>TL; DR</h2><p>下面的表格先对目前 survey 到的进行总结：</p><h3 id="表-I-公司维护的网页搜索产品及配套-MCP-Server"><a href="#表-I-公司维护的网页搜索产品及配套-MCP-Server" class="headerlink" title="表 I: 公司维护的网页搜索产品及配套 MCP Server"></a>表 I: 公司维护的网页搜索产品及配套 MCP Server</h3><div class="table-container"><table><thead><tr><th>名称</th><th>主要功能</th><th>免费额度/限制</th><th>搜索服务 私有部署支持</th><th>MCP Server</th><th>使用体验</th></tr></thead><tbody><tr><td>Brave Search</td><td>网页搜索、新闻聚合、视频/图像聚合</td><td>1 请求/秒，2000 请求/月，free plan 也需要绑定银行卡</td><td>不支持</td><td><a href="https://brave.com/search/api/guides/use-with-claude-desktop-with-mcp/">@modelcontextprotocol/server-brave-search</a></td><td>信用卡问题没能尝试到</td></tr><tr><td>Exa.ai</td><td>网页搜索、网页爬虫、AI 生成问答、Agent Research、Websets 表格整理</td><td>仅 1000 credits（价值 $10），后续需购买，信用卡订阅制</td><td>不支持</td><td><a href="https://github.com/exa-labs/exa-mcp-server">Exa MCP Server</a></td><td>牛的，只能说质量确实超过了其他几种方案，但是免费额度太少了</td></tr><tr><td>Firecrawl</td><td>网页抓取和爬取（转 Markdown 等）、映射、网页搜索、AI提取</td><td>500 credits（每页面 1 credit），2并发请求，低速率限制，后续需购买，信用卡订阅制</td><td>支持（开源）</td><td><a href="https://docs.firecrawl.dev/mcp-server">Firecrawl MCP Server</a></td><td>只要是自己使用，它的scrape 功能足够了（不会被封 IP 的话），但playwright 实例太耗费计算资源了</td></tr><tr><td>Tavily</td><td>语义 Web 搜索、Web 访问、Research、Extract</td><td>每月 1000 次搜索（学生认证免费），信用卡订阅制</td><td>不支持</td><td><a href="https://docs.tavily.com/documentation/mcp">Tavily MCP Server</a></td><td>中规中矩，免费版限速肯定没法商用部署，自己用马马虎虎，搜索结果有些杂</td></tr><tr><td>Jina</td><td>Reader（爬取 URL）、Web/学术搜索、SERP、Embedding、Reranker 等</td><td>无 API Key 时仅 Reader 功能（20 RPM）；免费计划 1000 万 tokens，后续需购买，信用卡订阅制</td><td>不支持</td><td><a href="https://github.com/jina-ai/MCP">Jina MCP Server</a></td><td>仅次于 exa</td></tr><tr><td>SearXNG</td><td>元搜索引擎（整合多个传统搜索引擎）</td><td>无账户/API Key，依赖实例承受能力</td><td>支持（开源）</td><td>社区维护</td><td>这个看似是 meta search engine，实际上能用的比较少，可能是各大搜索引擎反爬措施越来越好了</td></tr><tr><td>Bright Data</td><td>网页搜索、网页爬虫、突破网页机器人检查、Websets</td><td>5000 次请求，后续需购买，免费版限制仅 web search，<strong>国内不需要信用卡，可用支付宝</strong></td><td>不支持</td><td><a href="https://docs.brightdata.com/mcp-server/overview">Bright Data MCP Server</a></td><td>这个免费版不能用？返回结果全是空值（可能是测试时账户审核没下来），没敢尝试付费版</td></tr></tbody></table></div><h3 id="表-II-开发者维护的-MCP-Servers"><a href="#表-II-开发者维护的-MCP-Servers" class="headerlink" title="表 II:  开发者维护的 MCP Servers"></a>表 II:  开发者维护的 MCP Servers</h3><div class="table-container"><table><thead><tr><th>名称</th><th>支持的搜索引擎</th><th>API Key需求</th><th>使用体验</th></tr></thead><tbody><tr><td><a href="https://github.com/mrkrsl/web-search-mcp">web-search-mcp</a></td><td>Bing（浏览器模拟）、Brave Search（浏览器模拟）、DuckDuckGo</td><td>不需要</td><td>嗯，比较一般，可用性不佳</td></tr><tr><td><a href="https://github.com/Aas-ee/open-webSearch">Open-WebSearch MCP Server</a></td><td>Bing、Baidu、CSDN、DuckDuckGo、Exa、Brave、Juejin、GitHub READMEs</td><td>不需要</td><td>bing 几乎用不了，duckduckgo 调用次数一多就返回空值了</td></tr><tr><td><a href="https://github.com/yokingma/one-search-mcp">one-search-mcp</a></td><td>Web search: SearXNG、Firecrawl、Tavily、DuckDuckGo、Bing;  Local Browser: Bing, Google, Baidu, Sogou</td><td>SearXNG/Tavily/Firecrawl 需可选 API Key，其他不需要</td><td>SearXNG 配置有些问题，部署脚本对着 firecrawl 改的，总体一般，和 firecrawl + open-websearch 很像</td></tr></tbody></table></div><p>总而言之，单从 “想要给 Agent 加入 web search 能力” 这个需求来说，如果是个人使用（优先 free plan），不考虑部署难度的话，建议：</p><p><strong><u>one-search-mcp</u> (多个 API Key)   &gt;   <u>tavily 官方 MCP servers</u>   &gt;   <u>自部署 SearXNG/Firecrawl 搜索服务 + 社区 MCP server</u>  &gt;   <u>不需要 API Key 的社区 MCP servers</u>   &gt;   <u>Jina/Exa/Firecrawl 官方 MCP + 对应的官方搜索服务</u> (个人使用性价比不高)</strong>；</p><p>如果考虑商用和可靠性，建议 <strong>Bright Data (国内充值方便)    &gt;   Exa.ai (有点贵)   &gt;   Tavily (搜索引擎内容有点杂)</strong>；</p><p>下面是详细介绍。</p><h2 id="公司维护的网页搜索产品-amp-配套的-MCP-server"><a href="#公司维护的网页搜索产品-amp-配套的-MCP-server" class="headerlink" title="公司维护的网页搜索产品 &amp; 配套的 MCP server"></a>公司维护的网页搜索产品 &amp; 配套的 MCP server</h2><h3 id="1-Brave-Search"><a href="#1-Brave-Search" class="headerlink" title="1. Brave Search"></a>1. Brave Search</h3><p>可以进行的功能：</p><ul><li><p>基本的网页搜索 Web search</p></li><li><p>新闻聚合 News cluster</p></li><li><p>视频/图像聚合 Videos/Images</p></li></ul><p>建议注册 API 获得更稳定的体验。到目前为止（2025/08/26），free plan 的限额：</p><ul><li>1 request per second（限速每秒 1 个请求）；</li><li>2,000 requests per month（限量每个月 2000 请求）；</li></ul><p><img src="imgs/brave.png" width="450px" /></p><p>不支持私有部署网络搜索服务，MCP server 只能使用 API Key + proxy（<code>@modelcontextprotocol/server-brave-search</code>）连接官网的服务使用：<a href="https://brave.com/search/api/guides/use-with-claude-desktop-with-mcp/">brave search - guide: use with claude desktop with MCP</a>；</p><h3 id="2-Exa-ai-Web-search-for-LLMs"><a href="#2-Exa-ai-Web-search-for-LLMs" class="headerlink" title="2. Exa.ai: Web search for LLMs"></a>2. Exa.ai: Web search for LLMs</h3><p>它自称为为 AI 构建的搜索引擎，而不是为人类构建的，类似于 AI 版的Google。创始人 Jeff Wang 和 Will Bryk 认为，Google 为人类提供的服务，他们希望通过 Exa 为 AI 提供类似的功能。</p><p>因此，与 Perplexity 相比，Exa 更像是面向 B 端企业和开发者的 AI 研究工具，而非传统搜索引擎。</p><p>可以进行的功能：</p><ul><li>网页搜索</li><li>网页爬虫</li><li>AI 生成问答、Agent Research；</li><li>Websets 表格整理总结</li></ul><p>必须注册才能使用功能。比较尴尬的是 Exa 免费限额非常少：</p><ul><li>仅 1000 credits（价值 10 美刀），后面不会免费补充，需要手动购买；</li><li>每 1000 次请求消耗 5 ~ 25 美刀不等；</li><li>限制 Websets 结果 25 条；</li></ul><p>果然是面向 B 端的，价格土豪随意~</p><p><img src="imgs/exa.png" width="750px" /></p><p>不支持私有部署搜索服务，MCP server 使用类似 brave search：<a href="https://github.com/exa-labs/exa-mcp-server">Exa MCP server</a>；</p><h3 id="3-Firecrawl"><a href="#3-Firecrawl" class="headerlink" title="3. Firecrawl"></a>3. Firecrawl</h3><p>开源的 web data API，可以将爬取的网页转为 markdown 方便 AI 使用。</p><p>除了开源版，该公司提供了 cloud 版，具有比开源版更多的服务支持：</p><p><img src="imgs/open-source-cloud.png" /></p><p>可以进行的功能：</p><ul><li>网页抓取和爬取 Scrape &amp; Crawl：抓取一个 URL 并以 LLM 可读格式获取其内容（Markdown、通过 <a href="https://github.com/firecrawl/firecrawl#llm-extraction-beta">LLM Extract</a> 提取的结构化数据、截图、HTML）；</li><li>映射 Map：输入网站地址即可获取该网站的所有 URL，速度极快；</li><li>网页搜索 Web Search：搜索网络并从结果中获取完整内容；</li><li>AI 提取 LLM Extract：使用 AI 从单个页面、多个页面或整个网站中获取结构化数据。</li></ul><p>free plan 也比较少：</p><ul><li>500 credits，每次爬取/处理 1 个页面消耗一个 credit，后面不会免费补充；</li><li>支持 2 个并发请求；</li><li>较低的 rate limit；</li></ul><p><img src="imgs/firecrawl.png" width="550px" /></p><p>不过好在是<a href="https://github.com/firecrawl/firecrawl">开源</a>的，因此允许私有部署搜索服务：<a href="https://docs.firecrawl.dev/contributing/self-host">self-hosting</a>，你可能需要自己准备 redis、supabase 数据库。</p><p>和上面的几个方案一样，有配套的 <a href="https://docs.firecrawl.dev/mcp-server">MCP Server</a>；</p><h3 id="4-Travily"><a href="#4-Travily" class="headerlink" title="4. Travily"></a>4. Travily</h3><p>定位和上述几款都很像，号称 “Connect Your Agent to the Web”。</p><p>主要功能就是有语义的 web search、一般的 web access、research、extract；</p><p>学生认证免费（不过麻烦），free plan 的限额：每月 1000 次搜索机会；</p><p><img src="./imgs/tavily.png" width="550px" /></p><p>不支持私有部署搜索服务，有配套的 <a href="https://docs.tavily.com/documentation/mcp">MCP Server</a>；</p><h3 id="5-Jina"><a href="#5-Jina" class="headerlink" title="5. Jina"></a>5. Jina</h3><p>定位是 Agent Search Foundation，支持功能：</p><ul><li>Reader 爬取和处理 URL 指向页面的内容、Web Search 网络搜索、SERP；</li><li>Embedding 嵌入向量计算；</li><li>Reranker 网页/数据相关性排序；</li><li>DeepSearch、Classifier…</li></ul><p>允许不使用 API Key，一个 IP 限额：</p><ul><li>仅能使用 Reader 爬取指定 URL 页面信息，转成 LLM 易读的数据；</li><li>限速 20 RPM；</li></ul><p>free plan 限额：</p><ul><li>一千万 tokens，后面不会免费补充；</li><li>全功能开放，限速个人够用；</li></ul><p><img src="imgs/jina.png" width="650px" /></p><p>不支持私有部署搜索服务，有配套的 <a href="https://github.com/jina-ai/MCP">MCP Server</a>，亮点是支持学术搜索，例如爬取 Arxiv 上的文献。</p><h3 id="6-SearXNG"><a href="#6-SearXNG" class="headerlink" title="6. SearXNG"></a>6. SearXNG</h3><p>开源，定位是一个 <a href="https://en.wikipedia.org/wiki/Metasearch_engine">Metasearch engine</a>，综合各个搜索引擎信息的搜索引擎。</p><p>主要功能就只是对传统搜索引擎（bing/duckduckgo/google 等）做整合搜索。团队提供了很多个<a href="https://searx.space/">实例</a>可以免费使用，只要被请求方能够承受就行。</p><p>不提供账户/ API Key；</p><p>开源因此允许私有部署 <a href="https://docs.searxng.org/admin/installation.html#installation">self-hosting</a> 搜索服务，MCP server 是社区维护的。</p><h3 id="7-Bright-Data"><a href="#7-Bright-Data" class="headerlink" title="7. Bright Data"></a>7. Bright Data</h3><p>这家公司专门做反反爬措施的，号称能破除互联网上任何检测 AI 机器人的选项卡。主要业务是爬虫、网页数据收集，包括了 web search 和 scrape。</p><p>免费额度：</p><ul><li>5000 次请求（价值 $7.5），后面不会补充；</li><li>仅限 Web unlocker, browser API；</li></ul><p>注册时需要写不少于 50 词的小作文，以及各种信息。尝试了免费版的发现搜索结果都返回空值？之后再试试。</p><p>有配套 <a href="https://docs.brightdata.com/mcp-server/overview">MCP server</a>；</p><h2 id="开发者维护的-MCP-servers"><a href="#开发者维护的-MCP-servers" class="headerlink" title="开发者维护的 MCP servers"></a>开发者维护的 MCP servers</h2><ul><li><a href="https://github.com/mrkrsl/web-search-mcp">mrkrsl - web-search-mcp</a>：提供 3 个工具：browser-based <strong>bing</strong>、browser-based <strong>brave search</strong>、duckduckgo；<ul><li>前两个是用浏览器自动化工具 <code>playwright</code> 模拟用户访问 bing / 使用 brave search（需要已经安装 firefox/chromium 内核），因此不需要 API Key；</li><li>duckduckgo API 本身不需要 API Key，但搜索结果有限；</li></ul></li><li><a href="https://github.com/Aas-ee/open-webSearch">Aas-ee - Open-WebSearch MCP Server</a>：偏向国内用户搜索喜好，支持 bing、baidu、CSDN、duckduckgo、exa、brave、juejin、github READMEs，不需要 API Key；<ul><li>实际测试下来感受一般，搜索结果很少，并且大多是 duckduckgo 贡献的，其他的经常请求错误/返回空数据；</li></ul></li><li><a href="https://github.com/yokingma/one-search-mcp">yokingma - one-search-mcp</a>：<ul><li>支持 SearXNG, Firecrawl, Tavily, DuckDuckGo, Bing：可以选择给 SearchXNG / Tavily / Firecrawl 访问时使用 API Key。其他引擎不需要 API Key；</li><li>支持本地浏览器模拟访问：<strong>Bing</strong>, <strong>Google</strong>, <strong>Baidu</strong>, <strong>Sogou</strong>，需要已安装 chromium 内核；</li></ul></li></ul><p>其他测试的 MCP servers 质量一般，就不放上来了。有新的/好用的欢迎补充。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网络搜索的 MCP servers 由两个部分组成：一个是网页搜索服务，另一个是包装成符合 MCP 规范的 MCP server（供 Agent 使用）。&lt;/p&gt;
&lt;p&gt;考虑到现在 MCP servers 相当繁多，而且大多生命短暂（无人维护），因此想让 AI 用上网络搜索</summary>
      
    
    
    
    <category term="chat" scheme="https://blog.sjtuxhw.top/categories/chat/"/>
    
    
    <category term="AI" scheme="https://blog.sjtuxhw.top/tags/AI/"/>
    
    <category term="Agent" scheme="https://blog.sjtuxhw.top/tags/Agent/"/>
    
    <category term="MCP" scheme="https://blog.sjtuxhw.top/tags/MCP/"/>
    
  </entry>
  
  <entry>
    <title>CCF 2025 会议笔记</title>
    <link href="https://blog.sjtuxhw.top/technical/2025-ccf/"/>
    <id>https://blog.sjtuxhw.top/technical/2025-ccf/</id>
    <published>2025-08-03T03:36:45.000Z</published>
    <updated>2025-08-03T04:35:59.793Z</updated>
    
    <content type="html"><![CDATA[<h2 id="主论坛"><a href="#主论坛" class="headerlink" title="主论坛"></a>主论坛</h2><p>主论坛开幕主要介绍了开源社区的背景现状以及院士自己的工作，我选个比较感兴趣的记录一下。</p><p>郑纬民院士介绍的它们团队的成果 <strong>Mooncake</strong> 和 <strong>KTransformer</strong>；</p><h3 id="1-Mooncake"><a href="#1-Mooncake" class="headerlink" title="1. Mooncake"></a>1. Mooncake</h3><p>这个工作的核心是解决<strong>大规模语言模型（LLM）在线服务（尤其是长上下文场景）中的效率、成本和延迟问题</strong>，特别是围绕 <strong>KV Cache</strong> 的管理优化展开。简言之：<strong><u>用更多的存储资源（CPU内存、SSD、网络带宽）来换取更少的昂贵计算资源（GPU）消耗，从而显著提升服务吞吐量和用户体验</u></strong>。</p><p>郑院士向我们介绍 kimi AI 服务器其实更新迭代过 5 次，但每次都会崩溃，主要原因是大量用户在短时间内问了很复杂的问题，或者上传了很长的文档来分析，导致的显存紧缺。LLM 在回答你的问题之前，需要先“理解”你输入的所有内容（Prefill 阶段），然后才能开始一个字一个字地生成回答（Decoding 阶段）。这个过程中产生了一个关键的东西：<strong>KV Cache (Key-Value Cache)</strong>。</p><blockquote><p>[!NOTE]</p><p>KV Cache 是 LLM（尤其是 Transformer 架构）在处理输入（Prompt）时的计算中间结果（Key / Value vector）。类似 “记忆快照”，KV Cache 存放模型对当前上下文的理解的状态。在 Decode 阶段生成后续 Token 时，复用这个 KV Cache 可以避免重新计算之前的整个输入，<strong>极大地节省计算量</strong>。</p></blockquote><p>以前 Kimi AI 使用传统方法的痛点：</p><ol><li><strong>容量小，命中率低：</strong> 传统做法是把 KV Cache 存在 GPU 自己昂贵的高速显存（HBM）或者最多是本机 CPU 内存（DRAM）里。显存容量非常有限（比如  A800 80GB），DRAM 容量大些但也有限（比如单机 1 TB）。对于动辄数万甚至数十万 Token 的长上下文输入，单节点能缓存的 KV  Cache 非常有限，导致缓存命中率低，很多请求不得不进行完整的、耗时的预填充计算。</li><li><strong>预填充和解码相互干扰：</strong> 预填充阶段（计算密集型）和解码阶段（内存访问密集型）通常部署在同一个 GPU 节点上。当处理长上下文请求进行大规模预填充时，会严重抢占解码资源，导致生成 Token 的速度变慢（TBT 增加），用户体验变差。</li><li><strong>资源利用率不均：</strong> GPU 集群中最贵的 GPU 资源只能被绑定在特定的任务上，而集群中相对廉价的资源（如 CPU、DRAM、SSD、网络带宽）不能被充分利用。</li><li><strong>难以满足在线的延迟要求（SLO）：</strong> <ul><li><strong>首 Token 时间（TTFT）：</strong> 用户发出请求到收到第一个回复 Token 的时间。一般瓶颈在 Long Context Prefill 的计算过程。</li><li><strong>Token 间时间（TBT）：</strong> 两个连续回复 Token 之间的时间间隔。瓶颈是 解码阶段的干扰和 KV Cache Load。</li></ul></li><li><strong>调度不智能：</strong> 简单的负载均衡调度（如随机或看队列长度）没有考虑 KV Cache 重用，可能错过节省计算的机会 / 导致某些节点过载。</li></ol><blockquote><p>Deepseek 总结：</p><p>在长上下文、高并发的 LLM 服务场景下，传统架构受限于<strong>本地化、小容量</strong>的 KV Cache 存储方式以及<strong>预填充-解码耦合</strong>的设计，导致<strong>GPU 计算资源浪费严重（重复计算多）、整体服务吞吐量受限、难以同时满足 TTFT 和 TBT 的严苛延迟要求（SLO），服务成本高昂。</strong></p></blockquote><p>而 Mooncake 这个工作提出并实现了一套<strong>以 KV Cache 为中心的解耦架构（KVCache-centric Disaggregated Architecture）</strong>。主要是通过将集群节点明确分为独立的 Prefill Pool 和 Decode Pool，实现预填充与解码的物理解耦，从根本上避免长预填充任务对解码任务的干扰。</p><p>重要的创新点，也是这篇工作的核心设计之一：<strong><u>构建全局共享的分布式 KV Cache 池</u></strong>。这一机制不再将 KV Cache 局限于单个节点的 HBM 或 DRAM，而是整合整个 GPU  集群中所有节点（包括预填充节点和解码节点）的 CPU、DRAM、SSD 及高速 RDMA 网络资源，形成一个大容量的共享分布式存储池（<strong>Mooncake Store</strong>），其容量可轻松达到 PB 级别。</p><p>为实现 KV Cache 的高效调度，这个工作引入了<strong>全局调度器 Conductor</strong>。它不再仅关注节点负载，而是深度感知 KV Cache  的位置、热度和匹配度，以此做出更优决策：尽可能将新请求路由到拥有其提示词最长匹配前缀 KV Cache  的预填充节点，最大化缓存重用以减少计算量；同时在 “缓存重用收益” 与  “调度延迟” 间 trade-off，兼顾节点负载和网络状况；并动态管理缓存副本（将热点缓存复制到多个节点）和淘汰策略（LRU）。</p><p>Mooncake 设计了一个<strong>高效的 KV Cache 传输引擎</strong>来支撑全局缓存共享，其核心目标是充分发挥高速 RDMA 网络（如 8x400Gbps）的带宽，使传输 KV Cache  的开销小于重新计算的开销（满足论文中的不等式）。关键技术包括：拓扑感知路径选择，即理解服务器内部 NUMA、PCIe  拓扑，选择最优本地和远端网卡传输数据，避免内部总线瓶颈；细粒度切片与多路径并发，将大块 KV Cache 切分为小片（如  16KB），通过不同路径（不同网卡）并发传输，充分利用带宽；端点池化与故障处理，高效管理 RDMA 连接（Queue Pairs），采用类似  SIEVE 的算法管理连接池，并能智能处理网络故障（如网卡暂时不可用），自动切换路径。</p><p>针对超长上下文（如 &gt; 128k tokens）的处理，Mooncake 在 prefill pool 中引入<strong>分块流水线并行（Chunked Pipeline Parallelism, CPP）</strong>。具体而言，将输入序列切分成块（Chunk），由多个预填充节点组成流水线组，并行处理同一请求的不同数据块。相比跨节点的序列并行（SP），CPP  仅在流水线阶段边界通信，通信量显著减少，更易与计算重叠，对网络带宽压力小，且天然适应不同长度的请求，无需频繁动态调整节点分组（避免了弹性并行的复杂性）。</p><h3 id="2-KTransformer"><a href="#2-KTransformer" class="headerlink" title="2. KTransformer"></a>2. KTransformer</h3><p>这个工作是在工程上的优化（对 transformers 框架），目标是增强 Hugging Face Transformers 的使用体验，尤其聚焦于解决长上下文处理、资源受限环境下的部署效率，以及异构计算场景中的性能瓶颈等问题。</p><p>其架构的核心是一个基于模板的注入框架，设计上以可扩展性为核心。用户只需通过一行代码，就能将优化模块注入原有流程，替换原始的 PyTorch 模块为经过优化的变体。感觉有几个创新点，一个是<strong><u>注入框架的灵活性</u></strong>，让多种优化策略的组合变得简单，无需大幅修改原有代码；其次是 <strong><u>KVCache  管理的精细化</u></strong>，通过块划分和动态选择机制，在保证推理质量的前提下，将长上下文处理速度提升数倍甚至十数倍（如在 1M tokens 的  “针在干草堆” 测试中，速度比 llama.cpp 快近 10 倍）；此外，它特别关注资源受限场景，支持 <strong><u>GPU-CPU-disk 三级前缀缓存复用</u></strong>，降低 VRAM 需求（如将 DeepseekV2 的 VRAM 占用从 21G 降至 11G），同时兼容 Intel  Arc、AMD ROCm 等多种硬件，扩展了适用范围。</p><hr><p>鄂维南院士指出的 “AI 正经历从 model-centric 到 data-centric” 的转变比较有意思。笔者从发言内容来看他以前应该就是从事大数据处理相关的工作。</p><p>数据基础设施建设是下一个关键点，“数据产线” 将称为人工智能领域最主要的业态。</p><p>主要是数据采集、生成、AI-ready 数据的生产，等等。</p><hr><p>宇树科技创始人王兴兴主要介绍了几款吸引人眼球的机器人样式（真的很震撼！），笔者认为如果能大规模应用到实际的场景（例如家居/搜救/探索等情形），而不只是表演的领域，就更好了。</p><p>此外他同时表达了对 VLA 能否真正实现广泛的泛化性，以及能否生成任意的机器人动作表达了担忧。笔者本人其实也对 VLA 持观望态度，RL/VLA model 能否充分展现 scaling law 也是未知数。不过王兴兴表示，在未来的 10 年左右的时间，有能力生成任意动作序列的机器人就会出现，届时机器人才有真正参与劳动的可能。</p><h3 id="高峰论坛：开源发展未来和现状"><a href="#高峰论坛：开源发展未来和现状" class="headerlink" title="高峰论坛：开源发展未来和现状"></a>高峰论坛：开源发展未来和现状</h3><ol><li><strong>陈左宁院士</strong>指出，国内开源实践有时存在<strong>功利性过强</strong>的问题，尚未真正形成协同创新的文化与心态，这与社会文化背景有一定关联。</li><li><strong>梅宏院士</strong>强调，当前工业界<strong>协同精神不足</strong>制约了开源发展。开源源于自由软件的理想主义奉献，但仅靠理想主义难以支撑其壮大，<strong>市场机制的介入不可或缺</strong>。梅院士更犀利地指出：企业追求盈利本属正常，但<strong>不应动辄标榜公益，更不可借此误导公众或政策制定</strong>。</li><li><strong>吕建院士</strong>提醒企业需脚踏实地，<strong>少谈空泛的“技术引领”或“世界一流”口号</strong>。他认为，企业应首先与国家发展同频共振，国家强盛才是企业实现引领的坚实基础。</li><li><strong>王怀民院士</strong>分析了国内开源托管平台现状，认为虽数量众多但<strong>尚缺全球影响力</strong>。他相信未来必将诞生具有世界级影响力的平台，但<strong>仍需持续积累和努力</strong>。</li></ol><h2 id="分论坛：具身智能与机器人"><a href="#分论坛：具身智能与机器人" class="headerlink" title="分论坛：具身智能与机器人"></a>分论坛：具身智能与机器人</h2><p>笔者主要参与的是具身智能与机器人论坛，下面是一些笔记。</p><h3 id="马道林：面向机器人精细操作的多模态触觉传感与感知"><a href="#马道林：面向机器人精细操作的多模态触觉传感与感知" class="headerlink" title="马道林：面向机器人精细操作的多模态触觉传感与感知"></a>马道林：面向机器人精细操作的多模态触觉传感与感知</h3><p>通用机器人：行动能力 + 操作能力，<strong><u>其中精细操作能力是关键</u></strong>（家庭物理劳作）。</p><blockquote><p>人手为什么能够感受和完成精细操作？</p><p>“动觉”、“滑觉”、“力觉” -&gt; 大脑感受：触觉的闭环控制。</p></blockquote><p>高集成度触觉传感器、多维度感知能力、触觉闭环控制方法。</p><ul><li>触觉传感器硬件设计：把脉、抓麻将的示例；</li><li>触觉感知算法；</li></ul><p>几个前沿的探索：</p><ul><li>多指协同感知（不靠视觉，只靠触觉）：<strong>运动跟踪、间接识别</strong>；</li><li>外部接触感知（通过工具与目标物接触，因为有些时候没法在目标物上部署传感器）：基于运动学约束的间接接触识别，让机器手识别物体和环境之间的间接接触界面；</li><li>多传感器间自约束：接触界面的排他性（同时刻无法让多个传感器同时放在同一个位置）MTSC 约束方程解决。</li><li>触觉数据生成：多模态触觉仿真器（减小 sim2real gap）。具身智能领域很难获取数据。</li></ul><p>高精度触觉感知能够赋能精细操作！</p><p>观众提问：触觉传感和 VLA （别的信息层面）的结合？VTLA 是一个可能的方向。</p><h3 id="郝孝帅：大模型驱动的具身智能-规划、操纵、导航的探索"><a href="#郝孝帅：大模型驱动的具身智能-规划、操纵、导航的探索" class="headerlink" title="郝孝帅：大模型驱动的具身智能 - 规划、操纵、导航的探索"></a>郝孝帅：大模型驱动的具身智能 - 规划、操纵、导航的探索</h3><p>郝老师主要介绍了他们团队的 3 个工作。</p><h4 id="研究趋势"><a href="#研究趋势" class="headerlink" title="研究趋势"></a>研究趋势</h4><p><img src="imgs/trending.jpg" width="450px" /></p><p><img src="imgs/embodied-ais.jpg" width="450px" /></p><h4 id="1-具身智能的大脑大模型：RobotBrain"><a href="#1-具身智能的大脑大模型：RobotBrain" class="headerlink" title="1. 具身智能的大脑大模型：RobotBrain"></a>1. 具身智能的大脑大模型：RobotBrain</h4><p>建议阅读工作：<strong><a href="https://arxiv.org/abs/2502.21257">RobotBrain</a>（CVPR-2025）</strong>，以及 <a href="https://github.com/FlagOpen/RoboBrain">项目</a>；</p><p><strong>具身智能 Agent 的两个趋势</strong>：</p><ul><li>端到端 VLA 模型：OpenVLA、Pi0、RDT、RT 系列、GR 系列；</li><li>大小脑模型协同框架：RoboOS、Helix、Gemini Robotics、Rekep 等；</li></ul><p>本研究重点：开发具备跨本体、可泛化的具身智能大脑大模型；</p><hr><p>研究为什么选择大小脑模型协同框架的技术路线？</p><p>端到端模型虽决策高效，但泛化性、扩展性受限，受制于<strong>环境交互与硬件适配</strong>，难以适应多样化场景（归根结底还是端到端可解释性太差了，出了新的问题还得重新 train）。而模块化的大小脑协同架构在实验上更具强泛化、可解释性的优势。</p><ol><li>模块化：大小脑协同框架赋予具身智能体模块化优势，可扩展架构、高效开发、强适应性；</li><li>泛化性：基于 VLM 的大脑具备丰富多模态认知能力，且不受小脑模型影响；</li><li>可解释性：决策过程透明，提高人机协同效率；</li></ol><hr><p>具身智能大脑模型为什么重要？</p><p>通过多模态感知、抽象指令理解能力，输出原子任务规划（planning）、<strong>可操作区域（affordance，感觉是这个工作的比较大的创新点）</strong>、操作轨迹（trajectory），协同小脑模型实现 “感知-认知-决策-控制” 的全链路闭环。</p><h4 id="2-RoboOS：跨本体具身大小脑协同框架"><a href="#2-RoboOS：跨本体具身大小脑协同框架" class="headerlink" title="2. RoboOS：跨本体具身大小脑协同框架"></a>2. RoboOS：跨本体具身大小脑协同框架</h4><p>建议阅读工作：<a href="https://arxiv.org/abs/2505.03673">RoboOS</a>，以及 <a href="https://github.com/FlagOpen/RoboOS">项目</a>；</p><p>类似快慢系统：</p><p>云端大脑模型 RobotBrain（慢系统）进行全局感知与决策；</p><ul><li>时空感知：融合 3D 场景空间关系重建 + 历史状态追踪感知，构建动态时空认知；</li><li>规划指导：群体智能调度（复制任务协同 planning），细粒度动作指导生成；</li><li>反馈纠错（action-level -&gt; tool-level -&gt; task-level，三级动态重规划）；</li></ul><p>小脑模型做具体任务：</p><ul><li><p>Affordance（本工作的创新点之一）：在哪里抓取 + 怎样抓取？抓取问题：几何驱动 -&gt; 语义驱动；</p><blockquote><p>机器人需要知道 “抓起一个水壶” 意味着需要到 “有把手形状的区域” 去，以 “夹爪环绕把手” 的方式（而不是其他握法）来抓取。</p><p>这个 affordance 就是在告诉模型这个信息；</p></blockquote></li><li><p>AffordGrasp：多模态大模型推理 Affordance 用于任务导向的抓取；</p></li><li><p>Embodied Spatial Affordance（ESA，融入空间推理）：一个将物体 affordance、空闲空间 affordance、以及空间关系推理相结合的大规模数据集，为具身导航/操作任务提供了空间 affordance 的学习能力；</p><ul><li>物体 affordance：指导机器人进行 object navigation（解释参见下文）和抓取；</li><li>空闲空间 affordance：指导机器人导航到指定空闲区域、指示操作中物体放置位置；</li></ul></li></ul><h4 id="3-具身视觉导航"><a href="#3-具身视觉导航" class="headerlink" title="3. 具身视觉导航"></a>3. 具身视觉导航</h4><p>目前背景/研究方向两类：Object Navigation、Vision Language Navigation（VLN）；</p><p>MapNav：纯视觉端到端 VLN 方案。</p><p>提出了 Annotated Semantic Map（ASM）作为新型历史表征，取代历史帧，从而减少存储机器人观测和处理历史帧所需的存储和计算开销。能让我们充分借助 VLM 提升 VLN 性能；</p><p>如何证明 ASM 方案让 VLM 增进了对地图的语义理解？</p><ul><li>定性：实验证明 ASM 让 VLM 对智能体先前的导航记忆拥有了结构化的认识（例如见过的物体、障碍物位置、过去的轨迹和自身的位置等）；</li><li>定量分析：将 VLM 对图片的注意力可视化，ASM 能够成功地将 VLM 的注意力吸引到 labels 上；</li></ul><hr><p>Nav3A：理解人类高级指令、长程导航；</p><p>Nav3A 的方法概述：</p><ul><li>全局策略：使用 ReasonVLM 推理来解析高级指令（“晾衣服” -&gt; 衣架），并基于全局 3D 场景理解导航到最可能的区域（阳台）；</li><li>局部策略：使用 PointVLM 在不同的航点进行探索，并使用基于空间感知可供性理解的 NaviAfford 模型进行精确物体定位，最终找到目标物体（衣架）；</li></ul><h3 id="穆尧：生成式大模型驱动的具身智能大规模高质量数据合成开源平台"><a href="#穆尧：生成式大模型驱动的具身智能大规模高质量数据合成开源平台" class="headerlink" title="穆尧：生成式大模型驱动的具身智能大规模高质量数据合成开源平台"></a>穆尧：生成式大模型驱动的具身智能大规模高质量数据合成开源平台</h3><p>建议阅读工作：<a href="https://arxiv.org/abs/2506.18088">RoboTwin 2.0</a>，以及 <a href="https://github.com/RoboTwin-Platform/RoboTwin">项目</a>；</p><p>生成模拟数据的两个关键问题：</p><ul><li>Can the visual gap be bridged by generative models to enable zero-shot policy transfer to the real world?</li><li>Can physical simulation overcome challenging processes, such as screw thread simulation?</li></ul><h3 id="边旭：具身智能机器人在工业应用中的场景探索"><a href="#边旭：具身智能机器人在工业应用中的场景探索" class="headerlink" title="边旭：具身智能机器人在工业应用中的场景探索"></a>边旭：具身智能机器人在工业应用中的场景探索</h3><p>笔者认为这个汇报是最切应用实际、最接地气的，也很符合我对 “目前具身智能能否真的用到实际生活中” 的想象。主要就是这个 PPT 的内容：</p><p><img src="imgs/ppt.jpg" width="450px" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;主论坛&quot;&gt;&lt;a href=&quot;#主论坛&quot; class=&quot;headerlink&quot; title=&quot;主论坛&quot;&gt;&lt;/a&gt;主论坛&lt;/h2&gt;&lt;p&gt;主论坛开幕主要介绍了开源社区的背景现状以及院士自己的工作，我选个比较感兴趣的记录一下。&lt;/p&gt;
&lt;p&gt;郑纬民院士介绍的它们团队的成果</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="Conference" scheme="https://blog.sjtuxhw.top/tags/Conference/"/>
    
    <category term="AI" scheme="https://blog.sjtuxhw.top/tags/AI/"/>
    
    <category term="CCF" scheme="https://blog.sjtuxhw.top/tags/CCF/"/>
    
    <category term="Robot" scheme="https://blog.sjtuxhw.top/tags/Robot/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 论文精读 + 代码实现</title>
    <link href="https://blog.sjtuxhw.top/technical/transformer/"/>
    <id>https://blog.sjtuxhw.top/technical/transformer/</id>
    <published>2025-07-20T15:15:10.000Z</published>
    <updated>2025-07-30T13:14:09.876Z</updated>
    
    <content type="html"><![CDATA[<p>笔记温习一下经典的 Transformer 架构的论文，结合<a href="#代码实现及详解">代码实现和解读</a>。</p><span id="more"></span><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><ul><li><p>循环神经网络、卷积神经网络的演化过程、结构、代表性的模型；</p></li><li><p>传统的注意力机制（attention）已经在很多场合下成为序列/转录模型的不可分割的一部分，因为无论两个词语语义的依赖在输入/输出序列中距离多远，都能建模依赖关系。但是这种传统的注意力机制仍然没有用在 recurrent 网络中。</p></li><li><p>自注意力机制（self-attention）是通过关联单个序列中的的不同位置，来计算这个序列的 hidden representation。自注意力机制在此前被成功应用与阅读理解、抽象总结等任务中；</p></li><li><p>另外有工作表明，基于循环注意力机制（recurrent attention）的端到端记忆网络（end-to-end memory networks），它并没有采用传统 RNN 的序列对齐循环（sequence-aligned recurrence）的计算方法，仍然能在简单语言问答、语言建模等任务上取得比较好的效果；</p><blockquote><p>循环注意力机制：一种将注意力机制与循环神经网络（RNN）相结合的技术，常见的有 Recurrent Attention Model（RAM）和 Recurrent Attention Convolutional Neural Network（RA - CNN）等模型；</p><p>序列对齐循环（sequence-aligned recurrence）：是一种与循环神经网络（RNN）相关的计算方式。通常沿输入和输出序列的符号位置进行因子计算（Recurrent models typically factor computation along the symbol positions of the input and output sequences），将位置与计算时间中的步骤对齐，根据前一个隐藏状态  $h_{t-1}$ 和位置 $t$  的输入生成新的隐藏状态 $h_{t}$。这种计算方式具有内在的序列性，导致训练示例中的并行化难以实现，在处理长序列时，由于内存限制会影响跨示例的批处理效率。</p></blockquote></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li><p>RNN 和 GRU/LSTM 之类的模型已经在语言序列建模（尤其是序列到序列，或者称为 “转录模型”，transduction models）、机器翻译等领域达到了 SOTA 级别的效果；</p></li><li><p>Recurrent 类型的模型由于采用的是 sequence-aligned recurrence 的计算方法，极大阻碍了计算的并行化，尤其是在序列很长的情况下；</p><ul><li>虽然目前的工作进行了 factorization tricks 以及条件计算（后者还增强了模型的 performance）来优化性能，但是 Recurrent 网络串行计算的根源问题仍然无法解决；</li></ul></li><li><p>CNN 架构的模型如 ByteNet/ConvS2S 等使用 CNN 作为 basic building block，可以并行计算所有输入输出位置的 hidden representations 数据，但是输入输出间任意位置需要进行的计算量会随着位置距离增长而增长（ByteNet 是线性的，ConvS2S 是对数的）。</p><p>但这也会导致模型难以学习到较远距离的两个位置之间的依赖关系。</p></li></ul><p>基于上述背景，这个工作提出了 Transformer 模型架构，<strong>直接避开了 sequence-aligned recurrence 的做法</strong>，仅依靠注意力机制来构建一个输入/输出间的全局依赖。</p><p>目前 Transformer 也是第一个仅依靠自注意力机制（而不是使用 sequence-aligned recurrence 或者卷积的方法）来计算输入输出序列 representations 的转录模型。</p><p>这个架构的重要好处之一是可以尽可能地利用并行化的计算资源。另外 Transformer 还解决了 CNN 模型在解决序列长距离依赖时的高额时间开销问题：常数时间！</p><blockquote><p>但同时由于引入了平均注意力加权的位置参数（averaging attention-weighted positions），代价是 reduced effective resolution（丢失有效分辨率）。本文通过引入<strong><u>多头注意力机制</u></strong>来缓解这一点。</p></blockquote><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="imgs/transformer.png" width="320px" /></p><p>研究表明大部分的有竞争力的序列转录模型都有一个 encoder-decoder 结构。Transformer 也不例外：</p><ul><li>encoder（上图左框）将符号表示的输入序列 $(x_1,x_2,\ldots,x_n)$ 映射到序列连续表示（a sequence of continuous representations）$z=(z_1,\ldots,z_n)$；</li><li>给定序列的连续表示 $z$，decoder（上图右框）就能每次生成一个输出序列 $(y_1,\ldots,y_m)$ 的一个元素，并且每一步模型都是<strong><u>自回归的</u></strong>（self-regressive，指前面步骤中生成的符号会作为后面序列生成的额外的输入）。</li></ul><p>Transformer 总体就是遵循上述的架构设计，使用堆叠的 self-attention 块、由全连接层组成的 encoder 和 decoder，搭建出上图的结构。</p><h3 id="Encoder-和-Decoder-设计"><a href="#Encoder-和-Decoder-设计" class="headerlink" title="Encoder 和 Decoder 设计"></a>Encoder 和 Decoder 设计</h3><p>encoder 由 $N=6$ 的完全相同的层组成（参见上图示意），每个 layer 有两个 sub-layers：多头注意力机制，以及逐位的前馈全连接网络（position-wise fully connected feed-forward network）。</p><p>每个 sub-layers 周围引入残差连接块、正则化层，即每个 sub-layers 输出为 $\text{LayerNorm}(x+\text{Sublayer(x)})$，其中 $\text{Sublayer}$ 是 sub-layers 中实现的函数。</p><blockquote><p>为了容易实现残差连接块，模型的所有 sub-layers，包括 embedding layers 的输出维度都是 $d=512$；</p></blockquote><p>decoder 同样由 $N=6$ 的完全相同的层堆叠而成。不过其中的 sub-layers 有 3 个，除了 encoder 中有的两个以外，又加了一个 masked 多头注意力层（以及同样的残差连接-正则化层），用于处理输入的之前的输出序列（自回归嘛）。</p><blockquote><p>为什么处理输入的 output embedding 的多头注意力层有 mask 呢？</p><p>主要考虑到<strong>防止模型在训练时“作弊”（Peeking Ahead），即防止模型利用当前要预测位置之后的信息（未来信息）来预测当前的位置</strong>。</p><p>这个问题就像把 validation set 直接作为 training set 一样，这会严重影响训练效果。</p><p>目的就是让模型在预测 $y_t$ 时不会“看到” $y_{t+1}$ 以及以后的信息。</p></blockquote><h3 id="Attention-设计以及创新点"><a href="#Attention-设计以及创新点" class="headerlink" title="Attention 设计以及创新点"></a>Attention 设计以及创新点</h3><p>一个注意力函数实际上能被描述为 <code>&#123;a query, a set&#123;k: v&#125;&#125; -&gt; an output</code> 的映射。</p><p>其中查询（query）、键（key）、值（value）、输出（output，即注意力分数）都是向量。</p><p>而输出实质上就是值（values）的加权和，其中这些“权重” 是由一个 “适配性函数”（compatibility function）计算出的 <strong><u>这个查询 query 与对应键 key 的匹配的程度</u></strong>。</p><p>文章中介绍的这个算法就是 QKV 算法，注意力机制的<strong><u>一种高效的实现形式</u></strong>。</p><blockquote><p>[!IMPORTANT]</p><p>读者这里可能会好奇，为什么将 query 和 key 的匹配程度作为权重加权到 value 上就能得到注意力分数，且这个做法是有效的？也就是说：为什么 QKV 算法、以及注意力机制是有效的？</p><p>其实关键在于它模拟了人类认知中一个核心过程：<strong>选择性聚焦</strong>。它允许模型在处理信息时，<strong>动态地、有选择性地</strong>将有限的“认知资源”集中在输入信息中最相关、最重要的部分上，而忽略或弱化不相关的部分。</p><p>因此说，注意力机制的核心思想就是“动态、内容相关的信息选择”，就是让模型具备这种<strong>动态聚焦</strong>的能力。</p><p>它让模型在处理某个特定元素（<code>Query</code>）时，能够“有意识地”去“看”其他元素（<code>Key</code>），并根据它们与当前元素的相关性（<code>Query-Key</code> 匹配度）来决定从这些元素中提取多少信息（加权 <code>Value</code>）。</p><p>它也因此突破了固定编码的局限。做个比较：</p><ul><li><p>传统的神经网络层（如全连接层、CNN、RNN）在编码一个元素（如一个词、一个像素）时，主要依赖于其<strong>固定的上下文窗口</strong>或<strong>预定义的位置关系</strong>（如 CNN 的卷积核、RNN 的时序依赖）。</p><p>这种固定方式在处理长距离依赖、理解复杂关系或需要<strong>全局上下文</strong>信息时效率低下或效果不佳。</p></li><li><p>相比之下，注意力机制允许模型在处理序列中任何一个位置时，都能<strong>直接访问并评估序列中所有其他位置的信息</strong>，并根据<strong>内容的相关性</strong>（而非固定的位置或距离）来决定依赖程度。</p></li></ul><p>上述思考的有效性也被实验结果所证明。</p></blockquote><p>根据我们上面的注意力机制的定义，我们只需要设计一个 compatibility function 不就能完成注意力的计算了吗！我们记 compatibility function 为 $f_c$，那么</p><script type="math/tex; mode=display">\text{attention score}=f_c(q, k)\cdot v</script><p>这里 $f_c$ 算出的结果是一个关系矩阵 $R_{ij}=(r)_{ij}$ 表示 $q_i$ 与 $k_j$ 的匹配程度，最后矩阵向量点积表示求加权和，得到对应的注意力分数。</p><p>这里因为我们想以匹配程度作为参考，给 $v$ 做个权重，因此希望满足：</p><ul><li>计算结果的元素求和为 1（<strong>归一化与概率解释性</strong>）；</li><li>并且希望<strong>显著放大</strong>最高分数与其他分数之间的<strong>相对差异</strong>实现 “强聚焦” 的效果（<strong>突出显著项与抑制不相关项</strong>）；</li><li>还希望利于神经网络的后续梯度的计算（<strong>梯度计算的优化</strong>）；</li></ul><p>因此 softmax 完美符合上述要求（本身输出归一化、可解释性强、非线性指数放大效应、容易计算导数），我们修改为下面的公式更为准确：</p><script type="math/tex; mode=display">\text{attention score}=\text{softmax}(f_c(q, k))\cdot v</script><p>我们将 $f_c(q,k)$ 称为对齐分数，它的每个元素就是对应的、未归一化的 “query 和对应 key 的匹配程度”。</p><h4 id="创新点-1：缩放点积注意力-Scaled-Dot-Product-Attention"><a href="#创新点-1：缩放点积注意力-Scaled-Dot-Product-Attention" class="headerlink" title="创新点 1：缩放点积注意力 (Scaled Dot-Product Attention)"></a>创新点 1：缩放点积注意力 (Scaled Dot-Product Attention)</h4><p>对于适配性函数的具体定义，文章介绍了一种 “缩放点积” 的定义，即 $f_c(q,k)=\dfrac{1}{\sqrt{d_k}}\cdot q^T\cdot k$)，其中$q$ 和 $k$ 向量均为 $d_k$ 维。</p><p>因为计算机中一般需要批量并行计算，因此我们一般将输入向量堆叠成矩阵，具体计算起来会比上面单个向量的计算复杂一些：输入包含同样维度 $d_k$ 的 query 和 keys 向量（分别记为 $q_i$ 和 $k_i$），以及一个维度 $d_v$ 的 values 向量 $v_i$，输出对齐分数，计算方法：将一个 $q_i$ 与所有 $k_j$ 点积，每个都除以 $\sqrt{d_k}$，得到 $q_i$ 和 $k_j$ 的对齐分数。</p><p>例如 $q_i$（第 $i$ 个 query 向量）和 $k_j$（第 $j$ 个 key 向量）的关于缩放点积的对齐分数：</p><script type="math/tex; mode=display">e_{ij}=\dfrac{q_i^T\cdot k_j}{\sqrt{d_k}}</script><p>最终注意力分数计算过程等价于下面的矩阵式：</p><script type="math/tex; mode=display">\text{Attention}(Q,K,V)=\text{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V</script><p>我们将使用 “缩放点积” 作为适配性函数的注意力机制称为 “缩放点积注意力”。</p><blockquote><p>[!NOTE]</p><p>文章中也提到，并不是一开始就知道要用缩放点积函数作为 compatibility function，作者实际上先考虑的是常用的两种函数：加性、点积（分别对应加性注意力、点积注意力）。</p><p>加性函数是使用一个含有单层隐藏层的前馈神经网络，公式（$\tanh$ 是激活函数）：</p><script type="math/tex; mode=display">f_c(q,k)=v^T\tanh(W_q\cdot q+W_k\cdot k)</script><p>虽然理论上，上述加性函数和点积函数的复杂度相当，但实际计算机计算起来点积函数的时间和空间消耗都更好一些，因为后者可以利用被高度优化的矩阵乘法计算代码。</p><p>不过文章指出，使用点积函数时，在 $d_k$ 很大的情况下，效果不如加性函数，作者推测可能是<strong><u>点积结果过大导致 Softmax 梯度消失</u></strong>，因此在点积后添加了一个 $\dfrac{1}{\sqrt{d_k}}$ 的缩放，这才提出了缩放点积。</p><p>原文：We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has  extremely small gradients [4]. To counteract this effect, we scale the dot products by $\dfrac{1}{\sqrt{d_k}}$.</p></blockquote><h4 id="创新点-2：多头注意力机制"><a href="#创新点-2：多头注意力机制" class="headerlink" title="创新点 2：多头注意力机制"></a>创新点 2：多头注意力机制</h4><p>文章注意到，与其使用单个的注意力函数来处理 $d_{\text{model}}$ 个的 $q,k,v$，不如将它们投影（线性变换）到 $h$ 个不同方向（分别是 $d_k,d_k,d_v$ 维的线性空间），然后对它们并行地求注意力分数，每个方向都能得到 $d_v$ 维输出值（注意力分数）。</p><p>作者指出这样做的作用是，<strong><u>增强模型捕捉不同子空间信息的能力</u></strong>。</p><blockquote><p>原文：Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p></blockquote><p>这样的多头注意力计算更加繁琐一些，我们直接展示结论（$d_\text{model}$ 组数据同时计算的矩阵计算式）：</p><script type="math/tex; mode=display">\begin{aligned}\text{MultiHead}(Q,K,V)&=\text{Concat}(\text{head}_1,\ldots,\text{head}_h)W^O\\\text{where head}_i&=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\end{aligned}</script><p>其中 $W_i^Q\in\mathbf{R}^{d_{\text{model}}\times d_k},\space W_i^K\in\mathbf{R}^{d_{\text{model}}\times d_k},W_i^V\in\mathbf{R}^{d_{\text{model}}\times d_v}$ 以及 $W^O\in\mathbf{R}^{hd_v\times d_\text{model}}$ 均为用于投影的超参数矩阵。</p><p>本文的工作实验时取的值 $h=8,d_k=d_v=\dfrac{1}{h}d_{\text{model}}=64$。</p><p>Transformer 架构也充分利用了注意力机制，例如：</p><ul><li><p><strong><u>Encoder 的每一个自注意力层</u></strong>；</p><ul><li><p>输入：注意这里 <code>Q = K = V = 该层前一层的输出</code>；</p><blockquote><p>queries、keys 和 value 全部是一样的，这也是为什么称它为 “自注意力”；</p></blockquote></li><li><p>目的：让输入序列中的<strong>每个词（位置）</strong> 能够关注到输入序列（自身）中<strong>所有其他词（位置）</strong>，捕捉词与词之间的依赖关系（无论距离远近）；</p></li><li><p>特点：</p><ul><li>全局上下文：每个词的表示都融合了整个输入序列的信息；</li><li>并行计算：因为 <code>Q</code>, <code>K</code>, <code>V</code> 都来自同一序列的上一层输出，且计算不依赖顺序，整个层的计算可以高度并行化；</li><li>多头注意力；</li></ul></li></ul></li><li><p><strong><u>Decoder 的掩码自注意力层</u></strong>；</p><ul><li>输入：仍然是 <code>Q = K = V = 该层前一层的输出</code>；</li><li>目的：让输出序列中<strong>正在预测的位置 <code>i</code></strong> 能够关注到<strong>已经生成的输出序列中在它之前的所有位置（<code>1</code> 到 <code>i-1</code>）</strong>，但<strong>不能</strong>看到它自身 (<code>i</code>) 或它之后 (<code>i+1</code> 到 <code>m</code>) 的位置。简言之，<strong>保持自回归（Autoregressive）特性，避免信息泄露（作弊）</strong>；</li><li>特点：<ul><li>掩码：在计算 <code>Q</code>（位置 <code>i</code>）与所有 <code>K</code>（位置 <code>j</code>）的点积后、进行 softmax 之前，会将 <code>j &gt; i</code> 的位置对应的点积结果设置为一个非常大的负数（如 <code>-10^9</code> 或 <code>-inf</code>）。这样，经过 softmax 后，这些未来位置的权重就几乎为 0；</li><li>其他同 encoder 的自注意力层；</li></ul></li></ul></li><li><p><strong><u>Encoder-Decoder 注意力层</u>（encoder-decoder attention layers）</strong>：</p><ul><li><p>输入：注意这个时候与前面的不太一样：</p><ul><li><strong>查询（Q）</strong>：来自<strong>解码器前一层的输出</strong>（即 Masked Self-Attention 子层的输出，代表了当前预测位置 <code>i</code> 及之前的信息）。</li><li><strong>键（K）</strong> 和 <strong>值（V）</strong>：来自<strong>编码器的最终输出</strong>（即最后一层编码器的输出，代表了整个输入序列的编码信息）；</li></ul><blockquote><p>Query（来自 Decoder）会有意识地查询、注意来自不同序列的 Key 和 Value（来自 Encoder），</p><p>因此和自注意力相对，也称 <strong><u>Cross Attention</u></strong>；</p></blockquote></li><li>目的：让输出序列中<strong>正在预测的位置 <code>i</code></strong> 能够关注到<strong>整个输入序列的所有位置（<code>1</code> 到 <code>n</code>）</strong>。这是经典的“源-目标”注意力机制，让解码器在生成目标词时，能够动态地聚焦于输入序列中最相关的部分（<u>类似于传统 Seq2Seq + Attention 模型中的注意力</u>）；</li><li>特点：<ul><li>源-目标对齐： 核心作用是根据当前目标状态，在源端信息中找到最相关的上下文。这是翻译、摘要等任务的关键；</li><li>信息桥梁： 这是连接编码器和解码器信息的主要通道；</li><li>多头注意力；</li></ul></li></ul></li></ul><h4 id="创新点-3：逐位的前馈神经网络"><a href="#创新点-3：逐位的前馈神经网络" class="headerlink" title="创新点 3：逐位的前馈神经网络"></a>创新点 3：逐位的前馈神经网络</h4><p>之前介绍架构时指出每两个 sub-layers 中一个是逐位的全连接前馈神经网络，采用一个 ReLU 激活函数和两层线性全连接层：</p><script type="math/tex; mode=display">\text{SubLayer}_\text{FFN}(x)=\max(0,\space xW_1+b_1)W_2+b_2</script><p>其中输入输出的维度都为 $d_\text{model}=512$，两个线性层中间的维度是 $d_{ff}=2048$；</p><p>此外，文章指出这部分还可以用 kernel size 为 1 的两个卷积层来代替。</p><h4 id="创新点-4-Embeddings-and-Softmax-的使用"><a href="#创新点-4-Embeddings-and-Softmax-的使用" class="headerlink" title="创新点 4:   Embeddings and Softmax 的使用"></a>创新点 4:   Embeddings and Softmax 的使用</h4><p>和一般的序列转录模型一样，Transformer 使用 embedding 的方法将输入/输出的 tokens 转为 $d_\text{model}$ 维度的向量，并且使用 linear transformation 层和 softmax 层将 decoder 输出转换为预测的 next-token 概率。</p><p>在本文构建的模型中，作者将两个 embedding layers（input/output embedding layers，参见上图架构中的粉红色块）和 pre-softmax linear transformation（参见上图架构中 decoder 输出的第一个 Linear 块）共用了相同的参数权重矩阵，不过在 embedding layers 中，这些权重还会被乘以 $\sqrt{d_\text{model}}$；</p><h4 id="创新点-5：位置编码-Positional-Encoding"><a href="#创新点-5：位置编码-Positional-Encoding" class="headerlink" title="创新点 5：位置编码 Positional Encoding"></a>创新点 5：位置编码 Positional Encoding</h4><p>因为 Transformer 架构模型不含有 sequence-aligned recurrence 计算方法，也不含有卷积操作，所以，为了让模型利用并感知到序列的具体顺序信息，作者还在 input/output embedding 传给 encoder/decoder 前注入了 tokens 在序列中相对或绝对的位置信息，这被称为 “<strong>位置编码</strong>”。</p><p>位置编码和 embedding vectors 的维度都是 $d_\text{model}$ 维，因此可以直接相加起来。</p><p>一般位置编码可以通过学习获得，也可以事先给定，本文中选取了不同频率的正弦/余弦函数作为位置编码信息（这也是为什么上图架构图把 position encoding 部分画成了示波器的形状）：</p><script type="math/tex; mode=display">\begin{aligned}PE_{(pos,2i)}&=\sin(\dfrac{pos}{10000^{2i/d_\text{model}}})\\PE_{(pos,2i+1)}&=\cos(\dfrac{pos}{10000^{2i/d_\text{model}}})\end{aligned}</script><p>其中 $pos$ 表示 token 在 sequence 中的位置，$i$ 表示的是一个 embedding vector 的维度索引（共 $d_\text{model}$ 维）。因此每个位置 $pos$ 对应一个 $d_\text{model}$ 维向量，<strong>偶数列</strong>用正弦函数，<strong>奇数列</strong>用余弦函数。</p><blockquote><p>[!IMPORTANT]</p><p>为什么需要 “让模型感知到序列的具体顺序信息”（设计动机）？为什么这么设计（这么设计的原因）？</p><p>设计动机简单来说就一个：<strong><u>弥补 self-attention 的位置不变性的问题</u></strong>。</p><p>我们数学上注意到，自注意力机制（Self-Attention）本身是具有<strong><u>置换不变性</u></strong>的（Permutation Invariant）。即：若打乱输入序列顺序，输出不变（仅依赖词之间的相似度）。</p><p>但是同时我们又需要模型必须感知序列顺序，例如掌握语义的差别：“猫追狗”不等于“狗追猫”。</p><p>那么这样设计的原因也是和数学特性有关：<strong><u>对任意固定偏移量 $k$，$PE_{pos+k}$ 可表示为 $PE_{pos}$ 的线性变换</u></strong>。证明：</p><script type="math/tex; mode=display">\begin{bmatrix}\sin(\omega_i(pos+k))\\\cos(\omega_i(pos+k))\end{bmatrix}=\begin{bmatrix}\cos(\omega_ik)&\sin(\omega_ik)\\-\sin(\omega_ik)&\cos(\omega_ik)\end{bmatrix}\begin{bmatrix}\sin(\omega_i(pos))\\\cos(\omega_i(pos))\end{bmatrix}</script><p>其中 $w_i=\dfrac{1}{10000^{2i/d_\text{model}}}$，这意味着 $i$ 越大，位置编码的“频率” 越高，越倾向于捕获全局位置信息（长距离依赖），反之倾向于捕获局部位置信息（相邻词关系）。</p><p>这样的特性可以让模型轻松地学习相对位置。</p><p>除了数学特性，这么还有其他的两个方面的考虑：</p><ul><li><p>外推性（Extrapolation）：正弦/余弦函数的<strong>周期性</strong>允许模型泛化到比训练时更长的序列（如测试时遇到更长的句子），相比较下，可学习的位置嵌入（Learned Positional Embedding）则难以泛化到未见过的位置；</p><blockquote><p>原文：We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p></blockquote></li><li><p><strong>值域有界</strong>：<code>[−1,1]</code>，与词嵌入（通常归一化）兼容。</p></li></ul></blockquote><h2 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h2><p>文章基于包含了 45 万词语对的 WMT 2014 English-German 数据集训练，句子被 encoded 为  byte-pair encoding（参考了 CoRR 的工作）。</p><p>使用的梯度下降的优化器是自适应学习率的 Adam optimizer（$\beta_1=0.9,\beta_2=0.98,\varepsilon=10^{-9}$），并且自定义控制学习率的策略：先预留 $warmup_steps=4000$ 这些 steps 来让学习率线性增长，后面在以平方根反比的速度减小学习率：</p><script type="math/tex; mode=display">l=d^{-0.5}_\text{model}\cdot\min(\text{step\_num}^{-0.5},\text{step\_num}\cdot\text{warmup\_steps}^{-1.5})</script><p>文章训练过程中使用了 3 种正则化方法：</p><ul><li><p>Normal Dropout：和一般的神经网络一样，我们会在比较长的网络中添加一些 dropout 进行正则化，起到防止过拟合等作用，没什么新鲜的不作介绍。</p></li><li><p>Residual Dropout：对每个 sub-layer 的输出采用了残差连接（在输入下一层 sub-layer 以及归一化前）。另外，在向 embeddings 加 positional encodings 时也用了 dropout；实验用的 base model 的  dropout rate 取 0.1；</p></li><li>Label Smoothing：训练过程中，使用 smoothing rate 取 $\varepsilon=0.1$；</li></ul><blockquote><p>知识补充：什么是标签平滑（label smoothing）？</p><p>在传统的分类任务（如机器翻译的词预测）中，标签通常采用 <strong>one-hot 编码</strong>（正确词的概率=1，其他词=0）。但问题是这会使模型过度自信（overconfident），强制将正确词概率推至 1，其他词压至 0。容易导致过拟合，降低泛化能力。</p><p>标签平滑的做法是，将正确词的目标概率设为 $1-\varepsilon$，并将剩余概率 $\varepsilon$ <strong>均匀分配</strong>给所有其他词（共 $V$ 个词）：</p><script type="math/tex; mode=display">P=\left\{\begin{aligned}1-\varepsilon&,\space\text{if correct}\\\dfrac{\varepsilon}{V-1}&,\space\text{otherwise}\end{aligned}\right.</script><p>那么为什么说 label smoothing 会损害 perplexity（模型困惑度）呢？回顾 perplexity 定义（交叉熵的指数），<strong><u>perplexity 值越低表示模型预测的越准确、越不太可能有很多不确定的用词选择</u></strong>：</p><script type="math/tex; mode=display">\text{perplexity}=\exp(-\dfrac{1}{N}\sum\limits_{i=1}^N\log P(w_i|\text{context}))</script><p>而 label smoothing 会让<strong>目标分布更“平滑”</strong>，显然会提升模型的 perplexity；</p><p>但它也在另一个方便提升了模型的泛化性：</p><ul><li>模型不会对训练数据中的噪声或特定模式过度敏感；</li><li>并且减少了 over-confident 的可能，在测试时对模糊边界（如近义词）更鲁棒；</li></ul></blockquote><h2 id="效果简述"><a href="#效果简述" class="headerlink" title="效果简述"></a>效果简述</h2><p>文章先对于 “为什么选择自注意力机制” 做了一些理论上的比较：</p><p><img src="imgs/layer-comp.png" width="650px" /></p><p>综合了整体复杂度、串行计算效率，以及关键路径长度比较，self-attention 在保证 performance（原文：could yield more interpretable models）的同时确保高效：Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.</p><p>然后摆出了和其他 Seq2Seq 模型的优越 performance：</p><p><img src="imgs/perf.png" width="550px" /></p><h2 id="语境生词"><a href="#语境生词" class="headerlink" title="语境生词"></a>语境生词</h2><p>transduction model：转录模型，指输入序列、生成序列的模型；</p><p>eschew：避开，规避了；</p><p>counteract with：将…（不良效果）中和/抵销了。</p><h2 id="代码实现及详解"><a href="#代码实现及详解" class="headerlink" title="代码实现及详解"></a>代码实现及详解</h2><blockquote><p>本章前置知识：PyTorch 的基本使用，至少需要了解怎么用 PyTorch 搭建线性分类器/CNN 这样的模型。</p><p>代码中所有重要的部分均已用 <code>Note</code> 在注释中注明。</p></blockquote><h3 id="Input-Encoder-Layer"><a href="#Input-Encoder-Layer" class="headerlink" title="Input Encoder Layer"></a>Input Encoder Layer</h3><p>input encoder layer：</p><ul><li>根据论文原文，取 $d_\text{model}=hd_k=512$，因此输出的 token embedding 维数为 512；</li><li>另外根据原文，在 embedding layers 中还会将所有权重乘以 $\sqrt{d_\text{model}}$（参见创新点 4）；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InputEmbeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, vocab_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a Transformer input embedding layer</span></span><br><span class="line"><span class="string">        :param d_model: the dimension of an embedding vector</span></span><br><span class="line"><span class="string">        :param vocab_size: the full size of the vocabulary</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.vocab_sz = vocab_size</span><br><span class="line">        self.embedding = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size,</span><br><span class="line">            embedding_dim=d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.embedding(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><p>如上代码，PyTorch 内置的 <code>nn.Embedding</code> 足够进行词嵌入的运算。</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><ul><li>根据原文，注入位置信息的方法就是将同维度的 $PE$ 与 input/output encoding 相加；</li><li>计算公式已由原文给出；</li><li>注意在 这里添加了一个 dropout layer（这里是为了提升泛化性，要与 Layer Normalization 区分开），dropout rate 为 0.1；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, seq_len: <span class="built_in">int</span>, p_dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a Transformer position encoding layer</span></span><br><span class="line"><span class="string">        :param d_model: the dimension of an embedding vector</span></span><br><span class="line"><span class="string">        :param seq_len: the max length of the tokens for the input sequence</span></span><br><span class="line"><span class="string">        :param p_dropout: the dropout rate for the current layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.seq_len = seq_len</span><br><span class="line">        self.dropout = nn.Dropout(p_dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># PE_&#123;(pos,2i)&#125;=\sin(\dfrac&#123;pos&#125;&#123;10000^&#123;2i/d_\text&#123;model&#125;&#125;&#125;)</span></span><br><span class="line">        <span class="comment"># PE_&#123;(pos,2i+1)&#125;=\cos(\dfrac&#123;pos&#125;&#123;10000^&#123;2i/d_\text&#123;model&#125;&#125;&#125;)</span></span><br><span class="line">        pe = torch.zeros((seq_len, d_model))</span><br><span class="line">        <span class="comment"># construct Tensor(seq_len, 1) from (seq_len,)</span></span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, seq_len, dtype=torch.float64).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        divisor_part = torch.exp(</span><br><span class="line">            torch.arange(<span class="number">0</span>, seq_len, <span class="number">2</span>, dtype=torch.float64) / self.d_model * (-math.log(<span class="number">10000.0</span>)))</span><br><span class="line">        <span class="comment"># for all the seq_len (each token), for even/odd dimension</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos * divisor_part)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos * divisor_part)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: here the size of the position encoding vector is (seq_len, d_model),</span></span><br><span class="line">        <span class="comment"># which cannot deal with batch input embeddings (multiple sequences).</span></span><br><span class="line">        <span class="comment"># We should generate (1, seq_len, d_model) for **batch processing**</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: If you want to save a variable in a nn.Module which is not a learnable parameter,</span></span><br><span class="line">        <span class="comment"># then you need to register it as a buffer so that PyTorch will save it for you.</span></span><br><span class="line">        <span class="comment"># And next time (e.g., training/inference stage) you can load it from the model checkpoint file!</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply position encoding to input sequence</span></span><br><span class="line"><span class="string">        :param x: a tensor with shape(batch, seq_len, d_model) including all the embeddings in batched sequences</span></span><br><span class="line"><span class="string">        :return: PE(x)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># use pe buffer here</span></span><br><span class="line">        <span class="comment"># Note: truncate buffer self.pe to fit sequence length</span></span><br><span class="line">        <span class="comment"># Note: position encoding is fixed and is not learnable.</span></span><br><span class="line">        <span class="comment"># So we should tell PyTorch using &#x27;requires_grad_(False)&#x27;.</span></span><br><span class="line">        x = x + (self.pe[:, :x.shape(<span class="number">1</span>), :]).requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h3 id="正则化层（架构中的-Add-amp-Norm-的-“Norm”）"><a href="#正则化层（架构中的-Add-amp-Norm-的-“Norm”）" class="headerlink" title="正则化层（架构中的 Add &amp; Norm 的 “Norm”）"></a>正则化层（架构中的 <code>Add &amp; Norm</code> 的 “<code>Norm</code>”）</h3><p>先看正则化层，即文中的 $\text{LayerNorm}$（Layer Normalization），它就是对一个 <strong>单个样本（Token）在某一层的所有特征维度</strong>（沿 $d_\text{model}$）上进行归一化。</p><p>必要性：我们知道因为这些 embedding representation 每个特征维度可能相差很大（几个数量级），例如一个 token 的 embedding vector 可能是这样的：<code>[0.001, 10000.103, -9999999.113]</code>（比较极端），这不利于后续梯度计算和收敛。因此 Layer Normalization 是为了<strong>稳定深层网络训练</strong>（ 缓解梯度问题）、<strong>加速收敛</strong>（减少内部协变量偏移）的考量才设计的。</p><p>这个 Layer Normalization 的归一化方法是非常科学的，除了一般的归一化处理，还会进行<strong><u>仿射变换</u></strong>。</p><blockquote><p>这里仿射变换的必要性？</p><p>它让模型能够学习在归一化之后，<strong><u>是否以及如何恢复某些特征维度的原始重要性或偏差</u></strong>。如果没有它们，归一化可能会破坏网络已经学习到的一些表示能力。</p></blockquote><p>因此现在步骤如下：</p><ol><li><p>一般归一化：$\hat{h}=\dfrac{h-\mu}{\sqrt{\sigma^2+\varepsilon}}$，其中 $\varepsilon$ 是保证数值稳定性的极小数（老生常谈了），$\mu=\dfrac{1}{d_\text{model}}\sum\limits_{i=1}^{d_\text{model}}h_i$，$\sigma^2=\dfrac{1}{d_\text{model}}\sum\limits_{i=1}^{d_\text{model}}(h_i-\mu)^2$；</p></li><li><p>可学习的仿射变换（<strong><u>注意超参数 $\gamma$ 和 $\beta$ 都是可学习的</u></strong>，前者负责<strong>缩放</strong>归一化后的值，后者负责<strong>平移</strong>/偏移归一化后的值）：</p><script type="math/tex; mode=display">y=\gamma\cdot\hat{h}+\beta</script></li></ol><blockquote><p>[!NOTE]</p><p>这里可以与一般的 Batch Normalization 做个区分。</p><p><strong>Batch Norm</strong>： 对一个 Batch 内所有样本的 <strong>同一特征维度</strong> 计算均值和方差进行归一化。它依赖于 Batch Size 和序列长度（需要填充对齐），对 Batch Size 敏感，且在 RNN/Transformer 这类序列模型上应用较麻烦。</p><p>而 <strong>Layer Norm</strong>： 对 <strong>单个样本的所有特征维度</strong> 计算均值和方差进行归一化。它与 Batch Size 无关，天然适合处理不同长度的序列输入（每个 Token 独立归一化），可以针对每个样本、每个位置独立计算统计量，非常适合处理长度可变的序列数据（如句子），避免了 Batch Norm 在序列数据上的局限性（需要填充对齐、依赖 Batch Size 统计量）。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNormalization</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, eps: <span class="built_in">float</span> = <span class="number">1e-9</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.eps = eps</span><br><span class="line">        <span class="comment"># Note: use Parameter for learnable parameters in nn.Module</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(<span class="number">1</span>))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply layer normalization to current input (LayerNorm)</span></span><br><span class="line"><span class="string">        :param x: the tensor output from every sub-layer</span></span><br><span class="line"><span class="string">        :return: LayerNorm(x)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># &#x27;mean&#x27;/&#x27;std&#x27; along the dimension in each token (indexing elements of an embedding)</span></span><br><span class="line">        <span class="comment"># Note: &#x27;mean&#x27;/&#x27;std&#x27; always cancels the dimension it applies.</span></span><br><span class="line">        <span class="comment"># Here we need to keep it for further calculation, otherwise we will need to un-squeeze</span></span><br><span class="line">        mu = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        sigma = x.std(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gamma * (x - mu) / (sigma + self.eps) + self.beta</span><br></pre></td></tr></table></figure><p>至于残差连接块（residual dropout），我们将在后文介绍。</p><h3 id="逐位的全连接前馈神经网络"><a href="#逐位的全连接前馈神经网络" class="headerlink" title="逐位的全连接前馈神经网络"></a>逐位的全连接前馈神经网络</h3><p>计算公式和网络结构论文已经给出，参见 “创新点 3”。</p><p>这里建议实现时添加在激活函数后添加一个 dropout layer 进行正则化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FFNBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, d_ff: <span class="built_in">int</span>, ff_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a 2-layer fully connected feed-forward network with activation function ReLU.</span></span><br><span class="line"><span class="string">        :param d_model: input and output size</span></span><br><span class="line"><span class="string">        :param d_ff: hidden layer size</span></span><br><span class="line"><span class="string">        :param ff_dropout: dropout rate for this network</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_ff = d_ff</span><br><span class="line">        self.linear_1 = nn.Linear(in_features=d_model, out_features=d_ff)</span><br><span class="line">        <span class="comment"># Note: we add dropout layer here to do normalization</span></span><br><span class="line">        self.dropout = nn.Dropout(ff_dropout)</span><br><span class="line">        self.linear_2 = nn.Linear(in_features=d_ff, out_features=d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply this FFN to the input tensor.</span></span><br><span class="line"><span class="string">        (batch, seq_len, d_model) -&gt; (batch, seq_len, d_ff) -&gt; (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">        :param x: the input tensor</span></span><br><span class="line"><span class="string">        :return: SubLayer_&#123;FFN&#125;(x)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.linear_2(</span><br><span class="line">            self.dropout(</span><br><span class="line">                torch.relu(</span><br><span class="line">                    self.linear_1(x))))</span><br></pre></td></tr></table></figure><h3 id="多头自注意力块、掩码多头自注意力块、Encoder-Decoder-注意力块"><a href="#多头自注意力块、掩码多头自注意力块、Encoder-Decoder-注意力块" class="headerlink" title="多头自注意力块、掩码多头自注意力块、Encoder-Decoder 注意力块"></a>多头自注意力块、掩码多头自注意力块、Encoder-Decoder 注意力块</h3><p>这里如果看论文的公式会发现比较复杂，尤其涉及分块和 batch，很容易混淆 tensor shape。这里建议在架构图上表明每一步的 shape/size 的变化情况。</p><p>如果是 Encoder 的多头自注意力块（或者 Decoder 的掩码多头自注意力块），$Q=K=V=\text{Input}$，而 Encoder-Decoder 注意力则是 $Q=\text{Decoder Input},\space K,V=\text{Encoder Input}$。它们只是计算传入的参数有所不同。另外关于 Mask 我们等会考虑。</p><p>为了方便分析，我们现在省去 batch 的维度，因此对每一个 sequence（token）的 embedding vector 而言，多头注意力层的计算过程如下图所示：</p><p><img src="imgs/mha-shapes.png" /></p><p>现在我们逐步介绍。输入 shape 为 $(\text{seq-len},d_\text{model})$，注意无论是哪种 attention，在这个论文的实现中都是这个 shape；</p><ul><li><p>先计算多头 $\text{head}_i$：</p><ol><li><p>先定义可训练的模型超参数矩阵 $W^Q,W^K,W^V$，它们不能改变 $Q,K,V$ 的形状，所以大小显然都是 $d_\text{model}\times d_\text{model}$；</p></li><li><p>直接计算 $QW^Q,KW^K,VW^V$，然后将它们沿着 $d_\text{model}$（垂直于 embedding 特征维度）方向，平均拆成 $h$ 份，每份记为 $QW_i^Q,KW_i^K,VW_i^V$；</p><p>因为拆成 $h$ 份，因此大小都是 $\text{seq-len}\times \dfrac{d_\text{model}}{h}=\text{seq-len}\times d_k$（$d_k,d_v$ 的定义，$d_k=d_v=\dfrac{d_\text{model}}{h}$）；</p><blockquote><p>这一步相当于将 $Q,K,V$ 投影到 $h$ 个不同子空间；</p></blockquote></li><li><p>对拆好的每一份计算一次缩放点积注意力：$\text{head}_i=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$；</p></li></ol></li><li><p>现在 $\text{head}_i$ 包含了 $h$ 个不同子空间的 representation 的注意力信息，我们最后将它简单地拼起来，最后进行一个线性变换（$\times W^O$）。注意拼起来的 $\text{head}_i$ 的大小 $\text{seq-len}\times (h\cdot d_v)$，而要保证输入输出的 size 一致，因此线性变换 $W^O$ 张量大小需要 $(h\cdot d_v)\times d_\text{model}$。</p></li><li><p>最后的最后，和 position encoding、FFN 一样，我们需要添加一个 dropout layer 来正则化。</p></li></ul><p>另外，为了代码的可重用性，我们应该在类中定义一个可以计算 mask 的注意力公式，同时可以计算含有掩码的多头注意力块。如果需要 mask，那么应该在计算 $\text{head}_i$ 时（$QW_i^Q\times KW_i^K$ 完成后、softmax 计算前）针对对齐分数进行 mask。</p><blockquote><p>[!TIP]</p><p>这里有个比较有意思的处理方式，上面的超参数矩阵（$W^Q,W^K,W^V,W^O$）可以直接用无偏移的 <code>nn.Linear</code>（不包含激活函数的线性网络）表示，因为后者在数学上的表达式就是这样。注意 input feature 和 output feature 对应矩阵的长宽（利用 broadcast）。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, h: <span class="built_in">int</span>, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build a multi-head attention block with source mask (optional)</span></span><br><span class="line"><span class="string">        :param d_model: the dimension of an embedding vector</span></span><br><span class="line"><span class="string">        :param h: the number of the heads</span></span><br><span class="line"><span class="string">        :param p_dropout: dropout rate for this network</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.h = h</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_k = d_model // h     <span class="comment"># Use floor div</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span>, <span class="string">&quot;d_model is not divisible by h&quot;</span></span><br><span class="line"></span><br><span class="line">        self.w_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.w_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.w_v = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: h * d_v == d_model. so (h*d_v, d_model) == (d_model, d_model)</span></span><br><span class="line">        self.w_o = nn.Linear(d_model, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(p_dropout)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask, dropout</span>) -&gt; (torch.Tensor, torch.Tensor):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Helper for calculating MHA scores</span></span><br><span class="line"><span class="string">        :param query: Query tensor with batch and heads stacked (batch, h, seq_len, d_k)</span></span><br><span class="line"><span class="string">        :param key: Key tensor with the same shape of Query</span></span><br><span class="line"><span class="string">        :param value: Value tensor with shape (batch, h, seq_len, d_v), where `d_v == d_k`</span></span><br><span class="line"><span class="string">        :param mask: (optional) Source mask with shape (1, 1, seq_len, seq_len). Meaning: (i,j) =&gt; j for i</span></span><br><span class="line"><span class="string">        :param dropout: (optional) drop out network</span></span><br><span class="line"><span class="string">        :return: (split attention_scores, softmax-processed alignment scores)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        d_k = query.shape[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: we need to switch dimension seq_len and d_k to do multiplication</span></span><br><span class="line">        <span class="comment"># [IMPORTANT] the shape of aligned_scores:</span></span><br><span class="line">        <span class="comment"># (batch, h, seq_len, d_k) x (batch, h, d_k, seq_len) -&gt; (batch, h, seq_len, seq_len)</span></span><br><span class="line">        aligned_scores: torch.Tensor = (query @ key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            aligned_scores.masked_fill_(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        aligned_scores = aligned_scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            aligned_scores = dropout(aligned_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch, h, seq_len, seq_len) x (batch, h, seq_len, d_k) -&gt; (batch, h, seq_len, d_k)</span></span><br><span class="line">        <span class="keyword">return</span> aligned_scores @ value, aligned_scores</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Calculate MHA scores for input Q/K/V</span></span><br><span class="line"><span class="string">        :param q: Query tensor with shape (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">        :param k: Key tensor with the same shape of Query</span></span><br><span class="line"><span class="string">        :param v: Value tensor with the same shape of Key</span></span><br><span class="line"><span class="string">        :param mask: source mask () for the alignment scores</span></span><br><span class="line"><span class="string">        :return: Output tensor with shape (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        q_prime: torch.Tensor = self.w_q(q)</span><br><span class="line">        k_prime: torch.Tensor = self.w_k(k)</span><br><span class="line">        v_prime: torch.Tensor = self.w_v(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split q_prime, k_prime and v_prime into h pieces.</span></span><br><span class="line">        <span class="comment"># And we use different dimension to indicate partition!</span></span><br><span class="line">        <span class="comment"># Reshape (without creating new memory area):</span></span><br><span class="line">        <span class="comment"># (batch, seq_len, d_model) -&gt; (batch, seq_len, h, d_k)</span></span><br><span class="line">        <span class="comment"># [IMPORTANT] 注意：这里需要整理出 seq_len x d_k 相邻维度方便后续计算，因此应该交换 h 和 seq_len 的维度</span></span><br><span class="line">        <span class="comment"># (batch, seq_len, h, d_k) -&gt; (batch, h, seq_len, d_k)</span></span><br><span class="line">        q_split = q_prime.view((q_prime.shape[<span class="number">0</span>], q_prime.shape[<span class="number">1</span>], self.h, self.d_k)).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        k_split = k_prime.view((k_prime.shape[<span class="number">0</span>], k_prime.shape[<span class="number">1</span>], self.h, self.d_k)).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        v_split = v_prime.view((v_prime.shape[<span class="number">0</span>], v_prime.shape[<span class="number">1</span>], self.h, self.d_k)).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape of split_mha_scores: (batch, h, seq_len, d_v)</span></span><br><span class="line">        <span class="comment"># shape of alignment_scores: (batch, h, seq_len, seq_len)</span></span><br><span class="line">        split_mha_scores, alignment_scores = MultiHeadAttentionBlock.attention(</span><br><span class="line">            q_split, k_split, v_split, mask, self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># concat the split MHA scores and multiply by w_o (d_k == d_v)</span></span><br><span class="line">        <span class="comment"># concat: (batch, h, seq_len, d_v) -&gt; (batch, seq_len, h, d_v) -&gt; (batch, seq_len, h*d_v)</span></span><br><span class="line">        <span class="comment"># [IMPORTANT] do contiguous() here to declare memory copy explicitly</span></span><br><span class="line">        mha_scores = (split_mha_scores</span><br><span class="line">                      .transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">                      .view((split_mha_scores.shape[<span class="number">0</span>], -<span class="number">1</span>, self.h * self.d_k)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># multiply: (batch, seq_len, h*d_v) x (h*d_v, d_model) -(broadcast)-&gt; (batch, seq_len, d_model)</span></span><br><span class="line">        <span class="keyword">return</span> self.w_o(mha_scores)</span><br></pre></td></tr></table></figure><p>代码有些复杂，不过我用 <strong><code>[IMPORTANT]</code></strong> 标注出了 3 处比较重要、困难的部分，我们单独分析。</p><p>先看 <code>q_split = q_prime.view((q_prime.shape[0], q_prime.shape[1], self.h, self.d_k)).transpose(1, 2)</code> 这部分，<code>view</code> 是创建了一个 stride 不同的新的 tensor 对象，但是与 <code>q_prime</code> 共用数据内存（引用式 reshape），这个比较好理解。</p><p>但是为什么需要 <code>transponse</code> 将 $h$ 维度和 $d_k$ 交换呢？</p><p>这主要考虑到计算 attention score 时需要让 <code>seq_len</code> 和 $d_k$ 在相邻的维度上，方便后续计算。</p><p>然后再看 <code>attention</code> 计算函数的 <code>aligned_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)</code>，对应的是点积缩放 compatibility function $\dfrac{QK^T}{\sqrt{d_k}}$。</p><p>这里想搞清楚 shape 比较困难：为什么 <code>(batch, h, seq_len, d_k) x (batch, h, d_k, seq_len)</code> 得到形状 <code>(batch, h, seq_len, seq_len)</code>？</p><p>这主要是高维张量相乘特性，记住即可。如果是 <code>query.transpose(-2,-1) @ key</code> 那么 shape 就是 $(\text{batch}, h, d_k, d_k)$ 了。</p><p>你可以用简单的情况试一试，建立直觉：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a 的形状是 (3, 1, 2, 3), b 的形状是 (1, 1, 2, 3)</span></span><br><span class="line">a = torch.tensor([[[[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>]]], [[[<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span>], [<span class="number">4</span>,<span class="number">6</span>,<span class="number">6</span>]]], [[[<span class="number">5</span>,<span class="number">7</span>,<span class="number">7</span>], [<span class="number">6</span>,<span class="number">9</span>,<span class="number">9</span>]]]])</span><br><span class="line">b = torch.tensor([[[[<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]]]])</span><br><span class="line">c = a @ b.transpose(-<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">d = a.transpose(-<span class="number">2</span>, -<span class="number">1</span>) @ b</span><br><span class="line"><span class="comment"># e = a @ b 维度不匹配</span></span><br><span class="line"><span class="built_in">print</span>(a.shape, b.shape, c.shape, d.shape)</span><br></pre></td></tr></table></figure><p>最后来看这段将 MHA 的多头合并起来的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mha_scores = (split_mha_scores</span><br><span class="line">              .transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">              .view((split_mha_scores.shape[<span class="number">0</span>], -<span class="number">1</span>, self.h * self.d_k)))</span><br></pre></td></tr></table></figure><p><code>.transpose(1, 2)</code> 是将之前为了计算方便而交换的 $h$ 和 <code>seq_len</code> 维度再换回来，准备合并。</p><p><code>.contiguous()</code> 是显式地进行 tensor 内存 copy，让 stride 对应的底层数据结构是连续的，方便后续 <code>view</code> reshape 和其他操作。</p><blockquote><p>[!TIP]</p><div class="table-container"><table><thead><tr><th><strong>特性</strong></th><th><code>transpose()</code></th><th><code>transpose_()</code></th></tr></thead><tbody><tr><td><strong>是否原地修改</strong></td><td>❌ 返回新张量</td><td>✅ 修改原始张量</td></tr><tr><td><strong>内存共享</strong></td><td>✅ 与原始张量共享内存</td><td>✅ 同一张量内存地址不变</td></tr><tr><td><strong>连续性</strong></td><td>❌ 结果是非连续的</td><td>❌ 结果是非连续的</td></tr><tr><td><strong>内存复制时机</strong></td><td>仅在需要连续张量时触发（如 <code>contiguous()</code>）</td><td>同左</td></tr></tbody></table></div></blockquote><p>最后的 <code>.view()</code> 最终进行符合要求的合并操作。</p><h3 id="残差连接块（Add-amp-Norm-中的-“Add”）"><a href="#残差连接块（Add-amp-Norm-中的-“Add”）" class="headerlink" title="残差连接块（Add &amp; Norm 中的 “Add”）"></a>残差连接块（<code>Add &amp; Norm</code> 中的 “<code>Add</code>”）</h3><p>前面说了正则化层的定义，现在我们看残差连接层。正如论文的表达式：$\text{LayerNorm}(x+\text{SubLayer}(x))$，残差连接就是 $x+\text{SubLayer}(x)$，也就是上面架构图中将上一层的 input 拉过来的箭头。</p><p>为了方便起见，我们这里代码中的残差连接层的定义直接和正则化层写在了一起（调用关系），因为它们总是一同出现。</p><p>另外比较好玩的是，很多 Transformer Implementation 实际上是这么计算的：$x+\text{SubLayer}(\text{LayerNorm}(x))$，然后在 $\text{SubLayer}$ 计算完后再添加一个 dropout layer，最后再和 $x$ 残差连接起来。可能这样的工程效果更好？注：下面的代码也是这种和论文不一样的计算方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConnection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build residual connection layer (Add &amp; Norm)</span></span><br><span class="line"><span class="string">        :param p_dropout: the normal dropout rate for this layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p_dropout)</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sub_layer</span>):</span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sub_layer(self.norm(x)))</span><br></pre></td></tr></table></figure><h3 id="Encoder-amp-Decoder-Block"><a href="#Encoder-amp-Decoder-Block" class="headerlink" title="Encoder &amp; Decoder Block"></a>Encoder &amp; Decoder Block</h3><p>先考虑 Encoder。现在我们要将之前已经定义的模块组合起来成为一个 Transformer Encoder（参见架构图），我们分别定义 <code>EncoderBlock</code>（包含 MHA、FFN、两个 Add &amp; Norm），以及 <code>Encoder</code>（$N\times$ Encoder Block，论文中 $N=6$）。</p><p>注意，因为前面的多头注意力块实现的时候我们为了可复用性添加了 Mask 参数，所以这里在构造 Encoder Attention 的时候还需要代一个参数方便复用（尽管当前模型用不到）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mha: MultiHeadAttentionBlock, ffn: FFNBlock, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build a single encoder block with 1x MHA, 2x Residual Connections, 1x FFN.</span></span><br><span class="line"><span class="string">        :param mha: Multi-head attention block instance</span></span><br><span class="line"><span class="string">        :param ffn: Position-wise feed-forward network instance</span></span><br><span class="line"><span class="string">        :param p_dropout: the dropout rate for each of the residual connection block</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.mha_layer = mha</span><br><span class="line">        self.ffn_layer = ffn</span><br><span class="line">        self.residual_connections = nn.ModuleList([ResidualConnection(p_dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, src_mask</span>):</span><br><span class="line">        <span class="comment"># Note: sub_layer in ResidualConnection only has one argument,</span></span><br><span class="line">        <span class="comment"># but `forward` in MultiHeadAttentionBlock has 4 parameters (self excluded)</span></span><br><span class="line">        <span class="comment"># So we need to use lambda expr to construct function with one parameter</span></span><br><span class="line">        x = self.residual_connections[<span class="number">0</span>](x, <span class="keyword">lambda</span> i: self.mha_layer(i, i, i, src_mask))</span><br><span class="line">        x = self.residual_connections[<span class="number">1</span>](x, self.ffn_layer)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers: nn.ModuleList</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Construct a Transformer Encoder</span></span><br><span class="line"><span class="string">        :param layers:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = layers</span><br><span class="line">        <span class="comment"># Question: is this necessary?</span></span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>还要注意一下，在组合网络的时候需要注意和定义一个网络有略微区别，主要是需要向上层呈递和注册参数，方便训练优化器识别。下面的 Tip 提示一个易错点。</p><blockquote><p>[!TIP]</p><p>PyTorch 在处理多个模型拼接的时候，不能用普通 Python 列表来管理网络部件，必须使用 <code>nn.ModuleList</code> 来表示（当然你不嫌麻烦的话可以一个一个重复手写）。它允许 index / for 迭代、使用 <code>List[nn.Module]</code> 初始化。</p><p>读者可能会问，为什么不直接用 Python 列表存放 <code>nn.Module</code> 而必须用 <code>nn.ModuleList</code> 呢？</p><p>很多新手都会犯这样的错误（包括笔者），不用的话可能有些问题：</p><ol><li><p><strong><u>参数注册</u></strong>问题：<code>nn.ModuleList</code> 会自动将列表中的所有子模块注册到父模块中。这意味着子模块的参数（<code>nn.Parameter</code>）会被父模块的 <code>parameters()</code> 方法识别，从而被优化器发现并更新；</p><p>如果只是用列表的话，优化器可能没法识别到，或者无法正确保存/加载模型（<code>state_dict</code> 会缺失这些参数）；</p></li><li><p>另一种情况是<strong><u>设备移动</u></strong>问题，当调用 <code>model.to(device)</code> 时，<code>nn.ModuleList</code> 会管理并将所有子模块及其参数会自动移动到目标设备（如 GPU）；</p><p>如果只用列表的话，子模块可能不会被移动，导致模型一部分在 CPU、一部分在 GPU，引发运行时错误；</p></li><li><p><strong><u>模式状态</u></strong>也可能有问题。<code>model.train()</code> 和 <code>model.eval()</code> 两种情况网络的读写行为是不一样的，用普通列表会导致更新状态错误。</p></li></ol><p>举个栗子🌰：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BadModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 错误的。你不炸了吗</span></span><br><span class="line">        self.layers = [</span><br><span class="line">            nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoodModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 正确的（forward 与上面的相同）</span></span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line">        ])</span><br></pre></td></tr></table></figure></blockquote><p>那么好，我们回顾一下残差连接层的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConnection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p_dropout)</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><p>回答问题：</p><ol><li><p>为什么 <code>LayerNormalization</code> 和 <code>Dropout</code> 的网络不需要 <code>nn.ModuleList</code> 来帮它注册参数？</p><blockquote><p>答：<code>nn.Module</code> 类会自动管理所有存放在<strong><u>实例属性</u></strong>中的网络（不包括 Python 原生容器类型，因为对象的内存存放方式导致）。</p><p>上面的网络作为 <code>self.dropout</code> 和 <code>self.norm</code> 存在，因此已经被管理；</p></blockquote></li><li><p>为什么传入的 <code>sublayer</code> 不需要 <code>nn.ModuleList</code> 来帮它注册参数？</p><blockquote><p>答：外部传入的 <code>sublayer</code> <strong>不是</strong> <code>ResidualConnection</code> 的组成部分，约定由创建它的父模块负责。</p></blockquote></li></ol><p>看完上面的 Tip 和两个问题后，你应该能明白 <code>nn.ModuleList</code> 或者 <code>nn.Sequential</code> 的作用了，主要是方便管理<u>不方便一个个写成实例属性的情况</u>：</p><div class="table-container"><table><thead><tr><th>场景</th><th>推荐方式</th><th>示例</th></tr></thead><tbody><tr><td><strong>固定数量的子模块</strong></td><td>直接定义成实例属性</td><td><code>self.conv = nn.Conv2d()</code></td></tr><tr><td><strong>动态数量的模块集合 / 重复的模块</strong></td><td><code>nn.ModuleList</code></td><td>循环创建的 layer 列表</td></tr><tr><td><strong>需要按名字访问的模块集合</strong></td><td><code>nn.ModuleDict</code></td><td>通过键名访问的模块</td></tr><tr><td><strong>顺序执行的固定模块序列</strong>（串行计算）</td><td><code>nn.Sequential</code></td><td><code>self.seq = nn.Sequential(...)</code></td></tr></tbody></table></div><p>最后，Decoder 和 Encoder 非常相似，不再赘述，只要区分给 Encoder MHA 传入的 mask（source mask）和 Decoder MHA 的 mask（target mask）是不同的即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, masked_mha: MultiHeadAttentionBlock, mha: MultiHeadAttentionBlock, ffn: FFNBlock, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build a single decoder block with 1x Masked MHA, 1x Cross MHA, 1x FFN, 3x Residual Connections.</span></span><br><span class="line"><span class="string">        :param masked_mha: Masked MHA instance for decoder</span></span><br><span class="line"><span class="string">        :param mha: Multi-head cross attention block instance for decoder (encoder-decoder attention)</span></span><br><span class="line"><span class="string">        :param ffn: Position-wise feed-forward network instance</span></span><br><span class="line"><span class="string">        :param p_dropout: the dropout rate for each of the residual connection block</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.masked_mha_layer = masked_mha</span><br><span class="line">        self.mha_layer = mha</span><br><span class="line">        self.ffn_layer = ffn</span><br><span class="line">        self.residual_connections = nn.ModuleList([ResidualConnection(p_dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input, decoder_input, src_mask, target_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply DecoderBlock to Transformer Encoder/Decoder inputs (with masks)</span></span><br><span class="line"><span class="string">        :param encoder_input: input embeddings from encoder</span></span><br><span class="line"><span class="string">        :param decoder_input: input from decoder</span></span><br><span class="line"><span class="string">        :param src_mask: mask for MHA of Transformer Encoder</span></span><br><span class="line"><span class="string">        :param target_mask: mask for MHA of Transformer Decoder</span></span><br><span class="line"><span class="string">        :return: tensor proceeded by one single DecoderBlock</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        decoder_input = self.residual_connections[<span class="number">0</span>](</span><br><span class="line">            decoder_input, <span class="keyword">lambda</span> i: self.masked_mha_layer(i, i, i, target_mask))</span><br><span class="line">        output = self.residual_connections[<span class="number">1</span>](</span><br><span class="line">            decoder_input, <span class="keyword">lambda</span> d: self.mha_layer(d, encoder_input, encoder_input, src_mask))</span><br><span class="line">        output = self.residual_connections[<span class="number">2</span>](output, self.ffn_layer)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers: nn.ModuleList</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Construct a Transformer Decoder</span></span><br><span class="line"><span class="string">        :param layers: the layers of multiple DecoderBlocks</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = layers</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_from_encoder, x_from_decoder, src_mask, target_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x_from_decoder = layer(x_from_encoder, x_from_decoder, src_mask, target_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x_from_decoder)</span><br></pre></td></tr></table></figure><h3 id="Projection-Layer-Linear"><a href="#Projection-Layer-Linear" class="headerlink" title="Projection Layer (Linear)"></a>Projection Layer (<code>Linear</code>)</h3><p>在 Transformer 架构的最后有一个 Linear 块，即 pre-softmax linear transformation（参见 “创新点 4”），它的权重是与 Embedding layers 共用的（不过没有 $\sqrt{d_\text{model}}$ 的缩放），它的作用是最终将生成的特征 tensors 映射回 vocabulary 中。</p><p>这里为了简便起见，我们直接设置自由的权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ProjectionLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Build a projection layer for Transformer to convert features back into vocabulary</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, vocab_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># project: (batch, seq_len, d_model) -&gt; (batch, seq_len, vocab_size)</span></span><br><span class="line">        <span class="comment"># log-softmax: convert vocab_size dim to probabilities</span></span><br><span class="line">        <span class="keyword">return</span> torch.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Put-It-Together"><a href="#Put-It-Together" class="headerlink" title="Put It Together"></a>Put It Together</h3><p>最终我们定义一个 <code>Transformer</code> 类型，将上面的模块组合起来（以文本映射任务为例），并且为模型的参数设置初始值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, encoder: Encoder, decoder: Decoder,</span></span><br><span class="line"><span class="params">            src_embedding_layer: InputEmbeddings,</span></span><br><span class="line"><span class="params">            target_embedding_layer: InputEmbeddings,</span></span><br><span class="line"><span class="params">            src_pos_encoding_layer: PositionalEncoding,</span></span><br><span class="line"><span class="params">            target_pos_encoding_layer: PositionalEncoding,</span></span><br><span class="line"><span class="params">            proj_layer: ProjectionLayer</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.src_embed = src_embedding_layer</span><br><span class="line">        self.target_embed = target_embedding_layer</span><br><span class="line">        self.src_pos_encod = src_pos_encoding_layer</span><br><span class="line">        self.target_pos_encod = target_pos_encoding_layer</span><br><span class="line">        self.proj = proj_layer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        src = self.src_embed(src)</span><br><span class="line">        src = self.src_pos_encod(src)</span><br><span class="line">        <span class="keyword">return</span> self.encoder(src, src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, target, target_mask</span>):</span><br><span class="line">        target = self.target_embed(target)</span><br><span class="line">        target = self.target_pos_encod(target)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(target, target_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">project</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.proj(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_transformer</span>(<span class="params"></span></span><br><span class="line"><span class="params">        src_vocab_size: <span class="built_in">int</span>, target_vocab_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        input_seq_len: <span class="built_in">int</span>, output_seq_len: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        d_model: <span class="built_in">int</span> = <span class="number">512</span>,</span></span><br><span class="line"><span class="params">        n: <span class="built_in">int</span> = <span class="number">6</span>,</span></span><br><span class="line"><span class="params">        h: <span class="built_in">int</span> = <span class="number">8</span>,</span></span><br><span class="line"><span class="params">        dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        d_ff: <span class="built_in">int</span> = <span class="number">2048</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Construct a Transformer model from scratch.</span></span><br><span class="line"><span class="string">    :param src_vocab_size: size of vocabulary for the source langauge</span></span><br><span class="line"><span class="string">    :param target_vocab_size: size of vocabulary for the target langauge</span></span><br><span class="line"><span class="string">    :param input_seq_len: the approximate length of the input token sequence</span></span><br><span class="line"><span class="string">    :param output_seq_len: the approximate length of the output token sequence</span></span><br><span class="line"><span class="string">    :param d_model: the dimension of the embedding vectors (feature dimension)</span></span><br><span class="line"><span class="string">    :param n: the number of EncoderBlock/DecoderBlock in Encoder/Decoder</span></span><br><span class="line"><span class="string">    :param h: the number of the heads in MHA</span></span><br><span class="line"><span class="string">    :param dropout: dropout rate for all the dropout networks in the model</span></span><br><span class="line"><span class="string">    :param d_ff: the size for the hidden layer in all the FFNs</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    src_embed = InputEmbeddings(d_model, src_vocab_size)</span><br><span class="line">    target_embed = InputEmbeddings(d_model, target_vocab_size)</span><br><span class="line"></span><br><span class="line">    src_pos_enc = PositionalEncoding(d_model, input_seq_len, dropout)</span><br><span class="line">    target_pos_enc = PositionalEncoding(d_model, output_seq_len, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note: we cannot use list expression because</span></span><br><span class="line">    <span class="comment"># each EncoderBlock/DecoderBlock has different parameters</span></span><br><span class="line">    encoder_blocks = []</span><br><span class="line">    decoder_blocks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        encoder_mha = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line">        decoder_masked_mha = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line">        cross_mha = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line"></span><br><span class="line">        encoder_ffn = FFNBlock(d_model, d_ff, dropout)</span><br><span class="line">        decoder_ffn = FFNBlock(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line">        encoder_blocks.append(EncoderBlock(encoder_mha, encoder_ffn, dropout))</span><br><span class="line">        decoder_blocks.append(DecoderBlock(decoder_masked_mha, cross_mha, decoder_ffn, dropout))</span><br><span class="line"></span><br><span class="line">    encoder = Encoder(nn.ModuleList(encoder_blocks))</span><br><span class="line">    decoder = Decoder(nn.ModuleList(decoder_blocks))</span><br><span class="line"></span><br><span class="line">    proj_layer = ProjectionLayer(d_model, target_vocab_size)</span><br><span class="line"></span><br><span class="line">    model = Transformer(encoder, decoder, src_embed, target_embed, src_pos_enc, target_pos_enc, proj_layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 为模型参数设置服从标准分布的值</span></span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;笔记温习一下经典的 Transformer 架构的论文，结合&lt;a href=&quot;#代码实现及详解&quot;&gt;代码实现和解读&lt;/a&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="AI" scheme="https://blog.sjtuxhw.top/tags/AI/"/>
    
    <category term="ML" scheme="https://blog.sjtuxhw.top/tags/ML/"/>
    
    <category term="Paper" scheme="https://blog.sjtuxhw.top/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>阅读: A Hardware-Software Co-Design for Efficient Secure Containers</title>
    <link href="https://blog.sjtuxhw.top/technical/sc-paper/"/>
    <id>https://blog.sjtuxhw.top/technical/sc-paper/</id>
    <published>2025-07-01T10:14:11.000Z</published>
    <updated>2025-07-27T11:01:09.454Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇 2025 年的关于软硬协同的安全容器设计的文章。</p><h2 id="0-Overview"><a href="#0-Overview" class="headerlink" title="0. Overview"></a>0. Overview</h2><p>虚拟机级别的容器中，每个容器运行在虚拟机虚拟出的独立内核上，因此隔离性很强。但其依赖于通用虚拟机虚拟出的虚拟化硬件，与 OS 级别的容器相比，会导致不可忽略的性能开销。而在嵌套虚拟化场景下，secure container 运行在虚拟机中，这个性能的 gap 会显著地扩大。</p><p>本篇文章基于两个角度提出容器内核隔离（CKI），一个软硬协调的高效机密容器设计。</p><ul><li>首先，Protection Keys for Supervisor（PKS）可以帮助我们构建一个新的权限级别，用于在 Host Kernel 中安全地配置多个容器内核，而不涉及 non-root ring 0（Intel 中的 Guest Kernel 所处级别）；</li><li>其次，secure container 使用的通用虚拟化技术提供很多容器实际隔离并不需要的特性，例如二阶段页表翻译，这引入了可以避免的性能开销；</li></ul><p>因此容器内核隔离技术在跑容器内核时：</p><ol><li>避免使用虚拟化硬件，并移除不必要的虚拟化技术（像二阶段地址翻译）。它使用 PKS 来构建一个新特权级，用来隔离不同的容器内核，并且提供了跨特权级更高效的交互方式；</li><li>给每个容器内核使用了 single-stage address translation，并且用轻量级的方式监听这些页表，来确保跨容器虚拟内存隔离；</li></ol><p>实机实验证明了 CKI 技术的高效性，结果显示对于内存密集型应用相对于硬件辅助虚拟化 hardware-assisted virtualization (HVM) 和基于软件虚拟化 software-based virtualization (PVM) 技术分别提升了 72% 和 47%；</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>由于容器具有可移植性和可扩展性等优势，因此被广泛应用于云中构建和部署应用程序。有两种典型的容器架构：</p><ul><li><p>OS 级别容器。由于互不信任的容器之间共享操作系统内核中的漏洞，操作系统级容器因<strong><u>安全性差</u></strong>而饱受诟病。恶意的操作系统级容器可能会利用系统调用接口暴露的巨大攻击面来<strong><u>逃避隔离</u></strong>；</p></li><li><p>VM 级别容器。相比之下，虚拟机级容器在自己的客户操作系统内核上运行每个容器，具有<strong><u>更强的安全隔离性</u></strong>，在云计算中越来越受欢迎。<strong><u>破坏客户内核对主机内核或其他容器无害</u></strong>。</p><p>尽管进行了许多优化，但与操作系统级容器相比，虚拟机级容器仍显示出<strong><u>性能上的劣势</u></strong>，这是因为涉及到为通用虚拟机设计的虚拟化硬件。例如，遍历 walk 过 two-stage page table 可将内存密集型应用的延迟平均增加 46%；</p></li></ul><p>此外，性能差距随着嵌套虚拟化的增加而增大。据谷歌和阿里巴巴等知名云提供商称，基于公共基础设施即服务（IaaS）的云构建服务 (building cloud services) 的需求日益增长。在这种<strong><u>嵌套云</u></strong>中，虚拟机级容器必须在虚拟机内运行，由于 L2 VM（容器）、guest hypervisor（L1 内核）和 host hypervisor（L0 内核）之间的<strong><u>上下文切换过多</u></strong>，导致运行时开销很大。根据我们的评估，这种开销使内存密集型和 I/O 密集型应用的性能分别降低了 28%∼226% 和 1.8×∼4.3× 。</p><p>我们认为，性能开销源于 <strong><u>secure container 架构所需的权限级别与 CPU 硬件所提供的权限级别之间的不匹配</u></strong>。具体来说，主机内核需要隔离多个容器访客内核，而每个访客内核又需要隔离多个容器应用进程。因此，需要三个权限级别。然而，CPU 硬件通常为运行操作系统内核和应用程序提供两种权限级别，例如 x86 ring-0/ring-3 和 Arm EL1/EL0。因此，现有的安全容器（如 Kata Containers 和 Firecracker）利用硬件虚拟化扩展来获得额外的权限级别，从而导致性能开销。</p><p>一些 secure container 架构，如 PVM 和 gVisor，不需要虚拟化硬件。它们将容器 guest kernel 授权为用户模式，并将 guest kernel 和 container app 隔离在不同的地址空间中。不过，<strong><u>这些架构会产生更多上下文切换开销，因为容器内的系统调用和异常一定会 redirect 给 host kernel</u></strong>。例如，空系统调用在操作系统级容器中需要 90 ns 的时间，而在 PVM 容器中则需要 336 ns。</p><p>在本文中，我们基于两个观点提出了一种新的安全容器设计，称为 CKI（容器内核隔离）：</p><ol><li>首先，最近的 CPU 功能（Protection Keys for Supervisor，PKS 或 MPK）可以改造为在 kernel mode 中创建另一个特权级别，以容纳 guest（容器）kernel。这一新的权限级别允许客户内核有效地为其容器应用服务，从而带来性能上的优势，例如将裸机云和嵌套云中的上下文切换最小化；</li><li>其次，为每个容器分离内核是为了安全隔离，而不是通用虚拟化。具体来说，内存虚拟化中使用的两阶段地址转换机制为任何客户内核提供了透明的物理地址空间，确保了兼容性，但这与容器隔离要求无关。因此，支持任意虚拟机的虚拟化机制是不必要的（例如二阶段地址翻译可以被移除，以此提升性能）。</li></ol><p>但在创建新的权限级别时，CKI 面临着三个挑战。我们逐个应对：</p><ol><li><p>首先，PKS 仅用于内存隔离，而恶意 guest kernel 在 kernel mode 下执行时，可能会执行任意特权指令。为了解决这个问题，我们提出了针对 PKS 的<strong><u>轻量级硬件扩展，以实现指令隔离</u></strong>；</p></li><li><p>其次，CKI 中 PKS switch gates 的<strong><u>安全目标</u></strong>，即防止容器逃逸或主机拒绝服务（host DoS），<strong><u>需要在基本 MPK 门之外进行额外设计</u></strong>。例如，switch gates 需要新的机制来定位每 vCPU 区域，防止中断垄断或伪造；</p></li><li><p>PKS 在一个内核地址空间内只支持 16 个内存域，而一台机器可能承载数十到数百个安全容器。为了支持任意数量的容器，CKI <strong><u>结合了 PKS 和地址空间隔离技术，以隔离不同的客户内核</u></strong>。</p><p>具体来说，它为每个 guest kernel 创建了一个单独的地址空间，并在每个地址空间中映射一个内核安全监控器（KSM, kernel security monitor），然后使用 PKS 将 KSM 与 guest kernel 隔离。PKS 隔离会剥夺 guest kernel 的权限，使其只能通过 KSM 或 host kernel 提供的接口执行特权操作。KSM 实现的特权操作只能访问一个安全容器的私有数据（如页表更新），这些操作可通过快速 PKS 开关门调用。</p><p>我们只会在 KSM 中映射这些私有数据，这样 PKS gate 在切换时就不需要进行侧信道保护（PTI, IBRS, …），这能节省上百个时钟周期；</p></li></ol><p>最后，我们实现了 CKI 的原型，并利用真实世界的容器应用对其进行了评估。我们在裸机云（bare-metal）和嵌套云中对 CKI 进行了评估，并将其与硬件辅助虚拟化（HVM）和基于软件的虚拟化（PVM）进行了比较。在裸机云中，与 HVM/PVM 相比，CKI 可将内存密集型应用的延迟时间最多减少 18%/47%。在嵌套云中，与 HVM/PVM 相比，CKI 最多可将内存密集型应用的延迟降低 72%/47%，并且与 HVM/PVM 相比，I/O 密集型应用最多可获得 6.8 倍/1.2 倍的吞吐量。</p><p>总之，本文做出了以下贡献：</p><ul><li><p>全面探索了安全容器的设计空间，揭示了 CPU 权限级别与容器内核分离需求之间的不匹配；</p></li><li><p>一种名为 CKI 的新型安全容器设计，通过引入软硬件共同设计的权限级别实现了高效的内核分离；</p></li><li><p>一个系统原型，其在实际应用中的实验结果证明了它的有效性。</p></li></ul><h2 id="2-背景和动机"><a href="#2-背景和动机" class="headerlink" title="2. 背景和动机"></a>2. 背景和动机</h2><h3 id="2-1-Secure-Container-Models"><a href="#2-1-Secure-Container-Models" class="headerlink" title="2.1 Secure Container Models"></a>2.1 Secure Container Models</h3><p><strong><u>操作系统级容器</u></strong>通过在多个容器之间共享单个操作系统内核来实现轻量级隔离。然而，操作系统级隔离机制很薄弱，因为在 Linux 等商品级操作系统内核中发现了许多漏洞。复杂的 syscall interface 向 userspace 暴露了巨大的攻击面，容器可利用这些攻击面进行权限升级、信息泄漏或拒绝服务（DoS）。</p><p>而 secure container 通过限制 OS kernel 被 compromised 的影响来加强容器隔离。这可以通过两种容器架构来实现： <strong><u>虚拟机级容器和基于 enclave 的容器</u></strong>，如图所示。</p><p><img src="esc/scm.png" width="550px" /></p><ul><li>虚拟机级容器。虚拟机级容器在单独的 guest kernel 上运行每个容器。它们要求用户应用程序、guest kernel 和 host kernel 具有三级权限。容器内的恶意应用程序可利用内核漏洞入侵其 guest kernel ，但这对 host kernel 或其他容器无害。guest kernel 与 host kernel 之间的接口可能比系统调用接口简单得多，因此恶意 guest kernel 很难入侵 host kernel。</li><li>基于 enclave 的容器。和 OS 级容器一样，基于 enclave 的容器在共享的 kernel 上运行所有容器。但是 Secure Monitor（BlackBox，有一个运行在 Root Kernel 中的监控器，类似虚拟机监控器）或基于硬件的可信执行环境（TEE），都会剥夺共享内核任意访问<u>受保护容器的内存数据或执行上下文</u>的权限。</li></ul><p>不过，如果考虑了 CVE 之后，虚拟机级容器就是首选方案了。在收集近两年（2022-2023 年）可在容器中利用的 Linux 内核 CVE，并按安全影响对其进行分类（共 209 个）后发现，这些 CVE，97.3% 可导致 DoS 攻击，包括破坏系统状态（如越界写入、use-after-free）、导致不可恢复的错误（如空指针延迟、kernel panic）或独占硬件资源（如内存泄漏、死锁）。虽然 enclave-based 的容器可以保护容器数据的机密性和完整性，但由于内核共享设计，它们<strong><u>无法抵御 DoS 攻击</u></strong>。相反，虚拟机级容器由于采用了内核分离设计，可以防止 DoS。</p><h3 id="2-2-Secure-Containers-in-Nested-Clouds"><a href="#2-2-Secure-Containers-in-Nested-Clouds" class="headerlink" title="2.2 Secure Containers in Nested Clouds"></a>2.2 Secure Containers in Nested Clouds</h3><p>根据先前的研究，在基础设施即服务（IaaS）云租用的虚拟机中构建容器平台的需求日益增长。在这些嵌套云中，VM 级容器部署在另一个虚拟机内。例如，阿里巴巴云正在将 secure containers 从 bare-metal 实例转移到通用（虚拟化）实例，以实现更高的隔离度、节约成本以及更灵活、更有弹性的 Kubernetes 集群管理。谷歌的 gVisor 通常会考虑虚拟机内部署，并设计相应的优化措施。</p><p>在 IaaS 虚拟机内运行安全容器需要嵌套虚拟化，其中一个 L0 内核（host hypervisor）运行一个带有 L1 内核（guest hypervisor）的虚拟机，然后隔离多个安全容器（L2 虚拟机，guest hypervisor 上的 VM），每个容器都有自己的 L2 内核。内存密集型和 I/O 密集型容器应用都可以在嵌套虚拟化下部署。<strong><u>我们的目标是减轻这些应用的嵌套虚拟化开销</u></strong>。</p><h3 id="2-3-Memory-Protection-Keys"><a href="#2-3-Memory-Protection-Keys" class="headerlink" title="2.3 Memory Protection Keys"></a>2.3 Memory Protection Keys</h3><p>Memory Protection Keys，内存保护密钥（MPK）是 x86 CPU 上用于内存隔离的新硬件功能。MPK 将虚拟地址空间中的页面最多分为 16 个域，并利用页表项 (PTE) 中以前未使用的四个位来表示每个页面的 domain ID。它还引入了一个 32 位 per-core protection key register，用于配置每个域的访问权限。权限可设置为只读、读写或不可访问。</p><p>MPK 有两种变体： Protection Keys for Userspace（PKU）控制用户页面的权限，而 Protection Key for Supervisor（PKS）控制内核页面的权限。PKU 和 PKS 的 protection key register 分别称为 PKRU 和 PKRS。PKRU 可以使用名为 <code>wrpkru</code>（write PKRU）的高效指令进行配置，而 PKRS 则可以使用 <code>wrmsr</code>（write MSR）指令进行配置。</p><h3 id="2-4-Issues-of-VM-Level-Containers"><a href="#2-4-Issues-of-VM-Level-Containers" class="headerlink" title="2.4 Issues of VM-Level Containers"></a>2.4 Issues of VM-Level Containers</h3><p>虚拟机级容器需要三个权限级别，而 CPU 硬件只为内核和应用程序提供两个权限级别。虚拟机级容器的一个问题是缺少第三个 CPU 权限级别来有效地适应客户（容器）内核。具体来说，下图展示了现有的虚拟机级容器设计，表格则显示了这些设计的比较。现有设计存在的问题归纳如下。</p><p><img src="esc/vmc-issue-design.png" /></p><ul><li>硬件辅助虚拟化（HVM）通过专用的虚拟机控制结构（VMCS）和 EPT 隔离了 guest kernel，导致 EPT 转换和管理造成内存性能不理想，以及虚拟机退出缓慢造成嵌套云中 I/O 性能不佳；</li><li>基于软件的虚拟化（PVM）将 guest kernel 剥夺为用户模式，并使用影子分页隔离容器内存，导致在处理系统调用时出现额外的上下文切换，以及页表更新效率低下；</li><li>基于 LibOS 的容器打破了应用程序与客户内核之间的隔离，降低了安全保证并导致兼容性问题。</li></ul><h4 id="2-4-1-HVM-的问题"><a href="#2-4-1-HVM-的问题" class="headerlink" title="2.4.1. HVM 的问题"></a>2.4.1. HVM 的问题</h4><p>The container design with HVM faces <strong><u>both performance and compatibility issues</u></strong>.</p><ul><li><p><strong><u>EPT 的开销</u></strong>。</p><ul><li><p>在裸机云中，HVM 处理一次 EPT fault 需要 3µs 的时间，与操作系统级容器相比，这可能会使内存密集型应用的延迟增加 2%∼21%。HVM 还会因 two-staged page table walk 导致昂贵的 TLB miss 处理，内存密集型应用的平均超限率为 46%。</p></li><li><p>在嵌套云中，由于没有硬件支持三级地址转换，L1 内核依赖 L0 内核为每个 L2 虚拟机维护一个影子 EPT（SPTE），从而导致 EPT 管理的高开销。</p><blockquote><p>这不就丢失了一部分半虚拟化的优势了吗？</p></blockquote></li><li><p>在嵌套云中，HVM 容器中的页面故障需要 32µs 以上的时间。与操作系统级容器（图 4 中的 HVM-NST / RunC-BM）相比，这使内存密集型应用的延迟增加了 28%∼226%。</p></li></ul></li><li><p><strong><u>虚拟机退出重定向的开销</u></strong>。</p><ul><li><p>在嵌套云中运行 HVM 容器时，L1 内核和 L2 VM 使用不同的 VMCS 执行，导致 L0 对 VM 退出进行干预。具体来说，当 L2 虚拟机发生 VM Exit 时，会触发一个 trap 到 L0 内核，然后 L0 内核恢复 L1 内核来处理 L2 虚拟机退出。L1 内核处理完虚拟机退出后，再次向 L0 内核发出陷阱，然后 L0 内核恢复 L2 虚拟机。</p><p>在裸机云中，HVM 容器中的空的 hyper-call 只需 1.1us，而在嵌套云中则需要 6.7us。与消除了 L0 干涉的 PVM 相比，这种虚拟机退出开销将 I/O 密集型应用的吞吐量降低了 1.8×∼4.3× 倍；</p></li></ul></li><li><p>兼容性问题。HVM 在嵌套云中也面临兼容性问题。首先，一些 IaaS 云禁用硬件辅助嵌套虚拟化，以减少 L0 内核的攻击面。其次，新兴的 Confidential VM（CVM）不支持硬件辅助嵌套虚拟化，因为 L0 内核不受信任；</p></li></ul><h4 id="2-4-2-PVM-的问题"><a href="#2-4-2-PVM-的问题" class="headerlink" title="2.4.2. PVM 的问题"></a>2.4.2. PVM 的问题</h4><p>PVM 利用半虚拟化技术实现 VM 级容器。它以用户和内核模式分别运行容器应用程序和 host kernel。它还在单独的地址空间内以 user mode 运行 guest（容器）kernel。PVM 通过避免虚拟机退出到 L0 内核，<u>在嵌套云中获得了比硬件辅助虚拟化（HVM）更好的性能</u>。</p><ul><li><p><strong><u>系统调用重定向的开销</u></strong>。当应用程序调用系统调用时，它 trap 给 host kernel。然后，host kernel 会切换到 guest kernel 的 page table、返回用户态、调用 guest kernel 的 syscall handler。当处理结束后，会以一个相反的路径返回用户态应用程序。</p><p>与本地系统调用相比，该系统调用过程增加了两个 CPU 模式开关和两个页表开关。这将系统调用延迟从 90ns 增加到 336ns；</p><p>与裸机云中的 HVM 相比，I/O 密集型应用的平均开销为 6.6%。</p></li><li><p><strong><u>shadow paging 的开销</u></strong>。PVM 通过使用影子分页机制，保留了两阶段地址转换的灵活性：GVA -&gt; GPA -&gt; HPA。Host 为每个容器应用维护一个 SPT（将 GVA 转换为 HPA）。一个容器 page fault 涉及 host kernel 和 guest 之间至少 6 次上下文切换。两次切换用于将 page fault 重定向到 guest kernel，两次用于更新 shadow page table（SPT 由 host 维护 read-only），另外两次用于返回用户应用。</p><p>此外，页面故障处理过程中的仿真逻辑也会产生很高的开销，例如 page table walking（<strong><u>需要确认在哪个翻译阶段</u></strong>）、指令仿真、影子页表管理和异常注入。PVM 容器中的页面故障需要 7µs 的时间，而本地页面故障只需要 1µs 的时间。与操作系统级容器（PVM-BM / RunC-BM）相比，影子分页的开销使内存密集型应用的延迟增加了 6%∼73%。</p></li></ul><h4 id="2-4-3-LibOS-和-gVisor-的问题"><a href="#2-4-3-LibOS-和-gVisor-的问题" class="headerlink" title="2.4.3. LibOS  和 gVisor 的问题"></a>2.4.3. LibOS  和 gVisor 的问题</h4><p>基于 LibOS 的 secure container <strong><u>将 LibOS attach 到进程或虚拟机中的每个容器上</u></strong>。它们在 secure container 内不执行用户-内核隔离（user/kernel isolation），而是<strong><u>在同一地址空间运行应用程序和 libOS</u></strong>。这种设计避免了系统调用处理过程中的页表切换，但削弱了容器的隔离保证。同时，它们通常兼容性较差，例如无法完全支持容器中的多进程。</p><blockquote><p>优点：避免页表切换；缺点：削弱隔离性、安全性，不保证兼容性（如多进程）；</p></blockquote><p>gVisor 实现了一个名为 Sentry 的新用户空间内核，每个容器都运行在一个私有的 Sentry 实例上。gVisor 让 host kernel 处理应用程序的 page fault，避免了影子分页的开销。不过，由于涉及 IPC，Systrap 比本地系统调用慢得多。同时，作为一个重新实现的内核，gVisor 可能缺乏 Linux 内核的完全兼容性和优化功能。</p><blockquote><p>优点：避免 shadow page 开销，但是 IPC 导致性能下降，并且 Sentry 缺乏 Linux 一样的兼容性和优化能力；</p></blockquote><h2 id="3-解决方案总览"><a href="#3-解决方案总览" class="headerlink" title="3. 解决方案总览"></a>3. 解决方案总览</h2><h3 id="3-1-设计内涵和选择"><a href="#3-1-设计内涵和选择" class="headerlink" title="3.1 设计内涵和选择"></a>3.1 设计内涵和选择</h3><p><strong><u>设计内涵</u></strong>：根据第 2.4 节中的分析，为容器 guest kernel 高效构建新权限级别有两个设计含义（为什么要为容器 guest kernel 构建一个新特权级？）：</p><ol><li><strong><u>权限级别之间的高效切换</u></strong>。由于 guest kernel 经常与应用程序通信（即系统调用/异常）并执行特权操作（如页表更新和 I/O 请求），我们应尽量减少这些过程中的上下文切换开销。应避免 PVM 中的系统调用重定向或嵌套 HVM 中的虚拟机退出重定向等过度开销；</li><li><strong><u>无两阶段地址转换的内存隔离</u></strong>。两阶段地址转换是为通用虚拟化设计的，超出了容器隔离的需求，因为容器不依赖特定的物理内存布局。使用单级转换可以避免影子分页或 EPT 转换/管理的开销。</li></ol><p><strong><u>设计选择</u></strong>：MPK 可在单个 CPU 权限级别内执行高效的域隔离，这可用于构建新的权限级别。有两种可能的设计：</p><ol><li>在用户模式下运行 guest kernel，并使用 PKU（Design-PKU）将其与应用程序隔离；</li><li>在内核模式下运行 guest kernel，并使用 PKS（Design-PKS）将其与 host kernel 隔离。</li></ol><p>这两种设计都支持无需重定向的高效系统调用。它们还能避免嵌套云中的虚拟机退出重定向，因为 guest kernel 和 host kernel 使用相同的 VMCS 执行。</p><p>我们选择 Design-PKS 而不是 Design-PKU，原因如下。</p><ul><li>首先，由于 PKU 已被广泛用于各种应用（进程内隔离），Design-PKU 本身会干扰这些现有用例。这种冲突破坏了 PKU 在用户空间应用中的初衷；</li><li>其次，Design-PKU 需要替换容器应用中的 <code>wrpkru</code> 指令和系统调用指令。然而，正如 Hodor 所强调的，在没有源代码的情况下对任意（容器）镜像进行二进制重写可能是不可判定的，这可能会破坏容器二进制的兼容性或妨碍应用功能（如及时编译）；</li><li>第三，Design-PKU 在异常处理中会产生额外的性能开销。例如，从 host 向 guest kernel 注入 page fault 需要额外的跨特权级切换，在我们的测试平台上，page fault latency（原本约为 1,000ns）增加了约 750ns；</li></ul><h3 id="3-2-基于-PKS-隔离的挑战"><a href="#3-2-基于-PKS-隔离的挑战" class="headerlink" title="3.2 基于 PKS 隔离的挑战"></a>3.2 基于 PKS 隔离的挑战</h3><p>PKS 最初并不是为隔离容器 guest kernel 而设计的，这导致我们的设计面临以下挑战：</p><ol><li><p><strong><u>隔离域数量不足</u></strong>。PKS 在一个地址空间中最多只能支持 16 个域，远远低于安全容器的潜在数量。因此，在专用的 PKS 域中隔离每个客户内核是不可行的；</p></li><li><p><strong><u>缺乏特权指令隔离</u></strong>。PKS 只提供内存隔离，而在内核模式下运行的恶意 guest kernel 可以使用特权指令破坏隔离。二进制重写是从隔离软件中删除特定指令的常用技术。然而，要消除操作系统内核中未对齐位置的所有特权指令是不可行的。</p><p>对于嵌套云，一种潜在的解决方案是使用虚拟化硬件拦截和监控 L1 虚拟机中的所有特权指令，但这需要对 L0 内核进行侵入式修改，可能并不可行。</p></li><li><p><strong><u>switch gate 功能不完整</u></strong>。在基于 PKS 的隔离下，guest kernel 使用 PKS switch gate 与 host kernel 通信。但是，这种 switch gate 需要 PKS 原本不支持的功能。例如，host kernel 需要在 guest 执行期间使用 switch gate 拦截硬件中断，但原本实现的 switch gate 设计可能会让 guest kernel 向 host kernel 注入假中断。</p><blockquote><p>通俗来说，硬件中断（比如键盘输入、网络数据到达等）需要由主机内核统一处理，这个过程要通过 switch gate 机制来完成。但如果这个机制设计得太简单，恶意的客户内核就可能模仿硬件中断的信号格式，伪造出假的中断请求发送给主机内核。</p><p>可能导致的问题，一是主机内核被大量虚假请求干扰，无法处理真实的硬件中断（DoS）；二是假中断可能携带恶意指令，导致主机内核的安全机制被绕过；</p></blockquote></li></ol><h3 id="3-3-设计架构总览"><a href="#3-3-设计架构总览" class="headerlink" title="3.3 设计架构总览"></a>3.3 设计架构总览</h3><p>CKI 是一种虚拟机级容器架构，每个容器都运行在独立的内核上。它在不使用硬件虚拟化扩展的情况下实现了三种权限级别。PVM 是最先进（SOTA）的安全容器设计，不使用虚拟化硬件。下图显示了 CKI 的架构及其与 PVM 的不同之处。简而言之，CKI 避免了基于软件的虚拟化的系统调用重定向和影子分页的开销，因此无论在裸机还是嵌套云中都能获得更好的性能。</p><p><img src="esc/cki-arch.png" /></p><h4 id="3-3-1-抽象"><a href="#3-3-1-抽象" class="headerlink" title="3.3.1 抽象"></a>3.3.1 抽象</h4><p>每个 CKI 安全容器就像一个虚拟机，有一个 guest kernel 和多个用户进程，运行在 host kernel 上。guest kernel 提供操作系统功能，如内存管理、调度、文件系统和网络堆栈。host kernel 调度 guest 的 vCPU，为其分配内存，并使用 VirtIO 协议为 guest kernel 模拟虚拟设备（磁盘和网卡）。<strong><u>所有硬件中断都由 host kernel 处理</u></strong>。当 guest kernel 需要调用 host 功能或发生硬件中断时，guest vCPU 会通过一段名为 switcher 的上下文切换代码退出到 host kernel（第 4.2 节）。<strong><u>对于嵌套虚拟化，CKI 虚拟机退出过程不涉及 L0 干预</u></strong>（guest kernel 和 host kernel 均位于内核态）。host kernel 在恢复 guest vCPU 时可能会注入虚拟中断。</p><h4 id="3-3-2-与-PVM-的区别"><a href="#3-3-2-与-PVM-的区别" class="headerlink" title="3.3.2 与 PVM 的区别"></a>3.3.2 与 PVM 的区别</h4><p>CKI 与基于软件的虚拟化（PVM）有两个主要区别。</p><ul><li><p>首先，CKI 在内核模式下以新的权限级别运行 guest kernel，使客户用户无需 host kernel 干预即可调用系统调用。<strong><u>guest kernel 的内存映射在 guest 用户地址空间中，并通过 PTE U/K 位隔离</u></strong>，从而消除了系统调用时的页表切换；</p><blockquote><p>不加 KPTI 的时候，Linux 不也是把 kernel 的内存映射到用户地址空间的吗？</p></blockquote></li><li><p>其次，<strong><u>CKI 不实现两阶段地址转换。host kernel 为每个 guest VM 提供一些连续的 HPA 段</u></strong>，由 guest kernel 中的内存管理器直接管理。因此，guest user page fault 可由 guest kernel 直接处理，而不是在 host kernel 中触发 shadow page fault。通过消除虚拟机退出、GPA 到 HPA 转换和影子 PTE 生成，PTE 更新操作也得到了简化。</p></li></ul><h4 id="3-3-3-定义新的权限级别（Guest-Kernel-Mode）"><a href="#3-3-3-定义新的权限级别（Guest-Kernel-Mode）" class="headerlink" title="3.3.3 定义新的权限级别（Guest Kernel Mode）"></a>3.3.3 定义新的权限级别（Guest Kernel Mode）</h4><p>CKI 利用基于 PKS 的内核内部隔离来构建新的权限级别，以消除 guest kernel 的权限。一方面，它将 PKS 隔离与地址空间隔离相结合，限制 guest kernel 的内存访问权限（PKS 原本就进行内存隔离，但不提供特权指令隔离）。另一方面，它还能监控来自 guest kernel 的特权操作的执行情况（需要扩展，参见下文）。</p><p>具体来说，不同的 secure container 和 host kernel 被隔离在不同的地址空间。每个 guest kernel 都是非特权内核，与特权内核安全监控程序（KSM）一起运行，两者运行在同一地址空间，但在 PKRS（内核页保护密钥权限寄存器）中指定的 PKS 权限不同。KSM （PKRS 为零）可以访问所有虚拟内存，而 guest kernel（PKRS 为 <code>PKRS_GUEST</code>）不能访问其 KSM 的内存。</p><blockquote><p>因此，在每个 secure container 的地址空间内，guest kernel 和 KSM 只需要两个 PKS 域。因此，CKI 可以支持任意数量的安全容器，而不受 PKS 域限制的影响（<strong><u>克服挑战-1</u></strong>）。</p></blockquote><p>此外，CKI 还为 PKS 增加了一个轻量级硬件扩展，使特权指令在 guest kernel 中不可执行（参见 4.1）（<strong><u>克服了挑战-2</u></strong>）。</p><p>guest kernel 只能通过其 KSM 或 host kernel 提供的预定义接口执行特权操作。KSM 实现的特权操作只能访问安全容器的私有数据，例如页表更新（参见 4.3，这不需要下陷到 L0）和 <code>iret</code> 指令。<strong><u>这些特权操作可通过高效的 PKS gate（guest kernel 与 KSM 之间的切换）调用</u></strong>（参见 4.2）。由于只有私有数据才会映射到 KSM 中，CKI 从 PKS gate 中消除了代价高昂的侧信道缓解方案（如 PTI 和 IBRS）。</p><p>其他特权操作（如 VirtIO MMIO、定时器设置、<code>hlt</code> 指令）依赖于全局数据（如驱动程序/调度程序元数据），由 host kernel 提供。guest kernel 可通过专门设计的 switcher（guest kernel 与 host kernel 之间的切换器）调用此类操作。switcher 还包含中断门，可在 guest VM 执行期间拦截硬件中断，并将其重定向到 host kernel。CKI 依靠多种技术防止中断垄断和中断伪造（参见 4.4）（<strong><u>克服挑战-3</u></strong>）。</p><h3 id="3-4-威胁模型"><a href="#3-4-威胁模型" class="headerlink" title="3.4 威胁模型"></a>3.4 威胁模型</h3><p>CKI 继承了 VM 级容器的威胁模型。host kernel 和 KSM 隔离多个容器，而容器中的 guest kernel 则隔离多个用户进程。一个容器可能会被入侵，然后试图打破容器间隔离，例如执行破坏性特权指令或破坏关键内存结构（如页表、IDT）。由于 KSM 和 host kernel 的攻击面较小（hypervisor 接口），因此假定它们是可信的。</p><p>单个安全容器内的瞬时执行攻击不在攻击范围内。容器间的瞬时执行攻击可通过在每个容器自己的地址空间中运行并在 host kernel 中启用 Spectre 缓解功能来缓解。</p><h2 id="4-设计细节"><a href="#4-设计细节" class="headerlink" title="4. 设计细节"></a>4. 设计细节</h2><h3 id="4-1-基于-PKS-的特权指令隔离"><a href="#4-1-基于-PKS-的特权指令隔离" class="headerlink" title="4.1 基于 PKS 的特权指令隔离"></a>4.1 基于 PKS 的特权指令隔离</h3><p>出于性能考虑，CKI 选择在内核模式下构建新的权限级别，也就是说，容器用户进程可以以原有的高效方式与（容器）guest kernel 进行交互。例如，进程仍可通过 syscall 指令直接调用系统调用，而无需额外的上下文切换。</p><p>由于 guest kernel 不受信任，CKI 需要防止它执行特权指令，从而破坏安全隔离。目前的 PKS 硬件功能只能在内核模式下提供内存隔离，无法限制特权指令的执行。此外，现有的基于软件的指令隔离技术（如二进制重写）也不适用于 CKI（第 3.2 节、第 3.1 节）。</p><h4 id="4-1-1-硬件扩展"><a href="#4-1-1-硬件扩展" class="headerlink" title="4.1.1 硬件扩展"></a>4.1.1 硬件扩展</h4><p>因此，CKI 引入了一种轻量级硬件扩展，以防止 guest kernel 执行可能导致破坏性序列的特权指令。由于 PKRS 在 guest kernel 执行期间为非零（有限内存视图），而在 KSM 执行期间为零（无限内存视图），因此该扩展可以依靠 PKRS 寄存器的值来确定当前执行的是哪一个。当 PKRS 非零时，扩展会阻止所有破坏性特权指令。在客户内核中执行这些指令会触发异常，并向 host kernel 发出 trap。非破坏性特权指令仍可在客户内核中执行，以尽量减少开销。下表列出了特权指令及其在客户内核中是否被阻止（通过控制这个规则可以控制哪种指令会下陷到 host kernel）。</p><p><img src="esc/pks-blk.png" /></p><blockquote><p>注：block 表示如果 PKRS 非零时触发是否会阻塞并 trap 给 host kernel。</p></blockquote><h4 id="4-1-2-阻塞指令"><a href="#4-1-2-阻塞指令" class="headerlink" title="4.1.2 阻塞指令"></a>4.1.2 阻塞指令</h4><blockquote><p>任何写入系统寄存器、控制寄存器、<code>iret</code>、控制中断等指令（中断控制通过 host kernel 可见内存位）；</p></blockquote><p>除了上表中列出的无害指令外，大多数特权指令都被阻止。<strong><u>被屏蔽的指令可以使用基于软件的虚拟化中的类似技术进行虚拟化，即用调用宿主内核或 KSM 来代替它们</u></strong>。</p><p>我们屏蔽了任何写入系统寄存器（如 <code>GDTR</code> 和 <code>IDTR</code>）、控制寄存器或特定模型寄存器（MSR）的指令。中断返回指令 (<code>iret</code>)可能会修改段寄存器，因此会被阻止。我们还屏蔽了对容器 guest kernel 来说不必要的指令，如与 Port I/O 和系统管理模式相关的指令。</p><p>操作系统内核使用 <code>cli</code>/<code>sti</code> 和 <code>popf</code> 指令启用或禁用 CPU 上的中断处理。这些指令在 guest kernel 中被阻止，以防止 DoS。<strong><u>CKI 采用了半虚拟化的中断处理机制</u></strong>。所有硬件中断都由 host kernel 处理，host kernel 再向 guest kernel  注入虚拟中断。guest kernel 不使用特权指令管理中断启用/禁用状态（guest-aware 所以是半虚拟化），而是通过 host kernel 可见的内存位来管理。</p><h4 id="4-1-3-无阻塞指令"><a href="#4-1-3-无阻塞指令" class="headerlink" title="4.1.3 无阻塞指令"></a>4.1.3 无阻塞指令</h4><blockquote><p>修改 PKRS 的指令。x86 上使用 MSR 寄存器（特定模型寄存器），但不直接用 <code>wrmsr</code>（block），用包装后的新指令 <code>wrpkrs</code>；</p><p>并且限制 <code>wrpkrs</code> 的使用范围，二进制重写消除内核代码中的该指令，并 kernel 只读 + KSM 禁止新的可执行映射来避免安全问题（CKI 值用来提供容器环境，不需要支持那么多东西）。</p><p><code>sysret / swapgs</code> 系统调用相关也可以不阻塞。但是需要指令扩展来确保 PKRS 非零时中断不得被禁用（防止 DoS）；</p><p>关于 TLB flush 相关指令 <code>invlpg</code>（仅刷新当前 PCID 的 TLB 条目）是允许的，因为 secure container 和 host 隔离在不同的 PCID 上下文中，这能防止 performance attack；</p></blockquote><p>对 PKRS 寄存器的修改指令应能在 guest kernel 中执行，否则 guest kernel 将无法调用 KSM。现有的 x86 硬件将 PKRS 作为特定模型寄存器 (MSR) 来实现。但是，<code>wrmsr</code> 指令应在 guest kernel 中被阻止，以防止对其他 MSR 的任意操作。我们为修改 PKRS 引入了新的硬件指令 <code>wrpkrs</code>，其语义类似于现有的 <code>wrpkru</code> 指令（修改 PKRU，相当于 PKRS 的用户空间）。</p><p><code>wrpkrs</code> 指令只应出现在预先定义的 switch gates 上，因此我们使用先前工作中引入的类似<strong><u>二进制重写技术，消除了访客内核代码中的所有 <code>wrpkrs</code> 指令</u></strong>，包括未对齐指令。为防止客户内核动态创建 <code>wrpkrs</code> 指令，<strong><u>所有内核代码在 guest kernel 初始化期间都被映射为只读，KSM 禁止在容器执行期间进行新的内核可执行映射</u></strong>。</p><blockquote><p>[!NOTE] </p><p>CKI 不需要支持动态修补或加载 guest kernel 代码，因为这对容器来说是不必要的。请注意，CKI 的目的是提供一个容器环境，而不是支持任意的 guest kernel。</p></blockquote><p><code>sysret</code> 和 <code>swapgs</code> 指令用于处理系统调用。为这些指令调用 KSM 会将空系统调用延迟从 90ns 增加到 153ns。为了提高性能，我们允许这些指令在 guest kernel 中执行。sysret 指令可能会被用来修改 RFLAGS 寄存器并禁用内核中断（DoS）。因此，我们为该指令添加了一个轻量级扩展，以确保当 PKRS 非零时，IF（中断启用）标志保持开启。</p><p>guest kernel 可以使用 <code>invlpg</code> 清除 TLB。我们将每个 secure container 和 host 隔离在不同的 PCID 上下文中，以防止 performance attack，因为 <code>invlpg</code> 只刷新当前 PCID 的 TLB 条目。</p><h3 id="4-2-KSM-中用于-Context-Switch-的-Switch-Gates"><a href="#4-2-KSM-中用于-Context-Switch-的-Switch-Gates" class="headerlink" title="4.2 KSM 中用于 Context-Switch 的 Switch Gates"></a>4.2 KSM 中用于 Context-Switch 的 Switch Gates</h3><p>下图显示了 CKI 中的上下文切换流程。CKI 为最频繁的切换（即系统调用、异常和 KSM 调用）提供了快速路径。它为其他切换（即 host kernel 调用（hypercall）和硬件中断）提供慢速路径。</p><p><img src="esc/cki-paths.png" width="450px" /></p><p>在为这些上下文切换设计 PKS switch gates 时，KSM 中每 vCPU 区域的定位是一个难题。由于 guest kernel 可以任意修改 <code>kernel_gs</code>（Intel 中存放 per-CPU 信息地址的寄存器，有点像 AArch64 的 <code>TPIDR_EL1</code>），因此 KSM 无法依靠该寄存器来识别当前的 vCPU。</p><h4 id="4-2-1-系统调用和异常"><a href="#4-2-1-系统调用和异常" class="headerlink" title="4.2.1 系统调用和异常"></a>4.2.1 系统调用和异常</h4><p>当容器中的用户应用程序调用系统调用时，它会捕获到 <code>IA32_STAR</code> 寄存器中定义的 guest kernel 入口点。同样，当应用程序触发 page fault 等异常时，它也会跳转到中断描述符表（IDT）中的 guest kernel 入口点。在用户模式下，PKRS 被设置为 <code>PKRS_GUEST</code>，允许入口点调用不受信任的处理程序函数，而无需切换 PKS。系统调用和异常的进入和退出代码使用三条特权指令：<code>swapgs</code>、<code>sysret</code> 和 <code>iret</code>。<code>swapgs</code> 和 <code>sysret</code> 指令可在 guest kernel 中执行，而 <code>iret</code> 指令必须通过调用 KSM 才能执行（参见 4.1）。</p><h4 id="4-2-2-KSM-调用"><a href="#4-2-2-KSM-调用" class="headerlink" title="4.2.2 KSM 调用"></a>4.2.2 KSM 调用</h4><p>guest kernel 使用 <strong><u>KSM call gate</u></strong> 调用 KSM 提供的特权操作（下图）。该门将 PKRS 设为 0，切换到 guest kernel 无法访问的安全堆栈，调用处理函数，最后恢复 PKRS 和堆栈指针。</p><p><img src="esc/ksm-call-gate.png" width="450px" /></p><p>攻击者可能会利用类似 ROP 的攻击跳转到门结束时的 <code>wrpkrs</code> 指令，任意修改 PKRS 并执行恶意代码。为防止这种攻击，如上图中的 <code>switch_pks</code> 宏所示，修改后会检查新的 PKRS 值。</p><blockquote><p>[!IMPORTANT]</p><p>既然 <code>kernel_gs</code> 不可信，如何识别 per-vCPU 区域？</p><p>由于 KSM 可在多个 vCPU 上同时调用，因此每个 vCPU 都有自己的安全堆栈，位于 KSM 内存的每个 vCPU 区域。操作系统内核通常使用 <code>kernel_gs</code> 寄存器来定位每个 CPU 的变量，即每个 CPU 上的 <code>kernel_gs</code> 寄存器为本地 CPU 变量存储不同的基数。然而，CKI 允许 guest 执行 <code>swapgs</code> 指令（参见 4.1），因此恶意客户内核可以任意修改 <code>kernel_gs</code>。为了解决这个问题，<strong><u>CKI 将每 vCPU 区域放在一个恒定的虚拟地址上</u></strong>，这样就可以在没有 <code>kernel_gs</code> 的情况下找到它。</p><p>如下图所示，CKI 为 guest kernel 中的每个页表维护多个 per-vCPU 页表。当 guest thread 在不同的 vCPU 上执行时，会使用不同的 per-vCPU 页表。每个 per-vCPU 页表在相同的恒定的虚拟地址 (GVA) 上映射不同的 per-vCPU 区域 (HPA)。</p><p><img src="esc/cki-percpu-pt.png" width="250px" /></p></blockquote><h4 id="4-2-3-Hypercall"><a href="#4-2-3-Hypercall" class="headerlink" title="4.2.3 Hypercall"></a>4.2.3 Hypercall</h4><p>Guest kernel 使用 hypercall gate 调用 host kernel 提供的特权操作。该门首先将 PKRS 切换为零，因为它需要执行特权指令并访问 KSM 内存（每虚拟 CPU 区域）。然后，它执行一次完整的上下文切换，以保存 guest kernel 上下文并恢复 host kernel 上下文，其中包括页表切换、通用/系统寄存器切换和<u><strong>侧信道缓解</strong></u>（如 IBRS）。host 和 guest 上下文存储在每 vCPU 区域。然后，host kernel 从 guest 上下文读取请求并进行处理。请求完成后，host kernel 恢复 guest 上下文，guest kernel 从 hypercall 门恢复。</p><h4 id="4-2-4-Hardware-Interrupt"><a href="#4-2-4-Hardware-Interrupt" class="headerlink" title="4.2.4 Hardware Interrupt"></a>4.2.4 Hardware Interrupt</h4><p>硬件中断会触发从 guest 到 host kernel 的 trap。硬件中断的 IDT 条目指向一个 interrupt gate；</p><p>中断门将中断信息保存到每 vCPU 区域，然后切换到 host kernel。host kernel 读取信息，构建中断上下文，并调用中断处理程序。处理完中断后，host kernel 会恢复被中断的 guest 上下文。</p><p>我们添加了一个<strong><u>硬件扩展，用于在中断进入时保存 PKRS 寄存器，并将 PKRS 切换为零</u></strong>（参见 4.4）。处理中断后，<code>iret</code> 指令应在 PKRS 设置为零时执行（参见 4.1），但在恢复 guest kernel 上下文时，它需要将 PKRS 重新设置成 <code>PKRS_GUEST</code>。因此，我们<strong><u>扩展了 <code>iret</code> 指令，允许它修改 PKRS 寄存器</u></strong>。</p><p><img src="esc/cki-hvc-irq.png" width="450px" /></p><h3 id="4-3-内存保护机制"><a href="#4-3-内存保护机制" class="headerlink" title="4.3 内存保护机制"></a>4.3 内存保护机制</h3><p>恶意 guest kernel 可能会试图通过操纵页表来破坏内存隔离。为了隔离客户机的内存视图，KSM 会拦截并验证 guest kernel 中的所有页表更新（回忆页表更新是不涉及 L0 kernel 并且仅与 KSM private data 有关）。</p><h4 id="4-3-1-页表监控"><a href="#4-3-1-页表监控" class="headerlink" title="4.3.1 页表监控"></a>4.3.1 页表监控</h4><p>为了拦截页表更新，CKI 采用了与嵌套内核类似的机制，该机制基于以下不变式（invariant）：</p><ol><li>只有已声明的页才能用作页表页（PTP）；</li><li>已声明的 PTP 在 guest kernel 中是只读的（<strong><u>KSM / host kernel 才能修改</u></strong>）；</li><li>只有已声明的顶级 PTP 才能加载到 CR3 寄存器中。</li></ol><p>与嵌套内核不同，CKI 使用 PKS 而不是 PTE writable bit 来控制 PTP 的写入权限。CKI 将客户虚拟地址空间（GVA）中的所有 PTP 都划分到一个特定的 PKS 域中。它将 PKS 域 ID（<code>pkey_PTP</code>）添加到映射 PTP 的每个客户 PTE 中。执行 guest kernel 时，该 PKS 域在 PKRS 寄存器中被配置为只读。</p><blockquote><p>简言之：<strong><u>guest 页表所在页使用专用 PKS domain，在 guest kernel 执行时只读</u></strong>；</p></blockquote><p><strong><u>KSM 会为属于 guest 的每个物理页面维护一个描述符</u></strong>。guest kernel 可调用 KSM 声明 PTP 或更新 PTE。</p><ul><li>声明 PTP 时，会指定 PTP 级别并记录在描述符中。然后，KSM 会在页表中查找映射该 PTP 的 PTE，并将 <code>pkey_PTP</code>（PKS domain ID）添加到该 PTE 中。</li><li><strong><u>KSM 还会检查描述符中的引用计数器，以确保 PTP 只被映射一次</u></strong>。</li><li>更新 PTE 时，KSM 会验证新 PTE 是否指向有效的下一级 PTP 或属于客户的数据页，而不会映射已声明的 PTP。</li><li>此外，为防止恶意 <code>wrpkrs</code> 指令（参见 4.1），如果新映射是内核可执行的，KSM 将禁止更新。</li></ul><h4 id="4-3-2-Per-vCPU-的页表"><a href="#4-3-2-Per-vCPU-的页表" class="headerlink" title="4.3.2 Per-vCPU 的页表"></a>4.3.2 Per-vCPU 的页表</h4><p>如第 4.2 节所述，CKI 为每个 guest 页表维护多个每 vCPU 页表。每个 per-vCPU 页表映射 KSM 内存中不同的 per-vCPU 区域。具体来说，<strong><u>KSM 会为客户机中的每个顶级 PTP 维护多个 per-vCPU 副本</u></strong>。</p><ul><li>声明顶级 PTP 时，KSM 会将自己的代码和数据映射（包括 per-vCPU 区域）添加到每个副本中（有点像 kernel 给用户态程序配置页表的情形）。</li><li>当 guest kernel 调用 KSM 更新 CR3 时，KSM 会验证新的 CR3 值是否指向已声明的顶级 PTP，然后将相应的 PTP 副本加载到 CR3 中。</li><li>此外，KSM 还为读取顶层 PTP 中的 PTE 提供了一个接口，访问/脏位会从副本传播到原始 PTP。</li></ul><h4 id="4-3-3-与-Shadow-Paging-的比较"><a href="#4-3-3-与-Shadow-Paging-的比较" class="headerlink" title="4.3.3 与 Shadow Paging 的比较"></a>4.3.3 与 Shadow Paging 的比较</h4><p>与影子分页相比，CKI 的性能优势来自于更轻量级的 page fault 和 PTE 更新流程：</p><p>1) 轻量级页面故障。由于没有两阶段地址转换，CKI 中的用户页面故障可由客户内核直接处理。相比之下，在影子分页下，用户页面故障会被主机内核拦截，主机内核会执行页表走行以确定页面故障的类型（第一阶段或第二阶段），然后将页面故障注入客户内核。<br>2) 轻量级 PTE 更新。</p><ul><li>首先，在影子分页下，guest 中的 PTE 更新会触发虚拟机退出到 host kernel。相比之下，CKI 中的 guest kernel 可通过轻量级 PKS gate 执行 KSM call 从而进行 PTE 更新；</li><li>其次，影子分页将 GPA 与 QEMU 进程的虚拟内存区（VMA）关联起来。向 PTE 写入 GPA 时，必须从 VMA 的映射中找到与 GPA 相关的 HPA，这非常耗时。相反，CKI 将 HPA 委托给 guest kernel，允许 guest kernel 直接填充 PTE 中的 HPA，而不是 GPA。</li></ul><p>CKI 的一个局限是，<strong><u>它为每个 secure container 分配连续的物理内存段，这可能会由于内存碎片化而导致内存利用率较低</u></strong>。之所以做出这样的设计选择，是因为细粒度的离散内存分配会带来两个问题：</p><ol><li>它要求 KSM 在验证 PTE 更新时搜索大量元数据（页面粒度而不是段粒度），从而导致性能下降；</li><li>其次，容器内核（Linux）采用 buddy system 来管理物理内存，该系统可通过连续的内存段高效运行；</li></ol><p>因此，我们优先考虑运行时间性能，而不是内存利用效率。</p><h3 id="4-4-防止中断滥用"><a href="#4-4-防止中断滥用" class="headerlink" title="4.4 防止中断滥用"></a>4.4 防止中断滥用</h3><p>被入侵的 guest kernel 有三种通过滥用中断发起 DoS 攻击的潜在方法。</p><ol><li>中断垄断。它可能会修改中断门的代码，以独占所有中断。这样，主机和其他容器就无法再接收中断；</li><li>中断栈破坏。它可能会操纵中断堆栈，引发无法恢复的故障。具体来说，当中断发生时，CPU 会将上下文数据推入中断栈。如果中断发生在内核模式下，CPU 默认使用中断发生时的堆栈作为中断堆栈。恶意访客内核可能会将堆栈指针设置为无效地址，导致 CPU 尝试推送数据时出现三重故障；</li><li>中断伪造。它可以伪造中断，用不必要的中断请求压垮系统，从而降低系统性能或导致 host kernel 出现未定义的行为。</li></ol><p>CKI 可以防御所有这些攻击。</p><h4 id="4-4-1-防止中断垄断"><a href="#4-4-1-防止中断垄断" class="headerlink" title="4.4.1 防止中断垄断"></a>4.4.1 防止中断垄断</h4><p>CKI 有以下策略防止中断垄断：</p><ol><li><strong><u>门不可修改</u></strong>（位于 KSM）。CKI 在 KSM 内存中分配 IDT 和 interrupt gate 代码，使 guest kernel 无法修改它们；</li><li><strong><u>Guest Kernel 权限剥夺</u></strong>。它使用 PKS gate 权限剥夺机制（参见 4.1 节）确保 guest kernel 无法禁用中断处理或修改 IDTR（IDT 基地址寄存器）；</li><li><strong><u>门代码映射不可修改</u></strong>（位于 KSM）。guest kernel 不能更改或删除 IDT 或 interrupt gate 代码的映射，因为 KSM 在每个激活的页表中映射了自己的内存（参见 4.3）；</li></ol><p>有了这些机制，当中断发生时，CPU 控制流总能切换到正确的中断门。</p><h4 id="4-4-2-防止中断堆栈破坏"><a href="#4-4-2-防止中断堆栈破坏" class="headerlink" title="4.4.2 防止中断堆栈破坏"></a>4.4.2 防止中断堆栈破坏</h4><p>CKI 利用 x86 中断堆栈表 (IST) 功能，确保 CPU 始终使用正确的中断堆栈。具体来说，IST 允许设置特定的中断堆栈，并强制 CPU 在推送中断上下文之前切换到该堆栈。IST 初始化由 KSM 完成（guest kernel 无法执行相关特权指令），相应内存也位于 KSM 中（guest kernel 无法修改相应内存数据）。</p><p>简而言之，<strong><u>x86 IST + KSM 来初始化和内存存放</u></strong>。</p><h4 id="4-4-3-防止中断伪造"><a href="#4-4-3-防止中断伪造" class="headerlink" title="4.4.3 防止中断伪造"></a>4.4.3 防止中断伪造</h4><p>由于 interrupt gate 需要访问 KSM 内存并执行特权指令，因此当 guest kernel 发生中断时，它需要首先将 PKRS 切换为零。如果在门内通过 <code>wrpkrs</code> 指令进行切换，那么恶意 guest kernel 就会直接跳转到其中一个中断门，并向 host kernel 发送伪造的中断。</p><p>因此为了防止伪造中断，CKI 扩展了 IDT 配置，除了切换中断堆栈等原有功能外，还进一步支持切换 PKRS 寄存器。如下图中蓝色下划线文本所述，当发生硬件中断时，这一微小的硬件扩展会自动将 PKRS 寄存器置零。</p><p><img src="esc/cki-hvc-irq.png" width="450px" /></p><p>所以，interrupt gate 中没有 <code>wrpkrs</code> 指令。如果 guest kernel 跳转到门入口，PKRS 将保持 <code>PKRS_GUEST</code>，导致后续上下文切换失败。请注意，应用程序或 guest kernel 可能会使用 int 指令生成软件中断。<strong><u>硬件扩展只在硬件中断时切换 PKRS，而在软件中断时保持 PKRS 不变</u></strong>。</p><p>此外，guest kernel 也无法滥用 hypercall gate 进行中断伪造，因为 host kernel 可以根据 KSM（每 vCPU 内存区）中保存的信息识别不同的退出原因。</p><h2 id="5-实现"><a href="#5-实现" class="headerlink" title="5. 实现"></a>5. 实现</h2><h3 id="5-1-Guest-Kernel"><a href="#5-1-Guest-Kernel" class="headerlink" title="5.1 Guest Kernel"></a>5.1 Guest Kernel</h3><p>我们在 CKI 容器中将 Linux 内核作为访客内核运行。我们利用 Linux 内核中的半虚拟化实用程序（即 <code>pv_ops</code>）来 hook 特权操作。我们还在 Linux 内核中添加了一个新的启动程序，以移除传统的初始化操作。我们增加了 2000 行代码，修改了不到 80 行代码。</p><p>移除两阶段地址转换不需要大量的移植工作。</p><ul><li>传统内核可能依赖固定的低物理地址来启动真实模式。相反，CKI 通过半虚拟化直接从 long mode 启动虚拟 CPU（vCPU）；</li><li>传统的虚拟化堆栈使用两阶段地址转换来创建 MMIO 区域，这些区域在第一阶段映射，但在第二阶段不映射。我们用 hypercall 取代了客户内核（VirtIO 前端）中的 MMIO；</li></ul><p>在兼容性方面，CKI 有可能支持与基于软件的虚拟化相同的客户内核功能。</p><h3 id="5-2-Hardware-Extensions"><a href="#5-2-Hardware-Extensions" class="headerlink" title="5.2 Hardware Extensions"></a>5.2 Hardware Extensions</h3><p>第 7 节中的性能评估是在真实硬件而非模拟器上进行的。在评估中，我们使用 <code>wrpkru</code> 指令来模拟 <code>wrpkrs</code> 指令。根据我们基于 Gem5 模拟器的评估，在特权指令中添加 PKS 权限检查逻辑产生的开销可以忽略不计，因此我们在评估中直接使用未修改的指令。我们通过添加 wrpkru 指令来模拟中断进入和 <code>iret</code> 期间的 PKRS switching 开销。</p><h2 id="6-安全性分析"><a href="#6-安全性分析" class="headerlink" title="6. 安全性分析"></a>6. 安全性分析</h2><p>CKI 可以实现与基于软件的虚拟化（PVM）相同的安全目标，因为它实现了相同的隔离基元（见下图）。</p><ol><li><strong><u>页表和内存隔离</u></strong>（PKS）：PVM 通过 PTE U/K 位和单独的页表将 switcher 和 host kernel 内存与 guest kernel 隔离，而 CKI 通过 PKS 和独立页表将 KSM 和 host kernel 内存与 guest kernel 隔离（参见 4.3 节）；</li><li><strong><u>特权指令隔离</u></strong>（PKS EXT 权限剥夺）：PVM 通过在用户模式下运行，防止虚拟机 guest kernel 执行特权指令，而 CKI 通过 PKS 限制 guest kernel 执行特权指令（参见 4.1）；</li><li><strong><u>特权转换防护</u></strong>（Call Gates，包括 KSM call，Hypercall 等）：PVM 为虚拟机提供了一个预定义的系统调用入口点来调用主机内核。CKI 采用二进制重写技术，消除了客户内核中的 <code>wrpkrs</code> 指令，只留下有效的入口点来调用 KSM 或主机内核（参见 4.2）；</li><li><strong><u>中断防护</u></strong>（Interrupt Gates）：在 PVM 中，当虚拟机被硬件中断中断时，CPU 会调用 IDT 中定义的相应主内核处理函数。CKI 设计了中断门，将硬件中断重定向到主机内核，确保中断门不会被破坏或滥用（参见 4.4）。</li></ol><p><img src="esc/cki-sec.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这是一篇 2025 年的关于软硬协同的安全容器设计的文章。&lt;/p&gt;
&lt;h2 id=&quot;0-Overview&quot;&gt;&lt;a href=&quot;#0-Overview&quot; class=&quot;headerlink&quot; title=&quot;0. Overview&quot;&gt;&lt;/a&gt;0. Overview&lt;/h2&gt;&lt;p</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="Container" scheme="https://blog.sjtuxhw.top/tags/Container/"/>
    
    <category term="OS" scheme="https://blog.sjtuxhw.top/tags/OS/"/>
    
    <category term="Paper" scheme="https://blog.sjtuxhw.top/tags/Paper/"/>
    
    <category term="VM" scheme="https://blog.sjtuxhw.top/tags/VM/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱：Machine Learning Roadmap</title>
    <link href="https://blog.sjtuxhw.top/technical/ml-roadmap/"/>
    <id>https://blog.sjtuxhw.top/technical/ml-roadmap/</id>
    <published>2025-06-08T06:59:31.000Z</published>
    <updated>2025-06-11T07:42:57.145Z</updated>
    
    <content type="html"><![CDATA[<p>笔者感觉 ML 这块知识点太多，互联网上多数信息都难以结构化，尤其是一个方向的知识火起来后，每个人都写一篇博客，看的眼花缭乱。。因此笔者简单总结了一下机器学习领域的知识图谱，方便知识体系构建和回顾。</p><p>如有错误，欢迎读者勘误斧正。</p><p><img src="ml-roadmap.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;笔者感觉 ML 这块知识点太多，互联网上多数信息都难以结构化，尤其是一个方向的知识火起来后，每个人都写一篇博客，看的眼花缭乱。。因此笔者简单总结了一下机器学习领域的知识图谱，方便知识体系构建和回顾。&lt;/p&gt;
&lt;p&gt;如有错误，欢迎读者勘误斧正。&lt;/p&gt;
&lt;p&gt;&lt;img src</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="AI" scheme="https://blog.sjtuxhw.top/tags/AI/"/>
    
    <category term="ML" scheme="https://blog.sjtuxhw.top/tags/ML/"/>
    
    <category term="Roadmap" scheme="https://blog.sjtuxhw.top/tags/Roadmap/"/>
    
  </entry>
  
  <entry>
    <title>OS 的特权级切换与内存管理总览：以 Linux AArch64 为例</title>
    <link href="https://blog.sjtuxhw.top/review/os-el-and-mem/"/>
    <id>https://blog.sjtuxhw.top/review/os-el-and-mem/</id>
    <published>2025-05-30T08:49:12.000Z</published>
    <updated>2025-06-11T12:27:30.268Z</updated>
    
    <content type="html"><![CDATA[<p>特权级切换与内存管理这两块知识一直是 OS 的极其极其重要的组成部分：特权级切换是 OS 上下文切换和调度的基石，而内存管理则是一切隔离性（进程抽象）、资源可用性的基石。可惜对于初学者而言太过庞大，并且它们通常相互涉及，以至于总是掌握不了全貌。</p><p>笔者想从尽可能全面的视角记录总结一下它们究竟在做什么，方便日后查阅笔记、快速理解，因此不会过于深入细节（例如不会介绍 Buddy System 和 SLUB 机制的具体内容）。</p><p>最后我们将总结并运用已经了解的知识，讨论一下全局视角下的 OS 内核栈。</p><p>如有错误，欢迎读者勘误斧正。</p><blockquote><p>下面内容将以 AArch64 为例。</p><p>建议复习：通用寄存器 <code>x0-x30</code>、PC 程序计数器、4 个栈寄存器 <code>SP_ELx</code>、3 个异常链接器 <code>ELR_ELx</code>（从 1 开始）、3 个程序状态寄存器 <code>SPSR_ELx</code>、2 个页表基地址寄存器 <code>TTBRx_EL1</code>（注意只有 EL1 级别）、TCR/SCTLR/SCR/TPIDR/MPIDR 这些常见寄存器的用途。</p></blockquote><h2 id="A-特权级切换"><a href="#A-特权级切换" class="headerlink" title="A. 特权级切换"></a>A. 特权级切换</h2><p><img src="imgs/arm_exception_level.png" width="350px" /></p><p><code>EL0</code>：用户态程序、<code>EL1</code>：内核、<code>EL2</code>：hypervisor、<code>EL3</code>：monitor；</p><p>ARM 特权级切换和 x86 一样，都需要是 Context Switch，因此需要保存一些运行时数据（通用寄存器值、系统控制和特殊寄存器值、PCB/TCB、文件描述符、内存管理结构体等等）。</p><p>首先 ARM 本身的特权级比 x86 的复杂（Ring 0/3），因此切换过程对应地复杂一些。寄存器的保存方法如下：</p><p><img src="imgs/priv_reg_switch.png" width="550px" /></p><p>总结一下 ARM CPU 在特权级切换时硬件和软件所执行的任务：</p><p>首先无论特权级切换是硬中断还是软中断引起的，硬件都会开始处理上下文切换的工作：</p><ol><li><p>当前使用的所有 <code>EL0</code> 寄存器全部切换到使用 <code>EL1</code> 寄存器（例如 <code>SP_EL0</code> -&gt; <code>SP_EL1</code>、<code>TTBR_EL0</code> -&gt; <code>TTBR_EL1</code>）；</p></li><li><p>备份特殊寄存器：特殊寄存器 PC 保存到 <code>ELR_EL1</code> 方便返回当前用户态进程（和 x86 相同，如果是软中断则保存中断位置下一条指令的地址，如果是硬中断则保存中断位置当前指令地址）、特殊寄存器 PSTATE 保存到 <code>SPSR_EL1</code> 方便恢复当前用户态进程程序状态；</p></li><li><p>写入系统寄存器 <code>ESR_EL1</code>：将异常事件的原因代号保存在 <code>ESR_EL1</code> 寄存器，例如是 <code>svc</code> 指令导致（对应 x86 <code>syscall</code>，trap）还是缺页导致（fault）还是其他原因；如果是 page fault，那么将触发的内存地址保存在 <code>FAR_EL1</code> 寄存器（回忆 x86 的 <strong><u>CR2</u></strong> 寄存器）</p></li><li><p>恢复特殊寄存器：通过 <code>ESR_EL1</code> 寄存器和 <code>VBAR_EL1</code> 寄存器（Vector Based Address Register，回忆 2.1.7）计算出异常处理函数的起始地址，并写入 PC；</p><blockquote><p><code>SP_EL1</code> 保持原样就行；</p><p><code>PSTATE</code> 按需求设置，例如会被硬件设置为目标异常级别（<code>EL1</code>）的默认状态，即 DAIF 掩码生效，这步还会受到 <code>SPSR_EL1</code> 中保存的部分状态影响；</p></blockquote></li></ol><p>然后控制流交给 OS，此时 OS 会在执行异常处理函数真正内容前进行软件上下文保存（主要意图是防止内核程序破坏了用户态的程序状态）：</p><ol><li><p>将通用寄存器（<code>x0-x30</code>）压内核栈：因为等会内核可能就要用了；</p><blockquote><p>这步是最安全、标准的做法，很多 ISA 也都会这么做，但这显然有性能影响。</p><p>因此 ARM 允许优化，即根据 AAPCS (ARM Procedure Call Standard)，callee-saved registers (<code>x19-x29</code> 和 <code>SP</code>) 如果异常处理函数不会修改它们，并且它调用的函数也遵守 AAPCS，理论上可以不用保存；</p></blockquote></li><li><p>将 <code>SP_EL0</code> 压栈：因为可能后面会调度到不同进程/线程；</p><blockquote><p>某些情况，如特定的快速中断 FIQ（而非 IRQ），或者简单的、非 I/O 的系统调用，如果不会主动调度出去，理论上可以不保存；</p></blockquote></li><li><p>将 <code>SPSR_EL1/ELR_EL1</code> 压栈：必须保存。除了与第二点相同的原因以外，还多了另外一个重要原因：<strong>可能会发生嵌套异常</strong>（例如处理缺页时又被中断），若不保存很可能会导致之前用户态状态丢失；</p><blockquote><p>但 <code>ESR_EL1</code> 无关紧要，用完就可以扔了；</p></blockquote></li></ol><p>而特权级切换回来的时候一般是 OS 主动，所以先软件恢复上下文：弹栈出通用寄存器、<code>SP_EL0</code>、<code>SPSR_EL1/ELR_EL1</code>；</p><p>然后 OS 调用 <code>eret</code> 交给硬件恢复：</p><ol><li>恢复特殊寄存器：<code>ELR_EL1 -&gt; PC</code>、<code>SPSR_EL1 -&gt; PSTATE</code>（<code>SP_EL0</code> 已经软件恢复了）；</li><li>当前使用的所有 <code>EL1</code> 寄存器全部切换到使用 <code>EL0</code> 寄存器；</li></ol><p>我们发现和进入特权级相比少了些步骤，一个是备份特殊寄存器（内核态有的是计算出来的、有的是保留原先的值、还有的是按需设置的，所以内核态的这些特殊寄存器不需要备份），另一个是 <code>ESR_EL1</code> 只是给内核用的，用户态不需要；</p><h3 id="如果涉及调度？"><a href="#如果涉及调度？" class="headerlink" title="如果涉及调度？"></a>如果涉及调度？</h3><p>现在考虑更复杂的情况，上面特权态恢复到用户态是没有发生调度的情况。如果决定调度到其他线程，步骤会有不同吗？</p><p>答案是会的。假设当前是进程 A 下陷到内核态了，我们上面软件保存状态压入的是 A 的内核栈。现在假设我们需要调度并恢复进程 B 的状态（而不是 A 的），那么我们需要多做一步：<u>内核找到<strong>进程 B 的内核栈</strong>，将当前 <code>SP_EL1</code> 更改为 B 对应的内核栈顶部地址，然后执行上面的软件恢复动作</u>。</p><p>这就够了吗？还不够。你虽然压栈了，但切换到另一个进程时，内核怎么找到 A/B 的内核栈呢？因此<strong><u>还需要把进程上下文信息保存到 PCB 中</u></strong>，将内核栈与进程关联起来。这样要调度到哪个进程，就能顺手从 PCB 中拿出内核栈地址，然后恢复状态了。</p><p>现在你可能又又又又有些疑惑了，每个进程都有内核栈？那么内核栈在物理内存的哪里呢？答案需要等到了解内存管理才能揭晓，读者不妨阅读完后文以后再来回顾。</p><p>另外，上述过程仍不完整，因为在切换用户态进程时还需要切换用户态页表（<strong><u>内核态页表不用换</u></strong>）、TLB 刷新等操作，不过它们属于内存管理的范畴，也放在内存管理介绍。</p><blockquote><p>知识补充：应用程序需要保存的运行状态称为处理器上下文处理器上下文（<strong><u>Processor Context</u></strong>）：</p><p>应用程序在完成切换后恢复执行所需的最小处理器状态集合。</p><p>处理器上下文中的寄存器具体包括：</p><ul><li>通用寄存器 <code>x0-x30</code>；</li><li>特殊寄存器，主要包括 PC、SP 和 PSTATE；</li><li>系统寄存器，包括页表基地址寄存器等；</li></ul></blockquote><h2 id="B-物理内存管理"><a href="#B-物理内存管理" class="headerlink" title="B. 物理内存管理"></a>B. 物理内存管理</h2><p>应用程序（用户态）和 OS 内核本身最终都需要使用物理内存。用户态程序依靠 OS 为它包装的<strong><u>虚拟内存机制</u></strong>来完成事实上的隔离和足够的资源以供访问。那 OS 依靠谁来分配物理内存？</p><p>首先我们知道，内核本身也需要分配内存来运行自身的代码，并且内核也不能在 MMU 开启后再绕过 MMU 翻译，因此它也只能使用虚拟内存地址！</p><p>内核在启动时如何拿到虚拟内存？答案是 <strong><u>DIRECT MAPPING</u></strong> 机制。这个机制的内容如下：</p><ul><li><p>在内核启动时，从低地址跳转到高地址执行前（此时还没有抛弃 bootloader 映射的低地址空间），内核会将<strong><u>物理地址的全部空间</u></strong>一口气映射到虚拟地址的高地址空间（<code>TTBR1</code>），使用大页完成映射（如果 page size 是 4K，则采用 1G 大页，主要看具体实现）。</p><blockquote><p>此时只能叫映射，不能叫分配。</p><p>是的，只有物理页在内核中的数据结构 <code>struct physical_page</code> 中的字段 <code>allocated</code> 被设置为 1 后才能视作被分配。</p><p>此时内核中被分配的空间大小只有 bootloader 载入内核镜像的大小（例如 512M）。</p></blockquote></li><li><p>映射完成后，内核部分的所有空间的虚拟地址和所有物理地址，只是相差一个 offset（但是由于现代处理器不支持用段管理物理内存，因此只能用页表）；</p></li></ul><p>DIRECT MAPPING 本身分配规整，适合使用大页的方法，这也是内核适合使用大页的原因之一。</p><blockquote><p>小贴士：历史上曾经存在 “物理地址大小大于虚拟地址”的情况。这个时候内核怎么 DIRECT MAPPING 来管理物理内存呢？</p><p>答案是先 DIRECT MAPPING 映射 892 MB 大小的空间，再在映射的内核空间上方加一个滑动窗口，通过不断更改内核页表的方式来写这些物理内存；</p></blockquote><hr><p>如果内核自己需要为自己的数据结构分配空间（一般是立即使用）怎么办？答案是 Buddy System (<code>alloc_page</code>) + SLUB 机制 (<code>kmalloc</code>)；</p><blockquote><p>这两个机制不是一开始就想出来的，这是工程学的沉淀。</p></blockquote><ul><li>Buddy System 高效地、高利用率地以页为粒度向内核提供各种 page size 倍大小的空间。这个算法和物理页数据结构由内核定义的数据结构 <code>struct physical_page</code> 管理，不在 VMA 中，只记录在 buddy system 空闲链表中。分配的页一般不允许 evict / swap out；</li><li>SLUB 机制则按照内核调用 <code>kmalloc</code> 指定的需求，从资源池 / Buddy System (如果资源池不够) 那里获得的合适大小的资源并返回，极大降低内部碎片和资源浪费。如果有一类数据结构或分配模式大量地出现（例如 <code>dentry</code>），则专门为这个大小准备一个资源池，以增强性能；</li><li>如果内核一次性需要 4K (一个页大小) 或更多的数据，内核应该自己懂事地调用 <code>alloc_page</code> 来申请，而不是 <code>kmalloc</code>；</li><li>无论是 Buddy System 还是 SLUB，分配的内存地址都是 DIRECT MAPPING 的（即它们分配的物理地址直接对应到相差 offset 的内核空间，和最开始的映射是一样的）；</li></ul><hr><p>如果内核自己需要分配一块超大的空间，并且希望享受用户态 on-demand paging 的性能，怎么办？答案是特殊的 <code>vmalloc</code> 方法。</p><p>它和用户态的分配思路一样，on-demand paging，并且这样分配的空间允许 swap out（这个 OS 可以自定义）；需要时直接从 buddy system 按页取资源；</p><blockquote><p>注意：使用 <code>kmalloc</code> 创建的空间不会被 swap，确保内核关键数据结构不会在运行时不可用。</p></blockquote><p>vmalloc 分配的虚拟内存区域一般情况会在 DIRECT MAPPING 区域更高的地址（如下图 <code>KBASE + 0xFFFF_FFFF</code> ~ <code>0xFFFF_FFFF_FFFF_FFFF</code> 区域），由内核页表管理到物理内存的映射（可以 swap out）。</p><hr><p>如果用户态应用程序通过 <code>malloc</code> 向内核申请应用程序空间，怎么办？就是 ICS 的内容了！</p><ul><li>若这块空间不大，则通过 <code>brk</code> 分配堆。那应用程序的堆空间从哪来？答案是应用程序启动时 OS 会在载入可执行程序的同时预映射一些页给到应用。<ul><li>如果预映射的页用完了，内核需要通过页粒度的物理内存管理找到一个空闲的物理页（就是上面说的，直接通过 buddy system 拨一个物理页）并映射好给应用程序；</li></ul></li><li>若这块空间很大，通过 <code>mmap</code> 直接映射。内部实现是，只是先在 VMA 记录一下，然后 on-demand paging（拨物理页的方法同上）；</li></ul><p>这样我们会发现，其实<strong><u>任何应用程序已经被分配物理地址的 VM，都有在内核空间对应的区域（映射到同一段物理内存）</u></strong>。</p><p>总结一下，任意一个进程虚拟内存区域应该是这样的：</p><p><img src="imgs/chcore_start_mm.png" width="550px" /></p><p>从低地址到高地址分别为：用户态程序空间、设备空间（不在 user space 中）、unmapped region、direct mapping area（全量物理内存映射）、<code>vmalloc</code> 或 <code>ioremap</code> 动态占用空间、kernel 数据结构（如 kernel stack、kernel task struct 等等）、kernel code/data 区；</p><p>然后由于 kernel 始终有 direct mapping 的区域，就意味着它能看到物理内存上的所有数据结构。</p><h2 id="C-虚拟内存管理"><a href="#C-虚拟内存管理" class="headerlink" title="C. 虚拟内存管理"></a>C. 虚拟内存管理</h2><p>OS 给用户态程序提供的虚拟内存又是如何实现的？</p><p>在 OS 演变历史上，人们主要尝试过 3 种管理用户态程序内存方案：</p><ol><li><p>直接使用物理地址。问题是：干扰性、扩展性、安全性。因此我们需要增加一层隔离，让应用无法直接看见物理地址；</p></li><li><p>虚拟内存 + 翻译机制(地址分段)。有应用整段分配的，也有更细粒度的：</p><p><img src="imgs/mm-seg.png" /></p><p>随着物理内存与虚拟内存的差距增大、应用对于资源需求日益增长，人们很快发现，这种方法对物理内存连续性的要求：物理内存也必须以段为单位进行分配，<strong><u>导致内存利用率不大</u></strong>（外部碎片，段与段间留下的碎片空间；内部碎片，段内部预留的未使用的碎片空间）；</p><p>因此分段机制通常存在于 x86 设备，现代操作系统不依赖于分段。</p></li><li><p>虚拟地址 + 翻译机制(分页)。依赖于页表，并且通过多级页表有效压缩页表大小。略。</p></li></ol><p>前面说虚拟内存的翻译机制用的是分页，但 OS 对一个用户态程序的虚拟内存内部的管理则采用段来管理。主要考虑：</p><ul><li>段的数量远少于页，如果使用页管理则应用很可能用不完；</li><li>通常每一段都有相同的属性，比如只读、可执行等等，可简化管理；</li></ul><p>向 VMA 中添加记录的主要途径：</p><ul><li>OS 在创建应用程序时分配（数据段 <code>.rodata</code>、代码段 <code>.text</code> 等等，也包括栈区域和部分的堆）；</li><li>应用程序主动向 OS 发出请求，例如 <code>brk() / mmap()</code>（不论是 file-backed 还是匿名页）；</li><li>用户态 <code>malloc</code>；分配小空间或一般空间，则底层一般使用 <code>brk()</code>，分配大空间，则底层一般使用 <code>mmap()</code>；</li></ul><p>考虑新的问题：VMA 是否冗余？页表本身是不是已经能够说明了 OS 给当前进程分配的虚拟内存的情况了吗？理论上是这样，页表存放了 VMA 所有保存的数据。但是有以下考量：</p><ul><li>提升性能。它本身的数据结构就足够高效（红黑树+链表，最新版的 Linux 已经开始用 maple tree 数据结构了）。如何检查应用访问的内存有效？如何实现 kernel page fault handler？总不能把应用的页表扫一遍吧？</li><li>极大简化 <code>mmap() / munmap() / mprotect()</code> 等函数的实现。尤其分配大空间时 <strong><u>on-demand paging</u></strong> 就必须需要 VMA（归根到底还是性能）；</li><li>保存额外的信息（例如是否 file-backed、shared/private mapping），便于 OS 实现其他功能；</li></ul><p>再次总结虚拟内存的优势：</p><ul><li>高效使用物理内存：使用 DRAM 作为虚拟地址空间的缓存，将离散的、有限的空间给应用抽象成连续的、充足的空间；</li><li>简化内存管理：每个进程看到的是统一的线性地址空间；多个进程间方便安全地分享；</li><li>更强的隔离与更细粒度的权限控制：<ul><li>（隔离性）一个进程不能访问属于其他进程的内存；</li><li>（安全性）用户程序不能够访问特权更高的内核信息；</li><li>不同内存页的读、写、执行权限可以不同；</li></ul></li></ul><p>什么时候不需要：</p><ul><li>内存地址足够大（用页表开销会逐渐增大，使用段机制访问的优势就会凸显）、性能要求足够高；</li></ul><h2 id="D-全局视角：内核栈是什么？在哪里？"><a href="#D-全局视角：内核栈是什么？在哪里？" class="headerlink" title="D. 全局视角：内核栈是什么？在哪里？"></a>D. 全局视角：内核栈是什么？在哪里？</h2><p>首先搞清楚内核栈官方定义是啥？根据 <a href="https://www.kernel.org/doc/html/next/x86/kernel-stacks.html">官方文档</a>：</p><p><code>x86_64</code> page size (PAGE_SIZE) is 4K.</p><p>Like all other architectures, <code>x86_64</code> has a kernel stack for every active thread. These thread stacks are <code>THREAD_SIZE</code> (<code>2*PAGE_SIZE</code>) big. These stacks contain useful data as long as a thread is alive or a zombie. While the thread is in user space the kernel stack is empty except for the <code>thread_info</code> structure at the bottom.</p><p>In addition to the per thread stacks, there are specialized stacks associated with each CPU. These stacks are only used while the kernel is in control on that CPU; when a CPU returns to user space the specialized stacks contain no useful data.</p><p>注意到，官方文档说，OS 内核对每个活动线程都有一块内存空间，称为 “thread stack”（线程栈），这就是内核栈的一部分，它的作用就是在对应线程进入内核态时内核需要用的栈。一个 <code>x86_64</code> 的 thread stack 大约 8KB（2 个页大小），当线程在用户态时，内核栈是空的（除了 <code>thread_info</code> 结构体），直到进入内核态才有用。</p><p>并且除了 thread stacks，实际上还有与每个 CPU 相关的专门堆栈（<strong><u>Interrupt stack</u></strong>，中断栈），这也能看作内核栈的一部分。这些堆栈只在内核控制 CPU 时使用（用于处理硬中断和 softirq）；作用是<u>为内核中断处理提供了更多空间，而无需增加每个线程栈的大小</u>。当 CPU 返回用户空间时，这个专用堆栈也就不包含有用数据了。</p><p>嗯？看起来有两种栈都能称为 “内核栈”？</p><p>没错，“内核栈” 本身的定义就极为宽泛：<strong><u>操作系统内核为执行流（无论是内核线程还是代表用户进程执行内核代码）在内核态运行时分配的专用内存区域</u></strong>。</p><p>而内核栈就是内核线程使用的栈。</p><blockquote><p>这里的 “内核线程” 也是很宽泛的定义，实际上有好几类，常见的例子有：</p><ul><li><strong>中断下半部处理：</strong> <code>ksoftirqd</code> 线程处理延后的软中断任务；</li><li><strong>内存管理：</strong> 如 <code>kswapd</code> 线程负责页面回收；</li><li><strong>文件系统：</strong> 如日志提交 (<code>jbd2</code>)、数据同步 (<code>flush</code> 线程)；</li><li><strong>工作队列机制：</strong> <code>kworker</code> 线程执行由内核其他部分提交的延迟工作项；</li><li><strong>实现用户线程模型：</strong> 在 1:1 线程模型（如 Linux 的 NPTL）中，每个用户线程 LWP 都直接绑定一个内核线程（LWP 即轻量级进程，也就是建立在内核之上并由内核支持的用户线程）；</li></ul><p>我们上面的 “thread stack” 其实就是 “实现用户线程模型” 的这类内核线程的栈，它一般跟随着用户线程/进程的创建来分配，因此与用户态线程强绑定；</p><p>而其他类型的内核线程一般和用户线程一样，都是参与调度的，和用户线程的关系不紧密（是内核通过 <code>kthread_create / kthread_run</code> 启动的线程），只不过它们一般受内核专用调度器来调度。</p></blockquote><p>最后补充一下全局视角：以 Linux 为例，内核栈在哪里？更准确地说，“<strong>实现用户线程模型</strong>” 用到的内核栈（thread stack）在哪里？</p><p>这个问题比较复杂，我们先从几个简单的事实出发：</p><ol><li><p>在任何时刻，一个 CPU 核心<strong>正在使用</strong>的内核栈只有一个。这是当 CPU 在 kernel mode 执行代码时（例如处理系统调用、中断、异常、调度代码）所使用的栈。这个栈的指针（例如 x86 的 <code>RSP</code> 寄存器或 AArch64 <code>SP_EL1</code>）指向当前正在使用的内核栈顶部；</p></li><li><p>但是，一个 CPU 核心会处理多个进程/线程。<strong>每个用户空间的进程/线程在陷入内核态时，都需要使用自己独立的内核栈（有且仅有对应的一个）。</strong></p><p>现在回顾之前的知识 “特权级切换时 CPU 和 OS 应该做什么”，我们知道如果涉及进程切换，那么在内核态需要先切换内核栈，再回用户态。</p><p>另外，只是压栈还不行（否则想要切换到目标进程却发现找不到对应内核栈的位置。。），<strong><u>还需要把进程上下文信息保存到 PCB 中</u></strong>。</p><p>然后内存管理层面还需要切换用户态页表（更新 <code>TTBR0_EL1</code>）、刷新 TLB 等等。</p><blockquote><p>注 1：从哪里更新 <code>TTBR0_EL1</code>？每个进程的页表基地址则放在进程 <code>struct task_struct</code> -&gt; <code>struct mm_struct</code> -&gt; <code>pgd_t *pgd</code> 字段中。</p><p>注 2：<code>struct mm_struct</code> 除了页表基地址，还会存放 VMA list、堆栈信息、其他内存映射信息、页表锁等；</p></blockquote></li></ol><p>好了，知道了我们在这些地方要用到内核栈，我们只要捋清除内核栈从创建到销毁的一系列流程，就知道它究竟在哪了。</p><p>我们知道，DIRECT MAPPING 是内核虚拟地址空间中一个<strong>巨大、连续</strong>的区域，提供<strong>虚拟地址到物理地址的简单线性关系的转换</strong>，它的目的就是<u>提供一种高效访问<strong>几乎任何物理内存页</strong>的方式，避免为每次访问建立复杂页表映射的开销</u>。内核需要操作内存数据结构（如上面提到的 <code>struct page</code>、SLAB 分配器对象）、Buddy System 机制、或 DMA 缓冲区时，常使用此区域；</p><p>但内核栈是动态的、是为每个进程/线程<strong>动态分配</strong>的（通常在进程创建时 <code>fork()</code>/<code>clone()</code>），它与特定的执行上下文（进程/线程）紧密绑定。</p><p>就这点上来说，内核栈就像内核使用的比较大的数据结构，分配的方法就上面两种：一个是直接用 <code>alloc_page</code> 这样的方法从 buddy system 那里拿物理页（DIRECT MAPPING），另一个是使用 <code>vmalloc</code> 分配一个大的空间，然后 on-demand paging。</p><blockquote><p>不考虑 <code>kmalloc</code> 的原因参见前文，栈的大小也不小了。</p></blockquote><p>事实上，一开始 Linux 设计者选择了使用 buddy system 而非 <code>vmalloc</code> 来分配内核栈，是因为<strong><u>性能考量</u></strong>：<code>vmalloc</code> 通常 on-demand paging 分配的连续虚拟内存，但物理内存不一定连续，这对一般情形是有用的，但内核栈使用频繁，如果每次都走虚拟地址页表的翻译，性能会很差；因此内核栈需要尽量保证物理空间上连续，能受益到 locality 的性能。</p><p>但在 Linux 4.14 以后，引入了 <a href="https://www.kernel.org/doc/html/latest/mm/vmalloced-kernel-stacks.html"><code>VMAP_STACK</code></a>（参见链接）机制，允许采用 <code>vmalloc</code> 申请的内存作为内核栈，只需要使能 <code>CONFIG_VMAP_STACK</code> 内核选项即可。这样可以利用 <code>vmalloc</code> 自身的内存边界检查的特性，提供类似 guard page 这样的栈溢出检测的能力。</p><p>最后在进程退出时，内核会释放其内核栈占用的物理页帧，并解除对应的虚拟地址映射（清理内核页表）。</p><p>所以问题解决了，内核栈要么在 DIRECT MAPPING 区域（如果直接从 buddy system 拿物理页），要么在 <code>vmalloc</code> 分配的某段连续虚拟内存中（如果启用 <code>VMAP_STACK</code>），当然如果是由 <code>vmalloc</code> 分配，那么分配时 OS 会额外标记内核栈不允许 swap-out（通过标记物理页数据结构 <code>struct page</code> 的特殊字段 <code>GFP</code> 来禁止换出）。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;特权级切换与内存管理这两块知识一直是 OS 的极其极其重要的组成部分：特权级切换是 OS 上下文切换和调度的基石，而内存管理则是一切隔离性（进程抽象）、资源可用性的基石。可惜对于初学者而言太过庞大，并且它们通常相互涉及，以至于总是掌握不了全貌。&lt;/p&gt;
&lt;p&gt;笔者想从尽可能</summary>
      
    
    
    
    <category term="review" scheme="https://blog.sjtuxhw.top/categories/review/"/>
    
    
    <category term="OS" scheme="https://blog.sjtuxhw.top/tags/OS/"/>
    
  </entry>
  
  <entry>
    <title>机密计算与TEE：知识整理和试验笔记</title>
    <link href="https://blog.sjtuxhw.top/technical/tee-lab/"/>
    <id>https://blog.sjtuxhw.top/technical/tee-lab/</id>
    <published>2025-04-17T15:31:36.000Z</published>
    <updated>2025-04-17T15:53:43.512Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Part-1-机密计算与-TEE-技术入门知识整理"><a href="#Part-1-机密计算与-TEE-技术入门知识整理" class="headerlink" title="Part 1. 机密计算与 TEE 技术入门知识整理"></a>Part 1. 机密计算与 TEE 技术入门知识整理</h2><p>本节参考文献参见 1.5 节。</p><h2 id="1-0-Background"><a href="#1-0-Background" class="headerlink" title="1.0 Background"></a>1.0 Background</h2><p>为了防止未经授权的访问，数据安全性是基于三种方面构建的，即静态存储数据、传输中数据、和使用中数据。</p><p>业界目睹了几项引人注目的内存抓取（例如 Target 信用卡和个人信息泄露事件）和 CPU 侧信道攻击，还有许多虚拟机管理程序（Hypervisor）漏洞也被报道出来，这些攻击和漏洞的报道显著提高了业界对  “使用中数据”的关注，另外，涉及恶意软件注入的著名攻击事件，例如 Triton 攻击和乌克兰电网袭击，更是使得保护“正在使用的数据”成为数据安全领域迫在眉睫的努力方向。</p><p>机密计算就是针对数据在使用过程中的安全问题所提出的一种解决方案。它是一种基于硬件的技术，将数据、特定功能、应用程序，同操作系统、系统管理程序或虚拟机管理器以及其他特定进程隔离开来，让数据存储在受信任的执行环境（TEE）中，即使是使用调试器，也无法从外部查看数据或者执行操作。TEE 确保只有经过授权的代码才能访问数据，如果代码被篡改，TEE 将阻止其继续进行操作。</p><h4 id="为什么机密计算需要以硬件为基础？"><a href="#为什么机密计算需要以硬件为基础？" class="headerlink" title="为什么机密计算需要以硬件为基础？"></a>为什么机密计算需要以硬件为基础？</h4><p>由于计算架构中任何层级中的安全性都可能因为基础层的漏洞而功亏一篑，任何计算层级的安全性取决在其之下层级的安全性。这个共识推动了对最低层安全解决方案的需求，直至硬件的硅组件。  </p><p>通过为最低层硬件提供安全性，最大程度降低了对整个计算架构参与方依赖性，可以从所需受信任方列表中删除操作系统和设备驱动程序供应商，平台和外围设备供应商以及服务提供商及其系统管理员，从而减少在系统生命周期中任何时刻遭受潜在危害的风险。</p><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><img src="imgs/words.png" /></p><p>补充：GP，2010 年 7 月 GP（Global Platform，全球平台组织）提出了 TEE 可信执行环境的设计，他们设计的 API 被称为 GlobalPlatform API。该组织致力于跨行业协作，识别、开发和发布规范，以促进在安全芯片技术上安全且可互操作地部署和管理多个嵌入式应用程序。</p><p>先以 ARM 的 TrustZone 为例。随着智能手机的普及，手机上数据的价值越来越高，如电子支付密码（包括传统密码、指纹、人脸），带版权信息的数据等。为了进一步保护这些数据的安全，ARM 提出了 TrustZone 技术，其原理是将 CPU 的工作状态和其它相关硬件资源（中断、内存、外设和 cache 等）划分为安全（secure）和非安全（normal）两种类型，来达到数据隔离与保护。</p><blockquote><p>当 CPU 运行在 normal 状态时，将只能访问 non secure 空间的资源，而不能访问 secure 资源。当 CPU 运行在 secure 状态时，既能访问 non  secure 资源，也能访问 secure 资源；</p></blockquote><p>然后在 ARM 上，基于 Trustzone 技术实现了 TEE ，与之对应的是 REE，一般称 TEE 为 Secure World，REE 为 Normal World。OP-TEE（开源的 TEE OS）运行中 Secure World 里边，普通 OS（如 Linux，AOSP 等）运行在 Normal World 里边；</p><p><img src="imgs/tee-arch.png" width="450px" /></p><p>TEE 的特点：</p><ul><li>受硬件保护机制：TEE 隔离于 REE，只通过特定的入口于 TEE 通信，并不规定某一种硬件实现方法；</li><li>高性能：TEE 运行时使用 CPU 的全部性能；</li><li>快速通信机制：TEE 可以访问 REE 的内存，REE 无法访问受硬件保护的 TEE 内存；</li></ul><p>非安全操作系统在 TEE 规范中被称为富执行环境 (REE)。它通常是 Linux 操作系统的一个变种，例如 GNU/Linux 发行版或 AOSP。 </p><p>TEE 的设计在 ARM 架构上主要依靠 TrustZone 技术作为底层硬件隔离机制。然而，它的结构也使其能够与任何符合 TEE 概念和目标的隔离技术兼容，例如以虚拟机形式运行或在专用 CPU 上运行。</p><p>TEE 的主要设计目标是：</p><ul><li>隔离：TEE 提供与非安全操作系统的隔离，并使用底层硬件支持保护已加载的可信应用程序 (TA) 彼此隔离；</li><li>占用空间小：TEE 应保持足够小，以便驻留在合理数量的片上内存中，就像基于 ARM 的系统一样；</li><li>可移植性：TEE 旨在轻松插入到不同的架构和可用的硬件，并且必须支持各种设置，例如多个客户端操作系统或多个 TEE。</li></ul><p>研究者提供了三个类别用于比较不同的 TEE 实现，即<strong>功能性，安全性和可部署性</strong>：</p><ul><li><strong>功能标准</strong>：包括受保护的执行，密封存储，受保护的输入，受保护的输出和验证等环节，总体而言，这些标准衡量了整个使用周期中数据的物理保护，即接收，输入，处理，输出和验证；</li><li><strong>安全标准</strong>：包括数据隔离，信息流控制，清理，损害限制。此类别更侧重于避免对系统进行特定攻击的机制；</li><li><strong>可部署性</strong>：标准衡量了采用的障碍，包括对现有系统的支持，成本，开销和 SEE（Secure Execution Environment）性能；</li></ul><blockquote><p>由于功能和安全性标准之间存在一些重叠，只需要关注重点部分。</p></blockquote><h2 id="1-2-启动流程？"><a href="#1-2-启动流程？" class="headerlink" title="1.2 启动流程？"></a>1.2 启动流程？</h2><p>根据 GP 标准，TEE 的启动流程只在系统启动时执行一次，要求：</p><ul><li>启动流程至少建立一个信任根 (RoT)，需要一些机制和方法去实现；</li><li>一般情况下启动基于 ROM 代码：允许其他实现、依次验证加载的代码；</li><li>一般情况下 TEE 首先启动：阻止 REE 接口生效。 Trusted OS 首先启动，再启动 REE；</li></ul><h2 id="1-3-主流的基于硬件-TEE-技术"><a href="#1-3-主流的基于硬件-TEE-技术" class="headerlink" title="1.3 主流的基于硬件 TEE 技术"></a>1.3 主流的基于硬件 TEE 技术</h2><h3 id="1-3-1-ARM-TrustZone"><a href="#1-3-1-ARM-TrustZone" class="headerlink" title="1.3.1 ARM TrustZone"></a>1.3.1 ARM TrustZone</h3><p>架构：</p><ul><li>每个物理的处理器内核提供了两个虚拟内核：一个被认为是非安全的，被称为“非安全区”（Normal World），另一个被认为是安全的，被称为“安全区”（Secure World）；</li><li>以及一个在两者之间的上下文切换机制，称为监视模式（<code>EL3</code>）；</li><li>硬件支持：ARM-v8 本身支持名为 secure mode 的模式，用来区分 normal mode。通过设置 Secure Configuration Register 系统寄存器来使能该模式的支持，该寄存器的最后 1 bit 为 0 的话，则表示当前 CPU 处于为 secure mode（注意和特权级不一样）；</li></ul><p><img src="imgs/arm-secure-mode.jpg" width="450px" /></p><p>调用方式（最简单的情况下）：</p><p><img src="imgs/tee-call.png" width="450px" /></p><p>当非安全区的用户模式需要获取安全区的服务时，</p><ol><li>首先需要进入到非安全区的特权模式（Normal World 的 Rich OS 内核态）；</li><li>在该模式下调用 SMC（System Monitor  Call），处理器将进入到监视模式；</li><li>监视模式备份非安全区的上下文，然后进入到安全区的特权模式，此时的运行环境是安全区的执行环境；</li><li>此后进入到安全区的用户模式，执行相应的安全服务；</li><li>原路返回；</li></ol><p>上述 Normal World 和 Secure World 的区分可以是物理的，也可以是虚拟的。</p><p>物理和安全处理器以<strong><u>时间分割</u></strong>的方式共享物理处理器核心（分时复用？中断、调度是如何保证的？），这给了两个区域一个幻想，即它完全拥有处理器；</p><blockquote><p>这也给 Secure World 一个机会，使构建隔离的可编程环境成为可能，该环境可以运行各种安全应用程序。</p></blockquote><h3 id="1-3-2-Intel-SGX-Software-Guard-eXtensions"><a href="#1-3-2-Intel-SGX-Software-Guard-eXtensions" class="headerlink" title="1.3.2 Intel SGX (Software Guard eXtensions)"></a>1.3.2 Intel SGX (Software Guard eXtensions)</h3><ul><li><p>是对 Intel 架构的扩展。在原有架构上增加了一组新的指令集和内存访问机制；这些扩展允许应用程序实现一个被称为安全区（enclave）的容器，在应用程序的地址空间（用户态）中划分出一块被保护的区域，为容器内的代码和数据提供机密性和完整性的保护；</p></li><li><p>这块有限大小的加密内存区域称为 Enclave Page Cache（EPC），支持 32MB，64MB 或 128MB （SGX2 可支持 256MB）；</p></li><li><p>SGX 通过硬件访问控制机制来保护 Enclave 的内存。容器中的信息不会被特殊权限的恶意软件的破坏，即使底层的高特权系统软件（例如 OS 或虚拟机管理程序）是恶意的或已被破坏，SGX 仍可抵抗物理内存访问类的攻击；</p><ul><li>从外到内访问（非法）：Page Fault；</li><li>从内到外访问（合法）：由操作系统内存管理策略控制，而且 Enclave 只能在 Ring 3（用户态）请求系统调用；</li></ul></li><li><p>SGX 的实现需要处理器、内存管理部件、BIOS、驱动程序、运行时环境等软硬件协同完成。除了提供内存隔离与保护安全属性，SGX 架构还支持远程认证和密封的功能，可用于安全软件应用和交互协议的设计；</p></li><li><p>SGX 安全模型：</p><p><img src="imgs/intel-sgx.png" width="250px" /></p><ul><li><strong>可信计算基（Trusted Computing Base，TCB）</strong>被视为 CPU 组件，而系统的其他部分则被视为不可信任；</li><li>每个 SGX 应用程序至少包含两个不同的部分。 位于 Secure World 内部并在 Enclave Page Cache（EPC）中执行的受信任代码，以及位于不受信任的系统内存中并执行的不受信任的代码；</li><li>不可信任的部分创建 Enclave，并且定义 entry point，然后执行放置在 Enclave Page Cache 的、加密且受信任的内存中的 Secure World 程序；</li><li>Secure World 初始化后，不受信任的代码通过调用 <code>EENTER</code> 指令来调用 Enclave 代码，该指令将处理器模式从保护模式切换到安全区模式；然后处理器在 Enclave 内执行被调用的代码。调用 <code>EEXIT</code> 指令会导致 Enclave 内执行线程退出 Enclave，并且执行流程返回到不受信任的代码；</li></ul></li><li><p>除了用户创建的 Enclave 外，SGX 还使用了一些基础架构 Enclave（例如 Reference Enclave 和 Configuration Enclave）为本地或远程验证机制提供支持；</p></li><li><p>SGX 提供了一个可以保护静态 Enclave 数据的 Enclave 密封机制，可以在系统内存和 EPC 之间安全封送数据，并且使用硬件内存加密引擎（MEE）来对数据进行加密和解密。</p></li><li><p>SGX 支持 Enclave 内部的多线程处理：每个 APP 都可以有自己独立的 TEE，甚至可以创建多个 TEE Enclave；</p></li></ul><h3 id="1-3-3-AMD-SEV-Secure-Encrypted-Virtualization"><a href="#1-3-3-AMD-SEV-Secure-Encrypted-Virtualization" class="headerlink" title="1.3.3 AMD SEV (Secure Encrypted Virtualization)"></a>1.3.3 AMD SEV (Secure Encrypted Virtualization)</h3><ul><li><p>内存加密方案。对整个操作系统进行内存加密，操作系统本身在 TCB（Trust Computing Base）中。可以防止通过总线/ DRAM 遭受物理攻击；</p></li><li><p>通过提供加密的 VM 隔离来解决高特权系统软件类别的攻击，每个虚拟机使用一个密钥隔离客户机系统和虚拟机管理程序；</p></li><li><p>密钥由 AMD 安全处理器生成和管理，内存控制器中嵌入了AES-128 加密引擎。 提供适当的密钥后，将自动对主存储器（memory）中的数据进行加密和解密；</p></li><li><p>系统管理程序（Hypervisor）更改通过使用硬件虚拟化指令以及与 AMD 安全处理器的通信来管理内存控制器中相应的密钥。也就是说<strong><u>系统组件（比如系统管理程序）试图读取客户机的内存时，只能看到被加密后的字节</u></strong>；</p></li><li><p>AMD SEV VM 使用客户机中页表中的加密比特 C 来控制一个内存页是私密的还是共享的，比特 C 的位置有具体实现定义，可以是物理地址的最高位。</p><ul><li><p>VM 标记共享（非加密）内存页时，C bit = 0，表明不必使用该 VM 的内存加密密钥对其加密；</p></li><li><p>私密（加密）内存页只能用于 VM，标记 C bit = 1。一个典型的 VM 中，大多数内存页被标记为私密的，只有那些与外部通信的内存页才会标记为共享的；</p><p><img src="imgs/amd-sev.jpg" width="550px" /></p></li></ul></li><li><p>2017 年 AMD 又引入了 SEV-ES（Encrypted State）增加了对 CPU 寄存器状态的保护，当 VM 停止运行时，将加密所有 CPU 寄存器的内容；</p><blockquote><p>这样可以防止将 CPU 寄存器中的信息泄露给虚拟机管理程序之类的组件，甚至可以检测到对 CPU 寄存器状态的恶意修改。</p></blockquote></li><li><p>AMD 又推出了 SEV-SNP。其建立在原始的 AMD SEV 和 SEV-ES 的基础上，可提供额外的基于硬件的内存完整性保护，以抵御基于管理程序的攻击，比如：数据回放、内存重新映射等。</p><ul><li>基本原理：如果 VM 能够读取内存的私有（加密）页面，则在它下次操作前必须始终只能读到最后一次写入的值。这意味着，如果 VM 将值 A 写入内存位置 X ，每当以后读取 X 时，它要么必须看到值 A，要么必须得到一个异常，指示无法读取内存。</li></ul></li><li><p>该方案多用于云中的应用，可保护数据免受 CSP（Cloud Solution Provider）的侵害；</p></li></ul><h3 id="1-3-4-Apple-SEP-Secure-Enclave-Processor"><a href="#1-3-4-Apple-SEP-Secure-Enclave-Processor" class="headerlink" title="1.3.4 Apple SEP (Secure Enclave Processor)"></a>1.3.4 Apple SEP (Secure Enclave Processor)</h3><ul><li><p>使用一个独立于主处理器外的安全协处理器，其中包括基于硬件的密钥管理器，可提供额外的安全性保护。</p><ul><li>安全隔离区处理器是在片上系统（SoC）内制造的协处理器；它使用加密的内存，并包括一个硬件随机数生成器；</li><li>安全协处理器提供了用于数据保护密钥管理的所有加密操作，即使内核受到威胁，也可以维护数据保护的完整性；</li></ul><p><img src="imgs/apple-sep.jpg" width="450px" /></p></li><li><p>安全隔区是集成到 Apple 片上系统 (SoC) 的专用安全子系统，由安全隔区处理器提供主要计算能力；</p></li><li><p>安全隔区与应用程序处理器之间的通信受到严格的控制：将其隔离到一个中断驱动的邮箱以及共享的内存数据缓冲区；</p></li><li><p>安全隔区包括一个专用的安全隔区 Boot ROM。与应用程序处理器 Boot ROM 类似，安全隔区 Boot ROM 也是不可更改的代码，用于为安全隔区建立硬件信任根；</p></li></ul><h3 id="1-3-5-小结"><a href="#1-3-5-小结" class="headerlink" title="1.3.5 小结"></a>1.3.5 小结</h3><ul><li><p>ARM TrustZone，利用硬件监督模式（<code>EL3</code>），使用指令或中断在同一处理器上的执行环境之间切换，像更高一级 “OS” 在 Secure World（Rich OS）和 Normal World（TEE OS）间调度和切换；</p></li><li><p>Intel SGX，用户态安全 Enclave，对特定区域的内存进行加密保护，在一个用户态进程内形成安全容器，对其中从 RAM 进出的内存数据进行加密，TCB 的规模最小（只有硬件和这个 Enclave），可以使用原本的系统调用（因为用户态），初始代码是从普通空间复制而来，不是加密信息；</p><blockquote><p>这个方案只有远程校验才能从外部加载/存储密钥；</p></blockquote></li><li><p>AMD SEV，对整个 OS 内存加密，OS 本身就在 TCB 中，可以防止通过总线/ DRAM 遭受物理攻击，当客户机从租户获得密钥时，加密可以扩展到虚拟化环境；</p></li><li><p>Apple SEP，使用协处理器方案，即芯片组或 SoC 中内置协处理器，内部带有单独的操作系统和应用程序，应用通过安全信道与外部通信，通常包括加密引擎；</p></li></ul><h2 id="1-4-GP-API"><a href="#1-4-GP-API" class="headerlink" title="1.4 GP API"></a>1.4 GP API</h2><p>GlobalPlatform 组织提供的 TEE API 规定了大多数适用场景所需的方法。一般情况下，REE 侧构成：</p><ul><li>CA（Client APP）对应一些上层应用，比如指纹采集、支付应用等，通过调用 TEE Client API 实现与 TEE 环境的交互；</li><li>REE Communication Agent 为 TA 和 CA 之间的消息传递提供了 REE 支持；</li><li>TEE Client API 是 REE 中的 TEE 驱动程序提供给外部的接口，可以使运行在 REE 中的 CA 能够与运行在 TEE 中的 TA 交换数据。</li></ul><p>TEE 侧构成：</p><ul><li>TA（Trusted  Application）是 TEE 中完成特定功能的应用；</li><li>TEE Communication Agent 是可信操作系统的特殊组成部分，它与 REE Communication Agent 一起工作，使 TA 与 CA 之间安全地传输消息；</li><li>TEE Internal Core API 是 TEE 操作系统提供给 TA 调用的内部接口，包括密码学算法，内存管理等功能；</li><li>Trusted Device Drivers 可信设备驱动程序，为专用于 TEE 的可信外设提供通信接口；</li><li>Shared Memory 是一块只有 CA 和 TA 可以访问的一块安全内存，CA 和 TA 通过共享内存来快速有效传输指令和数据；</li></ul><p>CA 常用接口：</p><ul><li><code>TEEC_InitializeContext</code>：对变量 Context 进行初始化配置，用来建立 CA 和 TEE 的联系，向 TEE 申请共享内存地址用于存放数据；</li><li><code>TEEC_OpenSession</code>：建立一个 CA 和 TA 间的 session，用于 CA 和 UUID 指定的 TA 进行通信，是 CA 连接 TA 的起始点；</li><li><code>TEEC_InvokeCommand</code>：依靠打开的 session，将传送命令请求给 TA，并将必要的指令执行参数一并发送给 TA；</li><li><code>TEEC_CloseSession</code>：关闭 session，关闭 CA 和 TA 之间的通道；</li><li><code>TEEC_FinalizeContext</code>：释放 Context，结束 CA 与 TEE 的连接；</li></ul><p>TA 接口：</p><ul><li><code>TA_CreateEntryPoint/TA_DestroyEntryPoint</code>：为 CA 建立/移除接入点，注册/取消 TA 的服务；</li><li><code>TA_OpenSessionEntryPoint/TA_CloseSessionEntryPoint</code>：建立/关闭 CA 与 TA 之间的通讯通道；</li><li><code>TA_InvokeCommandEntryPoint</code>：接收 CA 传送的指令和参数，并在这 TEE 侧执行；</li></ul><p><img src="imgs/tee-routine.png" width="450px" /></p><h2 id="1-5-本部分参考文献"><a href="#1-5-本部分参考文献" class="headerlink" title="1.5 本部分参考文献"></a>1.5 本部分参考文献</h2><ul><li><a href="https://optee.readthedocs.io/en/latest/">Optee Doc</a>；</li><li><a href="https://blog.csdn.net/feelabclihu/article/details/128157100">TEE SMC 理解 - CSDN</a>；</li><li><a href="https://zhuanlan.zhihu.com/p/401632688">浅谈基于硬件TEE的技术方案 - 知乎</a>；</li><li><a href="https://www.jianshu.com/p/929d806f7caf">TEE和MesaTEE - 简书</a>；</li><li><a href="https://kickstartembedded.com/2022/11/07/op-tee-part-3-setting-up-op-tee-on-qemu-raspberry-pi-3/">OP-TEE: Part 3 – Setting up OP-TEE on QEMU &amp; Raspberry Pi 3</a>；</li><li><a href="https://cs.brown.edu/courses/csci2390/2019/notes/s18-ryoan.pdf">TEE Thread Model - CS BROWN EDU</a>；</li></ul><h2 id="Part-2-TEE-的-OS-移植笔记-ARM"><a href="#Part-2-TEE-的-OS-移植笔记-ARM" class="headerlink" title="Part 2. TEE 的 OS 移植笔记 (ARM)"></a>Part 2. TEE 的 OS 移植笔记 (ARM)</h2><h2 id="2-1-Revisit-ARM-TrustZone"><a href="#2-1-Revisit-ARM-TrustZone" class="headerlink" title="2.1 Revisit: ARM TrustZone"></a>2.1 Revisit: ARM TrustZone</h2><h3 id="2-1-1-Basic-Settings"><a href="#2-1-1-Basic-Settings" class="headerlink" title="2.1.1 Basic Settings"></a>2.1.1 Basic Settings</h3><p>ARM 从 ARMv6 的架构开始引入了 TrustZone 技术。</p><p> TrustZone 在硬件层面，借助 Secure Configuration Register（SCR）将 CPU 的工作状态分为了<strong>正常世界状态 （Normal World Status，NWS）和安全世界状态 （Secure World Status，SWS）</strong>。</p><blockquote><p>CPU 在访问安全设备或者安全内存地址空间时，芯片级别的安全扩展组件会去<strong>校验 CPU 发送的访问请求的安全状态读写信号位</strong>（Non-secure bit，NS bit）是 0 还是 1，以此来判定当前 CPU 发送的资源访问请求是安全请求还是非安全请求。</p><p>而处于非安全状态的 CPU 将访问指令发送到系统总线上时，其访问请求的安全状态读写信号位都会被强制设置成 1，表示当前 CPU 的访问请求为非安全请求。</p><p>而非安全请求试图去访问安全资源时会被安全扩展组件认为是非法访问的，于是就禁止其访问安全资源，因此该 CPU 访问请求的返回结果要么是访问失败，要么就是返回无效结果，这也就实现了对系统资源硬件级别的安全隔离和保护。</p></blockquote><p>支持 TrustZone 的芯片提供了对外围硬件资源的硬件级别的保护和安全隔离。当 CPU 处于正常世界状态时，任何应用（包括 Rich OS）都无法访问安全硬件设备，也无法访问属于安全世界状态下的内存、缓存（Cache）以及其他外围安全硬件设备。总的来说，TrustZone 需要起到以下作用：</p><ul><li>隔离功能（安全状态和非安全状态）；</li><li>外设和内存 （物理上分开）；</li><li>总线请求；</li></ul><p>这是一个支持 TZ 的 SoC：</p><p><img src="imgs/arm-tz-soc.png" width="650px" /></p><p>TEE 支持基于 TrustZone 提供可信运行环境，为开发人员提供了 API，以方便他们开发实际应用程序。</p><p>在实际应用时，可以将用户的敏感数据保存到 TEE 中，并由可信应用（TA）使用重要算法和处理逻辑来完成对数据的处理。当需要使用用户的敏感数据做身份验证时，则通过在 REE 侧定义具体的请求编号（ID）从 TEE  侧获取验证结果。验证的整个过程中用户的敏感数据始终处于 TEE 中，REE 侧无法查看到任何 TEE 中的数据。对于 REE 而言，TEE 中的 TA 相当于一个黑盒，只会接受有限且提前定义好的合法调用（TEEC），而至于这些合法调用到底是什么作用，会使用哪些数据，做哪些操作在 REE 侧是无法知晓的。如果在 REE 侧发送的调用请求是非法请求，TEE 内的 TA 是不会有任何的响应或是仅返回错误代码，并不会暴露任何数据给 REE 侧。</p><p>TEE 的系统配置、内部逻辑、安全设备和安全资源的划分是与 CPU 的集成电路设计紧密挂钩的，<strong>使用 ARM 架构设计的不同 CPU，TEE 的配置完全不一样</strong>。国内外针对不同领域的 CPU 也具有不同的 TEE 解决方案。</p><p>这里我想选择开源的 OPTEE 来进行移植，因为它文档支持丰富、提供了完整的 SDK、遵循 GP API。</p><h3 id="2-1-2-ARM-v8-的-TrustZone"><a href="#2-1-2-ARM-v8-的-TrustZone" class="headerlink" title="2.1.2 ARM-v8 的 TrustZone"></a>2.1.2 ARM-v8 的 TrustZone</h3><h3 id="2-1-3-总线安全扩展"><a href="#2-1-3-总线安全扩展" class="headerlink" title="2.1.3 总线安全扩展"></a>2.1.3 总线安全扩展</h3><h3 id="2-1-4-TrustZone-中断控制"><a href="#2-1-4-TrustZone-中断控制" class="headerlink" title="2.1.4 TrustZone 中断控制"></a>2.1.4 TrustZone 中断控制</h3><h3 id="2-1-5-MMU-安全扩展"><a href="#2-1-5-MMU-安全扩展" class="headerlink" title="2.1.5 MMU 安全扩展"></a>2.1.5 MMU 安全扩展</h3><h3 id="2-1-6-Cache-amp-TLB-安全扩展"><a href="#2-1-6-Cache-amp-TLB-安全扩展" class="headerlink" title="2.1.6 Cache &amp; TLB 安全扩展"></a>2.1.6 Cache &amp; TLB 安全扩展</h3><p>TODO</p><h2 id="2-2-ARM-Trusted-Firmware"><a href="#2-2-ARM-Trusted-Firmware" class="headerlink" title="2.2 ARM Trusted Firmware"></a>2.2 ARM Trusted Firmware</h2><h3 id="2-2-1-Concepts"><a href="#2-2-1-Concepts" class="headerlink" title="2.2.1 Concepts"></a>2.2.1 Concepts</h3><p>ARM可信任固件（ARM Trusted Firmware，ATF）是由 ARM 官方提供的底层固件，该固件统一 了 ARM 底层接口标准，如电源状态控制接口（Power Status Control Interface，PSCI）、安全启 动需求（Trusted Board Boot Requirements， TBBR）、安全世界状态（SWS）与正常世界状态（NWS）切换的安全监控模式调用（SMC）操作等。ATF 旨在将 ARM 底层的操作统一使代码能够重用和便于移植。</p><h3 id="2-2-2-ATF-主要功能"><a href="#2-2-2-ATF-主要功能" class="headerlink" title="2.2.2 ATF 主要功能"></a>2.2.2 ATF 主要功能</h3><p>ATF 的源代码共分为 bl1、bl2、bl31、bl32、bl33 部分，其中：</p><ul><li>bl1、bl2、bl31 部分属于固定的固件；</li><li>bl32 和 bl33 分别用于加载 TEE OS 和 REE 侧的镜像；整个加载过程可配置成安全启动的方式，每一个镜像文件在被加载之前都会验证镜像文件的电子签名是否合法；</li><li>bl31 任务是接受 TEE OS 注册的服务入口点，并负责完成安全世界状态和正常世界状态之间的切换；</li></ul><p>ATF主要完成的功能如下：</p><ul><li><p>配置和初始化：</p><ul><li><p>初始化 Secure World 状态运行环境、异常向量、 控制寄存器、中断控制器、配置平台的中断。</p></li><li><p>初始化 ARM 通用中断控制器（General Interrupt Controller，GIC）2.0 版本和 3.0 版本的驱动初始化。</p></li><li><p>执行 ARM 系统 IP 的标准初始化操作以及安全扩展组件的基本配置。</p></li></ul></li><li><p>安全监控模式调用（Secure Monitor Call， SMC）请求的逻辑处理代码（Monitor 模式 / EL3）。</p></li><li>实现可信板级引导功能，对引导过程中加载的镜像文件进行电子签名检查。</li><li>支持自有固件的引导，开发者可根据具体需求将自有固件添加到 ATF 的引导流程中。</li></ul><p><img src="imgs/atf-arch.png" width="650px" /></p><p>其中：</p><ul><li><p>SMC Dispatcher：处理非安全世界的 SMC 请求，决定哪些 SMC 由 Trusted Firmware 在 EL3 处理，哪些转发给 TEE 进行处理；</p></li><li><p>Trusted Firmware 处理 PSCI 任务、或者 SoC 相关工作，例如一个播放 DRM Video 的调用情况：</p><p><img src="imgs/atf-call-eg.png" width="650px" /></p><ol><li>调用相关 CA，使用 <code>libDRM.so</code>；</li><li>库调用 TrustZone Driver 向 Secure World 中的 TA 发起请求，同时将相关信息传递到 shared message buffer（非安全）中；</li><li>SMC 进入 Secure World 的 TEE OS 后，从 message buffer 中获取相关信息并：<ul><li><strong>Trusting the message</strong>：由于 message 是不可信的，所以 secure world 需要对这些内容进行一些认证；</li><li><strong>Scheduling</strong>：对于 PSCI 类型快速处理并且不频繁请求，进入 EL3 处理完后退出到非安全状态。对于一些需要 TEE OS 处理的任务，不能被非安全中断打断，避免造成安全服务不可用；</li></ul></li></ol></li><li><p>以 OPTEE 为例，更细致一点：</p><p><img src="imgs/arm64-tz-components.png" width="450px" /></p><ul><li><p>CA 一般不直接使用 TEE Client API，而是使用中间 Service API 提供的服务，TEE Client API 再调用 OP-TEE Driver（暴露在设备文件系统 <code>/dev/tee0</code>），完成后续动作；</p></li><li><p>TEE Supplicant 为 TEE Daemon，有权使用 <code>/dev/teepriv0</code> 设备，在 OP-TEE 驱动的<u>挂载过程中</u>会建立正常世界状态与安全世界状态之间的共享内存，用于 OP-TEE Driver 与 OP-TEE 之间的数据共享；同时还会创建两个链表，分别用于保存来自 OP-TEE 的 RPC 请求和发送 RPC 请求的处理结果给 OP-TEE；</p><blockquote><p><strong>来自 OP-TEE 的 RPC 请求主要包括 socket 操作、REE 侧文件系统操作、加载 TA 镜像文件、数据库操作、共享内存分配和注册操作等</strong>。</p></blockquote><ul><li>该进程在 Linux 系统启动过程中被自动创建，在编译时，该进程的启动信息会被写入到 <code>/etc/init.d</code> 文件中，而该进程的可执行文件则被保存在文件系统的 <code>bin</code> 目录下；</li><li>该进程中会使用一个 loop 循环接收来自 OP-TEE 的 RPC 请求，且每次获取到来自 OP-TEE 的 RPC 请求后都会自动创建一个线程，用于接收 OP-TEE 驱动队列中来自 OP-TEE 的 RPC 请求信息，之所以这么做是因为时刻需要保证在 REE 侧有一个线程来接收 OP-TEE 的请求，实现 RPC 请求的并发处理；</li></ul></li></ul></li></ul><h3 id="2-2-3-Targets"><a href="#2-2-3-Targets" class="headerlink" title="2.2.3 Targets"></a>2.2.3 Targets</h3><p>现在我们就明白了，如果我们想把 TEE 功能移植到一个 ARM OS 上（例如 ChCore，OpenHarmony EduDist），需要集齐下面的组件：</p><ul><li>Normal World 状态的客户端库（提供给 CA 使用的 <code>libteec</code>）；</li><li>Normal World 状态的可信驱动（给 Rich OS Kernel 使用的驱动，用以发起 SMC 等操作）；</li><li>Secure World 的可信内核系统及配套的可信硬件驱动（TEE OS &amp; Secure Driver）；</li><li>安全世界状态的可信应用库（<code>libutee</code>）；</li><li>ATF 固件（应配置启动参数）；</li></ul><h2 id="2-3-OPTEE-启动过程试验"><a href="#2-3-OPTEE-启动过程试验" class="headerlink" title="2.3 OPTEE + 启动过程试验"></a>2.3 OPTEE + 启动过程试验</h2><h3 id="2-3-1-基于-QEMU-的-OPTEE-启动过程"><a href="#2-3-1-基于-QEMU-的-OPTEE-启动过程" class="headerlink" title="2.3.1 基于 QEMU 的 OPTEE 启动过程"></a>2.3.1 基于 QEMU 的 OPTEE 启动过程</h3><p>观察编译后的二进制目录：</p><p><img src="imgs/optee-build-dir.png" width="650px" /></p><ul><li><p>Linux (Rich OS) 镜像以及根文件系统：</p><ul><li><p>Image、linux.bin、uImage（u-boot wrapper 的 Image）；</p></li><li><p>rootfs.cpio.gz；</p></li></ul></li><li><p>ATF 固件（with OPTEE OS configurations）以及 u-boot；</p></li></ul><p>在 OPTEE 官方提供的启动脚本中，入口 BIOS 为 <code>bl1.bin</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">run-only:</span><br><span class="line">    <span class="built_in">ln</span> -sf $(ROOT)/out-br/images/rootfs.cpio.gz $(BINARIES_PATH)/</span><br><span class="line">    $(call check-terminal)</span><br><span class="line">    $(call run-help)</span><br><span class="line">    $(call launch-terminal,54320,<span class="string">&quot;Normal World&quot;</span>)</span><br><span class="line">    $(call launch-terminal,54321,<span class="string">&quot;Secure World&quot;</span>)</span><br><span class="line">    $(call wait-for-ports,54320,54321)</span><br><span class="line">    <span class="built_in">cd</span> $(BINARIES_PATH) &amp;&amp; $(QEMU_BUILD)/aarch64-softmmu/qemu-system-aarch64 \</span><br><span class="line">        -nographic \</span><br><span class="line">        -serial tcp:localhost:54320 -serial tcp:localhost:54321 \</span><br><span class="line">        -smp $(QEMU_SMP) \</span><br><span class="line">        -s -S -machine virt,secure=on,mte=$(QEMU_MTE),gic-version=$(QEMU_GIC_VERSION),virtualization=$(QEMU_VIRT) \</span><br><span class="line">        -cpu $(QEMU_CPU) \</span><br><span class="line">        -d unimp -semihosting-config <span class="built_in">enable</span>=on,target=native \</span><br><span class="line">        -m $(QEMU_MEM) \</span><br><span class="line">        -bios bl1.bin\</span><br><span class="line">        -initrd rootfs.cpio.gz \</span><br><span class="line">        -kernel Image -no-acpi \</span><br><span class="line">        -append <span class="string">&#x27;console=ttyAMA0,38400 keep_bootcon root=/dev/vda2 $(QEMU_KERNEL_BOOTARGS)&#x27;</span> \</span><br><span class="line">        $(QEMU_XEN) \</span><br><span class="line">        $(QEMU_EXTRA_ARGS)</span><br></pre></td></tr></table></figure><p>先观察启动日志：</p><p><img src="imgs/optee-qemu-start-1.png" width="550px" /></p><p><img src="imgs/optee-qemu-start-2.png" width="550px" /></p><p>ATF 作为最底层固件，OP-TEE OS、 BootLoader、Linux 内核的加载都是由 ATF 来完成的，而且 <strong>ATF 实现了安全引导的功能</strong>。</p><p>bl2 启动时通过触发 SMC 通知 bl1 将 CPU 控制权限交给 bl31，bl31 通过解析特定段中是否存在 OP-TEE Kernel 的入口来确定是否需要加载 OP-TEE。OP-TEE Kernel 启动后会触发安全监控模式调用重新进入到 bl31 中继续执行。</p><p>bl31 通过查询链表的方式获取下一个需要被加载 REE 侧的镜像文件，并设定好 REE 侧运行时 CPU 的状态和运行环境，然后退出 EL3 并进入 REE 侧镜像文件的启动，一般第一个 REE 侧镜像文件为 BootLoader，BootLoader 会加载 Linux 内核。</p><p>上面的安全引导过程是一个比较复杂和标准化的过程：</p><p><img src="imgs/atf-boot-seq.png" width="650px"/></p><h3 id="2-3-2-设备树文件（Device-Tree）"><a href="#2-3-2-设备树文件（Device-Tree）" class="headerlink" title="2.3.2 设备树文件（Device Tree）"></a>2.3.2 设备树文件（Device Tree）</h3><p>Linux kernel 在 ARM 架构中引入 <code>device tree</code>（flattened device tree，FDT）的时候，怀揣着一个 Unify Kernel 的梦想，即同一个 Image，通过切换不同的 DTB（device tree binary/blob）支持多个不同的平台。</p><p>device tree 在 kernel 中普及之后，U-Boot 也引入了 device tree 的概念。因此为了和 kernel 类似，U-Boot也需要一种新的 image 格式，这种格式需要支持以下特性：</p><ul><li>Image 中需要包含多个 DTB 文件；</li><li>可以方便的选择使用哪个 DTB 文件；</li></ul><p>结合以上两点需求，U-Boot 推出了新的 image 格式：FIT image（flattened image tree）。它利用了 Device Tree Source files（DTS）的语法，生成的 Image 文件也和 DTB 文件类似（从 ITS 编译为 ITB）；</p><p>在 ARM64 架构下，U-Boot 启动 Linux 内核<strong>必须使用设备树（Device Tree）文件</strong>。</p><p>与早期的 ARM32 架构不同，ARM64 Linux 内核完全移除了对“板级文件”（Board-Specific Files）的支持，强制要求通过设备树（<code>*.dtb</code>）描述硬件配置。没有设备树，内核无法获取硬件信息（如 CPU、内存、外设等），导致启动失败。</p><p>U-Boot启动内核时需完成以下步骤：</p><ol><li><strong>加载内核镜像</strong>：将 <code>Image</code> 或 <code>vmlinux</code> 加载到内存。</li><li><strong>加载设备树文件</strong>：将编译后的设备树二进制文件（<code>*.dtb</code>）加载到另一块内存区域。</li><li><strong>传递参数</strong>：通过寄存器（如 ARM64 的<code>x0</code>）将设备树地址告知内核。</li></ol><p>可以在 U-Boot 命令行中执行下面的指令来查看（如果是以 <code>-sd</code> 形式给出）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看设备树加载地址</span></span><br><span class="line"><span class="built_in">printenv</span> fdtaddr</span><br><span class="line"><span class="comment"># 手动加载设备树示例</span></span><br><span class="line"><span class="comment"># sd/emmc id=0, partition 1</span></span><br><span class="line">load mmc 0:1 <span class="variable">$&#123;fdtaddr&#125;</span> /boot/myboard.dtb</span><br></pre></td></tr></table></figure><h3 id="2-3-2-U-Boot-OpenHarmony-Edu-Dist-启动方案"><a href="#2-3-2-U-Boot-OpenHarmony-Edu-Dist-启动方案" class="headerlink" title="2.3.2 U-Boot + OpenHarmony Edu Dist 启动方案"></a>2.3.2 U-Boot + OpenHarmony Edu Dist 启动方案</h3><p>考虑最简单的情形，直接由  U-Boot 启动 OpenHarmony，如果成功后再尝试加入 OPTEE。</p><h4 id="U-Boot-指令-UBOOT-BOOTCOMMAND"><a href="#U-Boot-指令-UBOOT-BOOTCOMMAND" class="headerlink" title="U-Boot 指令 (UBOOT_BOOTCOMMAND)"></a>U-Boot 指令 (<code>UBOOT_BOOTCOMMAND</code>)</h4><p><code>booti</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">booti &lt;kernel_addr&gt; [initrd_addr[:initrd_size]] [fdt_addr]</span><br></pre></td></tr></table></figure><ul><li><code>booti</code> 命令用于引导内核时加载一个二进制的内核镜像 (<code>Image</code>)，通常针对 ARM64 架构。</li><li>与 <code>bootm</code> / <code>bootz</code> 类似，<code>booti</code> 也用于启动内核，但是二者针对的镜像格式不同。   <ul><li><code>bootm</code> 主要用于启动 uImage 格式的内核（一般由 <code>mkimage</code> 工具打包而成）；</li><li><code>bootz</code> 用于启动 zImage 格式的内核；</li><li><code>booti</code> 则用于加载 Linux 的原始内核镜像（<code>Image</code> 文件）；</li></ul></li></ul><h4 id="尝试一：将-Image-和-ramdisk-img-打包成-boot-img-FAILED"><a href="#尝试一：将-Image-和-ramdisk-img-打包成-boot-img-FAILED" class="headerlink" title="尝试一：将 Image 和 ramdisk.img 打包成 boot.img (FAILED)"></a>尝试一：将 <code>Image</code> 和 <code>ramdisk.img</code> 打包成 <code>boot.img</code> (FAILED)</h4><p>理想情况下，启动过程应该是：</p><ul><li>bootloader 初始化 ROM 和 RAM 等硬件，加载分区表信息。</li><li>bootloader 根据分区表加载 <code>boot.img</code>，从中解析并加载 <code>ramdisk.img</code> 到内存中。</li><li>bootloader 准备好分区表信息，ramdisk 地址等信息，进入内核，内核加载 ramdisk 并执行 init。</li><li>init 准备初始文件系统，挂载 <code>required.fstab</code>（包括 <code>system.img</code> 和 <code>vendor.img</code> 的挂载）。</li><li>扫描 <code>system.img</code> 和 <code>vendor.img</code> 中 <code>etc/init</code> 目录下的启动配置脚本，执行各个启动命令。</li></ul><p>先 dump 出 <code>-machine virt</code> 机器的 DTB 文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-machine virt,dumpdtb=./virt.dtb</span><br></pre></td></tr></table></figure><p>使用 <code>u-boot</code> 编译后的工具 <code>dtc</code> 解析内容（便于查看）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dtc -I dtb -O dts ./virt.dtb &gt; virt.dts</span><br></pre></td></tr></table></figure><p>然后自行为 U-Boot 编写 ITS（Image Tree Source）文件：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">/dts-v1/;</span><br><span class="line"> </span><br><span class="line">/ &#123;</span><br><span class="line">    description = <span class="string">&quot;U-Boot zImage-dtb-ramdisk&quot;</span>;</span><br><span class="line">    <span class="meta">#address-cells = <span class="string">&lt;1&gt;</span>;</span></span><br><span class="line">    images &#123;</span><br><span class="line">        kernel<span class="number">-1</span> &#123;</span><br><span class="line">            description = <span class="string">&quot;Linux kernel &quot;</span>;</span><br><span class="line">            data = /incbin/(<span class="string">&quot;./Image&quot;</span>);</span><br><span class="line">            type = <span class="string">&quot;kernel&quot;</span>;</span><br><span class="line">            arch = <span class="string">&quot;aarch64&quot;</span>;</span><br><span class="line">            os = <span class="string">&quot;linux&quot;</span>;</span><br><span class="line">            compression = <span class="string">&quot;none&quot;</span>;</span><br><span class="line">            load = &lt;<span class="number">0x40800000</span>&gt;;</span><br><span class="line">            entry = &lt;<span class="number">0x40800000</span>&gt;;</span><br><span class="line">        &#125;;</span><br><span class="line">         dtb<span class="number">-1</span> &#123;</span><br><span class="line">            description = <span class="string">&quot;ohos dtb &quot;</span>;</span><br><span class="line">            data = /incbin/(<span class="string">&quot;./virt.dtb&quot;</span>);</span><br><span class="line">            type = <span class="string">&quot;flat_dt&quot;</span>;</span><br><span class="line">            arch = <span class="string">&quot;aarch64&quot;</span>;</span><br><span class="line">            os = <span class="string">&quot;linux&quot;</span>;</span><br><span class="line">            compression = <span class="string">&quot;none&quot;</span>;</span><br><span class="line">        &#125;;</span><br><span class="line">        ramdisk<span class="number">-1</span> &#123;</span><br><span class="line">            description = <span class="string">&quot;Ramdisk Image&quot;</span>;</span><br><span class="line">            data = /incbin/(<span class="string">&quot;./ramdisk.img&quot;</span>);</span><br><span class="line">            type = <span class="string">&quot;ramdisk&quot;</span>;</span><br><span class="line">            arch = <span class="string">&quot;aarch64&quot;</span>;</span><br><span class="line">            os = <span class="string">&quot;linux&quot;</span>;</span><br><span class="line">            compression = <span class="string">&quot;none&quot;</span>;</span><br><span class="line">            load = &lt;<span class="number">0x44000000</span>&gt;;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;;</span><br><span class="line">    configurations &#123;</span><br><span class="line">        <span class="keyword">default</span> = <span class="string">&quot;conf-boot&quot;</span>;</span><br><span class="line">        conf-boot &#123;</span><br><span class="line">            description = <span class="string">&quot;booting ARM Linux Kernel Image Ramdisk&quot;</span>;</span><br><span class="line">            kernel = <span class="string">&quot;kernel-1&quot;</span>;</span><br><span class="line">            fdt = <span class="string">&quot;dtb-1&quot;</span>;</span><br><span class="line">            ramdisk = <span class="string">&quot;ramdisk-1&quot;</span>;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>mkimage</code> 编译生成 <code>boot.img</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkimage -f boot.its boot.img</span><br></pre></td></tr></table></figure><p>使用 <code>u-boot.bin</code>（U-Boot 编译产物，引导固件）作为 BIOS：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-bios u-boot.bin \</span><br></pre></td></tr></table></figure><p>然后尝试了两种加载方法：</p><ul><li><code>virtio-blk-device</code> 加载；</li><li>进入 U-Boot 命令行直接 load 文件：<ul><li>只显示 <code>virt.dtb</code> 加载到 <code>0x40000000</code> 处；</li></ul></li></ul><p>失败，U-Boot 未能识别分区情况。</p><h4 id="尝试二：将-Image-设备树-ramdisk-img-打包进-SD-设备-FAILED"><a href="#尝试二：将-Image-设备树-ramdisk-img-打包进-SD-设备-FAILED" class="headerlink" title="尝试二：将 Image, 设备树, ramdisk.img 打包进 SD 设备 (FAILED)"></a>尝试二：将 <code>Image</code>, 设备树, <code>ramdisk.img</code> 打包进 SD 设备 (FAILED)</h4><p>创建空的 SD Card 镜像：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=boot.disk bs=1M count=1024</span><br></pre></td></tr></table></figure><p>创建 GPT 分区，两个分区，一个存放 Kernel 和设备树，另一个存放 Rootfs：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgdisk -n 0:0:+10M -c 0:kernel boot.disk</span><br><span class="line">sgdisk -n 0:0:0 -c 0:rootfs boot.disk</span><br></pre></td></tr></table></figure><p>检查结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sgdisk -p boot.disk</span><br></pre></td></tr></table></figure><p>找个空闲的 loop 设备，并且把镜像挂载上去，准备写入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">losetup -f</span><br><span class="line"><span class="comment"># 假设返回 /dev/loopxx</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo losetup /dev/loopxx boot.disk</span><br><span class="line"><span class="comment"># 更新之前创建的表，让 host kernel 看到</span></span><br><span class="line">sudo partprobe /dev/loopxx</span><br></pre></td></tr></table></figure><p>对新的分区格式化为 EXT4：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.ext4 /dev/loopxxp1</span><br><span class="line">sudo mkfs.ext4 /dev/loopxxp2</span><br></pre></td></tr></table></figure><p>挂载到两个任意不同空目录上，然后复制文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t ext4 /dev/loopxxp1 p1/</span><br><span class="line">sudo mount -t ext4 /dev/loopxxp2 p2/</span><br><span class="line">sudo <span class="built_in">cp</span> Image p1/</span><br><span class="line">sudo <span class="built_in">cp</span> virt.dtb p1/</span><br><span class="line">sudo <span class="built_in">cp</span> ramdisk.img p2/</span><br></pre></td></tr></table></figure><p>卸载 loop 设备：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo umount p1 p2</span><br><span class="line">sudo losetup -d /dev/loopxx</span><br></pre></td></tr></table></figure><p>将写好的 <code>boot.disk</code> 作为 SD 设置 QEMU，U-Boot 启动时用 <code>mmc</code> 指令读取。</p><p>失败，U-Boot 未能通过内存校验。</p><h4 id="尝试三：使用-mkimage-生成-ITB-文件，U-Boot-直接加载-BUGGY"><a href="#尝试三：使用-mkimage-生成-ITB-文件，U-Boot-直接加载-BUGGY" class="headerlink" title="尝试三：使用 mkimage 生成 ITB 文件，U-Boot 直接加载 (BUGGY)"></a>尝试三：使用 <code>mkimage</code> 生成 ITB 文件，U-Boot 直接加载 (BUGGY)</h4><p>添加 OPTEE 时，注意编译选项给 OPTEE OS 指定的 <code>NS_SHM</code>（Non-Safe Shared Memory）从 <code>0x42000000</code> 开始。因此把 Kernel 地址改到 <code>0x42000000</code>，RamDisk 改到 <code>0x45000000</code>，然后将 <code>mkimage -f xxx.its boot.itb</code> 加载到 <code>0x50200000</code>，并且从该处启动。</p><p>最终成功启动 OPTEE Kernel，并且成功加载 OpenHarmony Kernel。但是卡在 Starting Kernel 这步，仍然在排查原因。</p><h2 id="2-4-OpenTrustee-ChCore-OS-迁移试验"><a href="#2-4-OpenTrustee-ChCore-OS-迁移试验" class="headerlink" title="2.4 OpenTrustee (ChCore OS) 迁移试验"></a>2.4 OpenTrustee (ChCore OS) 迁移试验</h2><p>未完待续…</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Part-1-机密计算与-TEE-技术入门知识整理&quot;&gt;&lt;a href=&quot;#Part-1-机密计算与-TEE-技术入门知识整理&quot; class=&quot;headerlink&quot; title=&quot;Part 1. 机密计算与 TEE 技术入门知识整理&quot;&gt;&lt;/a&gt;Part 1. 机密</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="OpenHarmony" scheme="https://blog.sjtuxhw.top/tags/OpenHarmony/"/>
    
    <category term="TEE" scheme="https://blog.sjtuxhw.top/tags/TEE/"/>
    
    <category term="TrustZone" scheme="https://blog.sjtuxhw.top/tags/TrustZone/"/>
    
  </entry>
  
  <entry>
    <title>另一个角度看 Rust 所有权和借用</title>
    <link href="https://blog.sjtuxhw.top/technical/rust-owner/"/>
    <id>https://blog.sjtuxhw.top/technical/rust-owner/</id>
    <published>2025-04-06T12:20:14.000Z</published>
    <updated>2025-04-17T15:47:07.262Z</updated>
    
    <content type="html"><![CDATA[<p>笔者在一开始了解 Rust 的内存管理机制的时候，阅读了官方文档，以及网络上的教科书，它们首先都引入了所谓 “所有权” 和 “借用” 的概念。</p><p>笔者认为这样的叙述方法非常合适，特别是如果读者对操作系统原理与实现、C/C++ 语言不了解，那么这么讲授的方法是大概是最好的。因为这能够很快教给读者 rust 语言的规则，而不需要很多的知识储备或者语境。</p><p>但这也带来了弊端：很多读者会把它当作只有 rust 才有的特性、规定，但实际上这种系统设计思想可以用在很多地方。对于为什么 rust 要这么设计，则需要读者学了很长时间之后，结合计算机相关基础知识才能慢慢领会。</p><p>因此，个人认为应该从 C/C++ 如何变得安全的角度来讨论 rust 的这个语言特性会比较方便，因为 rust 本身就是在对标 C/C++ 内存不安全的问题，然后在此基础上进行改进。</p><p>笔者想提供一种思路，从 C/C++ 开发者的视角来解释 rust 为什么这么设计，然后再定义这两个性质，希望能对读者有所启发。本人学识短浅，望读者勘误/斧正。</p><h3 id="如何设计一种全新的内存管理机制？"><a href="#如何设计一种全新的内存管理机制？" class="headerlink" title="如何设计一种全新的内存管理机制？"></a>如何设计一种全新的内存管理机制？</h3><p>假设你想创造一个新的编译型语言（就叫 rust），想和 C/C++ 一样，<u>支持直接操纵变量的内存地址、引用</u>，但想要更安全，并且正在为这个语言设计编译器。</p><p>我们知道，对于一个正常高级语言而言（无论是 C++/Java 还是其他的什么），为变量分配内存的方式有两类：</p><ul><li>一个是为编译时已知大小的变量分配（例如一个整型）。由于编译时长度已知，我们直接可以让编译器在栈（stack）上管理它们就行，这样不仅空间和时间效率高，而且不会有内存泄漏问题；</li><li>一个是运行时才知道大小的变量分配（例如一个保存用户输入的字符串）。这时需要程序运行时动态分配内存，也正是为了应对这种需求，OS 才有了 “堆”（heap）这种运行时内存结构，允许程序在运行时动态分配内存。</li></ul><p>好，第一个（已知大小）的变量分配已经解决。以 C++ 为例，我们在写编译器时只需要把这些变量考虑在内，然后生成汇编的时候在当前函数的 activation record 中按需要的大小移动 <code>%rsp</code> 分配就行。</p><p>对于第二个（编译时未知大小）的变量分配，我们通常有几种办法：</p><ul><li><p>语言直接向开发者暴露分配和释放堆空间的接口（C/C++）。这种内存管理方式需要手动分配和释放堆上内存，可能编写出内存不安全代码/内存泄漏；</p><blockquote><p>所谓内存不安全代码，可能是访问了指针指向的错误的内存区域、错误地修改了某块内存，导致程序意外结束。</p><p>所谓内存泄漏不是真正 “泄漏”，在 OS 的角度看，只是应用程序内存释放不及时，或者要释放的地址丢了（unreachable），导致释放速度赶不上分配速度，最终耗尽了当前进程的虚拟内存空间。</p></blockquote></li><li><p>语言向开发者暴露分配堆空间的接口 (Java/JavaScript) 或者 自动分配堆空间 (Python)，并采用自动垃圾回收（GC）的方式来释放堆空间。这种内存管理方式的好处是开发者不需要关心堆空间释放问题。</p><p>但是 GC 对程序的影响就很有讲究了，人们为了防止 “GC 释放速度赶不上分配速度” 这种情况发生，想了很多种 GC 算法，例如 mark &amp; swap、reference count、copy collection、generation collection、Parallel Scavenge、G1GC、ZGC 等等，限于篇幅不再展开。</p><p>即便想的如此周全，还有各种各样的问题：例如 STW（时停开销）、堆扫描的时间开销、循环引用导致内存泄漏（指 refcnt 方法），等等。</p></li></ul><p>于是你想，如果我们开发的语言既不想用第一种方法（不安全），也不想用第二种方法（性能没法达到 C/C++ 水平），能不能<strong>同时兼顾安全性和性能</strong>呢？</p><p>看起来只有<strong><u>在编译时就完成 “编译时未知大小的变量” 的堆分配工作</u></strong>了（让编译器帮我们管理堆空间）！</p><p>这就是 rust 语言在设计时考虑内存分配的思路。</p><p>好，现在我们需要解决针对变量操纵的几个问题：如何进行变量分配、如何进行变量传递（何时值传递、何时引用传递）。</p><p>不妨先完全借鉴 C/C++ 内存分配的思路，为 rust 设计一种方案：</p><ul><li>对于编译时已知大小的变量，直接分配在栈上，默认使用值传递。即便是复合数据类型（不管有没有指针域）都是如此。<ul><li>引入 “引用” 类型，来显式使用引用传递，节约构造成本。</li><li>复制构造和值传递的方法，就是逐内存 byte 的复制，我们不妨称之为 <strong><u>copy trait</u></strong>（<code>Copy</code> 特性）；</li></ul></li><li>对于编译时未知大小的变量，分配在堆上（需要手动释放），使用指针操纵（读写），指针本身使用值传递。</li></ul><p>这种方案对于当年的 rust 创造者来说不可容忍，因为：</p><ul><li>内存不安全。我们希望编译器帮我们回收堆上的空间；</li><li>指针可以通过值传递 / copy trait 到处传播，不知道何时释放比较合适，不知道会不会出现 UAF / double free 的问题。</li></ul><p>所以作出改进：</p><ol><li>指针（当然在 rust 中可以不叫指针，只是表示堆上数据的一个类型）可以值传递，但是必须让编译器可以追踪到、必须明确变量内存释放的 “责任” 在确定的变量身上（<strong><u>把这个动作和有“责任”的变量的生命周期绑定起来</u></strong>，类似 C++ RAII），以便进行准确无误的、及时的回收工作，并且不允许实现隐式的 copy trait；</li><li>用于引用传递的 “引用” 类型需要区分变量的可变性（因为 rust 之前设计的 “变量可变性” 要应对并发安全的场景）。并且需要注意到，引用也会影响到编译器对 “变量内存释放的责任” 的追踪。我们特别规定“引用”类型的传递是不能传递它引用的变量的 “内存释放的责任” 的！</li></ol><h3 id="对号入座"><a href="#对号入座" class="headerlink" title="对号入座"></a>对号入座</h3><p>其实，<u>改进 1 中 “内存释放的责任” 就是所有权抽象，改进 2 中的 “引用不能传递内存释放的责任” 就是借用抽象</u>。</p><p>到此为止，我们就能理解为什么 rust 要设计 “所有权” 和 “借用” 了。</p><p>现在笔者再搬出所有权、借用的 “规定”，你看看能不能对号入座了？</p><p>Rust “所有权” 制定了以下规则（为了明确内存释放的责任）：</p><ol><li><p>Rust 中的<strong><u>每个值（特指堆上的数据）在同一时刻只能被一个变量所拥有</u></strong>，这个变量被称为该值的 “<strong>所有者 (owner)</strong>”；</p><blockquote><p>从变量的角度说，同一时刻只能绑定一个特定的值；</p><p>其实有个特例（参见下文），可变引用（<code>&amp;mut T</code>）即便信息在栈上，也被所有权管理，因为它经过编译器翻译后底层是指针，为了确保赋值/复制的数据安全、方便所有权追踪，就这么设计了。</p></blockquote></li><li><p>当所有者离开绑定值声明的作用域后，这个值将被丢弃（drop）；</p></li></ol><p>就第一条而言，我们记住一个原则：<strong><u>“所有权” 是针对堆上的数据的</u></strong>，我们需要所有权管理的也就是堆上的数据。</p><p>就第二条而言，大多数语言在效果上都是差不多的：即一个变量只在声明有效的作用域内能够使用。</p><p>Rust “引用/借用” 制定了以下基本规则：</p><ol><li>引用需要区分变量的可变性（<code>&amp;T</code> 和 <code>&amp;mut T</code>，我们不难发现 <strong><u>Rust 的引用就是 C++ 指针的另一种表述形式</u></strong>，而不是 C++ 的引用）；</li><li>一个变量的不可变引用（<code>&amp;T</code>）生命周期内可以出现多次，因为不会改变所有权；<ul><li>因为不可变引用的安全性，<code>&amp;T</code> 单独实现 Copy Trait（意味着它不被所有权管理）；</li></ul></li><li>一个变量的可变引用（<code>&amp;mut T</code>）在它的生命周期内只能出现一次、并且与不可变引用互斥。这是为了防止变量可见性冲突，也就是并发程序中共享资源的读者和写者间的关系；<ul><li>同时考虑到可变引用也要支持赋值/复制，因此不实现 Copy Trait（<u>意味着即便它不存放在堆上，也被所有权管理</u>！）；</li><li>考虑一下，可变引用被编译器翻译后，底层实现是指针，因此在所有权管理范围内；</li></ul></li></ol><p>Rust 通过上面的 “所有权” 和 “借用” 的规则，巧妙地保证了：</p><ul><li>编译器能够始终追踪到堆上变量整个生命周期的使用情况，并且能自动判断释放的合适时机，不需要手动释放，也不需要 GC；</li><li>引用不会干扰内存管理的安全性；</li><li>可变引用的互斥性，结合变量可变性限制，<strong><u>杜绝数据竞争现象</u></strong>，维护数据安全假设。</li></ul><p>最后，Rust 利用是否定义 Copy Trait 将一个类型是否会被所有权管理区分开来，方便具体实现。</p><h3 id="归纳，然后演绎"><a href="#归纳，然后演绎" class="headerlink" title="归纳，然后演绎"></a>归纳，然后演绎</h3><p>现在我们明白了 Rust “所有权” 和 “借用” 的内容和原因，Rust 所有看似难以理解的语言特性都能得到合理解释。我们举几个例子：</p><p><strong>Q0：为什么下面的例子有所有权问题？是不是违反了 “引用不会传递所有权” 的约定？</strong></p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">s1</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">mut1</span> = &amp;<span class="keyword">mut</span> s1;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x</span> = mut1;</span><br><span class="line">    <span class="comment">// 已知 println 前 x 已经析构，并不存在两个可变引用冲突的问题</span></span><br><span class="line">    <span class="comment">// 但是报错 mut1 所有权已经转移</span></span><br><span class="line">    <span class="title function_ invoke__">println</span>(<span class="string">&quot;&#123;&#125;&quot;</span>, mut1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>答：并没有违反。因为直到最后 <code>s1</code> 仍然可以访问（你可以把 <code>mut1</code> 改成 <code>s1</code> 看看），证明 <code>s1</code> 的可变引用不会传递 <code>s1</code> 的所有权。</p><p>这个例子只是恰好展示了可变引用的性质。我们知道如果同时定义同个变量的两个可变引用，编译器会提示冲突：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">v</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello,&quot;</span>);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r</span> = &amp;<span class="keyword">mut</span> v;</span><br><span class="line"><span class="comment">// 编译器错误：有多于一个可变引用</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable">x</span> = &amp;<span class="keyword">mut</span> v;</span><br></pre></td></tr></table></figure><p>但是编译器允许可变引用传递，不过代价是所有权传递：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">v</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello,&quot;</span>);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r</span> = &amp;<span class="keyword">mut</span> v;</span><br><span class="line"><span class="comment">// 目前是正确的，但编译器的做法是，将 `r` 的所有权传给 `x` 了</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable">x</span> = r;</span><br></pre></td></tr></table></figure><p>这恰恰印证了一点：<strong><u><code>&amp;T</code> 不可变引用是有 Copy Trait 的，但是可变引用 <code>&amp;mut T</code>没有 Copy Trait，因此赋值会出现所有权转移</u></strong>（参见“对号入座”一节的可变引用规则）。</p><p><strong><u>可变引用和 不可变引用 相当于 C++ 中的指针和常量指针</u></strong>（不是 C++ 引用，想想为什么）。</p><p>因此最开始的例子中的问题是，<code>s1</code> 并没有所有权转移，所有权转移的是 <code>mut1</code> 可变引用变量本身：<code>mut1</code> 所有权转移到 <code>x</code> 上，然后 <code>x</code> 立即被编译器设计的汇编析构了，<code>mut1</code> 当然是无效的。</p><p><strong>Q1：为什么下面的例子没有内存释放或者所有权的问题？</strong></p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">s1</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">len</span> = <span class="title function_ invoke__">calculate_length</span>(&amp;s1);</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;The length of &#x27;&#123;&#125;&#x27; is &#123;&#125;.&quot;</span>, s1, len);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">fn</span> <span class="title function_">calculate_length</span>(s: &amp;<span class="type">String</span>) <span class="punctuation">-&gt;</span> <span class="type">usize</span> &#123;</span><br><span class="line">    s.<span class="title function_ invoke__">len</span>()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>答：因为 <code>&amp;String</code> Rust 引用不会传递所有权。在 <code>calculate_length</code> 结束后，编译器知道所有权（释放的责任）不在局部变量 <code>s</code> 这里，就不会释放它；</p><p>所有权仍然位于 <code>s1</code>，因此只有 <code>main</code> 结束（<code>s1</code> 生命周期结束），编译器才会去释放 <code>s1</code> 的堆上空间；</p><p><strong>Q2：为什么下面的例子无法通过编译？</strong></p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">s</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line">    <span class="title function_ invoke__">change</span>(&amp;s);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">fn</span> <span class="title function_">change</span>(some_string: &amp;<span class="type">String</span>) &#123;</span><br><span class="line">    some_string.<span class="title function_ invoke__">push_str</span>(<span class="string">&quot;, world&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>答：因为 Rust 引用根据变量的可变性作出区分。更改不可变引用就破坏了 Rust 作出的数据安全假设。</p><p><strong>Q3：为什么可变引用同时只能存在一个？</strong></p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">s</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r1</span> = &amp;<span class="keyword">mut</span> s;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r2</span> = &amp;<span class="keyword">mut</span> s;</span><br><span class="line"><span class="comment">// 错误</span></span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;, &#123;&#125;&quot;</span>, r1, r2);</span><br></pre></td></tr></table></figure><p>答：虽然 Rust 引用不传递所有权，但是多个可变引用在并发场景相当于共享资源多写者，破坏了 Rust 的数据安全假设。</p><p><strong>Q4：为什么可变引用和不可变引用不能同时存在？</strong></p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">s</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r1</span> = &amp;s;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r2</span> = &amp;s;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r3</span> = &amp;<span class="keyword">mut</span> s;</span><br><span class="line"><span class="comment">// 错误</span></span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;, &#123;&#125;, and &#123;&#125;&quot;</span>, r1, r2, r3);</span><br></pre></td></tr></table></figure><p>答：同 Q3。这是并发场景共享资源临界区同时存在写者和读者，破坏了 Rust 数据安全假设。</p><p><strong>Q5：为什么这样的写法又可以？</strong></p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">s</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r1</span> = &amp;s;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r2</span> = &amp;s;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">r3</span> = &amp;<span class="keyword">mut</span> s;</span><br><span class="line"><span class="comment">// 正确</span></span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;&quot;</span>, r3);</span><br></pre></td></tr></table></figure><p>答：在新版 Rust 编译器中，默认使用 Liveness Analysis 进行 Dead Code Elimination。<code>r1</code> 和 <code>r2</code> 的生命周期只有定义的一行。到定义 <code>r3</code> 时，<code>r1/r2</code> 已经死亡了。如果读者了解过编译原理，这很容易可以看出。</p><p><strong>Q6：为什么这样的引用无法通过编译？</strong></p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">reference_to_nothing</span> = <span class="title function_ invoke__">dangle</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">fn</span> <span class="title function_">dangle</span>() <span class="punctuation">-&gt;</span> &amp;<span class="type">String</span> &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">s</span> = <span class="type">String</span>::<span class="title function_ invoke__">from</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line">    &amp;s</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>答：这里和 C/C++ 有很大不同。虽然 <code>s</code> 创建在堆上，但是请记住回收是交给编译器完成的，而编译器是将回收动作和变量生命周期绑定的。</p><p>也就是说，<code>s</code> 作为一个局部变量，在函数返回后生命周期就会结束，编译器会释放掉 <code>s</code> 涉及的堆空间。这个效果就和 C/C++ 返回指向函数栈上的局部变量的指针一样。</p><p>总的来说，Rust 这么做就是为了方便追踪变量的释放责任（所有权），方便判断恰当的释放时机。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对 C/C++ 开发者，总结一下：</p><ul><li><p>Rust 的所有权管理的是堆上的数据。本质上是通过所有权机制，让编译器帮你管理堆上的空间，不需手动分配和释放堆空间、不需 GC；</p><blockquote><p>也就是：只在编译期完成所有内存分配、释放的规则的制定。</p></blockquote></li><li><p>所有实现 <code>Copy</code> traits 的类型，都是代表可以安全进行值拷贝的类型。例如不含指针域的定长简单复合类型、基本类型。</p><ul><li><code>Copy</code> traits 类比为 C++ 中的默认复制构造方法，隐式调用。</li></ul></li><li><p>其他复合类型（一般是含有指针域的），在 rust 中都不能实现 <code>Copy</code> trait，因为可能隐式复制构造会造成指针共享，破坏了 rust 的所有权机制。</p></li><li><p>Rust 函数传参和 C++ 相同，默认值传递（无论是什么类型），除非显式使用引用记号（<code>&amp;T / &amp;mut T</code>，即 Rust 引用）；</p><ul><li>需注意 Rust 对数据竞争的安全性要求；</li><li>需注意可变引用没有 Copy Trait，因此它本身就被所有权管理；</li><li>需注意引用的生命周期，防止冲突。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;笔者在一开始了解 Rust 的内存管理机制的时候，阅读了官方文档，以及网络上的教科书，它们首先都引入了所谓 “所有权” 和 “借用” 的概念。&lt;/p&gt;
&lt;p&gt;笔者认为这样的叙述方法非常合适，特别是如果读者对操作系统原理与实现、C/C++ 语言不了解，那么这么讲授的方法是大概</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="Programming" scheme="https://blog.sjtuxhw.top/tags/Programming/"/>
    
    <category term="C++" scheme="https://blog.sjtuxhw.top/tags/C/"/>
    
    <category term="Rust" scheme="https://blog.sjtuxhw.top/tags/Rust/"/>
    
  </entry>
  
  <entry>
    <title>更多的 I/O 多路复用</title>
    <link href="https://blog.sjtuxhw.top/review/io-mul-more/"/>
    <id>https://blog.sjtuxhw.top/review/io-mul-more/</id>
    <published>2025-04-01T04:13:25.000Z</published>
    <updated>2025-04-17T15:47:17.555Z</updated>
    
    <content type="html"><![CDATA[<p>最近总结了一些 OS I/O 多路复用的知识。之前对 I/O Multiplexer 的认知还停留在 <code>select</code> 系统调用，现在是时候扩展一下视野了。</p><h3 id="1-从-Socket-模型开始"><a href="#1-从-Socket-模型开始" class="headerlink" title="1. 从 Socket 模型开始"></a>1. 从 Socket 模型开始</h3><p>Socket 作为一个应用层和传输层间的的抽象，支持网络层 IPv4 / IPv6，以及传输层 TCP / UDP。</p><p>双方要进行网络通信前，各自需要创建一个 Socket。</p><p>如果是基于 UDP 的套接字：</p><p><img src="imgs/udp-socket.png" width="350px" /></p><p>如果是基于 TCP 的套接字：</p><p><img src="imgs/tcp-socket.png" width="450px" /></p><p>以基于 TCP 的套接字为例，首先使用 <code>socket()</code> 创建一个网络协议为 IPv4，以及传输协议为 TCP 的 Socket 结构体，然后使用 <code>bind()</code> 绑定 Server IP 和进程服务端口 port，并监听 <code>listen()</code> 在该端口上（<code>listen</code> 仅改变状态）；</p><blockquote><p>之所以需要指定 Server IP，是因为一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址。Socket 允许指定监听的网卡。<code>0.0.0.0</code> 表示监听所有的 network interfaces；</p><p>port 即为传输层信息，对应指定线程的服务。</p></blockquote><p>Server 端 socket 进入监听状态后，调用阻塞函数 <code>accept()</code>，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。</p><p>如果客户端使用 <code>connect()</code> 发起连接后，双方会进行 TCP 3 次握手。在连接过程中，server 端 OS kernel 会为每个 socket 都维护两个队列：</p><ul><li>一个是 “还没完全建立” 连接的队列，称为 <strong>TCP 半连接队列</strong>（服务端 socket 处于 <code>syn_rcvd</code> 状态）；</li><li>另一个是 “已经建立” 连接的队列，称为 <strong>TCP 全连接队列</strong>，这个队列都是完成了三次握手的连接（此时服务端处于 <code>established</code> 状态）；</li></ul><p>当全连接队列不为空时，内核会拿出一个已连接的 socket（称为 <strong>已连接 socket</strong>）并响应任意一个阻塞在 <code>accept()</code> 上的服务端线程，此时该服务线程会使用这个已连接的 socket 来响应客户端（一般会新开一个进程/线程/使用其他方案来处理）。</p><blockquote><p>注意，<code>accept()</code> 第一参数是<strong><u>监听 socket 的文件描述符</u></strong>，返回的是<strong><u>已连接 socket 的文件描述符</u></strong>，它们不一样。因为考虑多线程情况，<code>accept</code> 放在一个循环里，这个监听 socket 专门用于接收连接请求。</p></blockquote><h3 id="2-提升-Socket-服务能力：为什么选择-I-O-多路复用"><a href="#2-提升-Socket-服务能力：为什么选择-I-O-多路复用" class="headerlink" title="2. 提升 Socket 服务能力：为什么选择 I/O 多路复用"></a>2. 提升 Socket 服务能力：为什么选择 I/O 多路复用</h3><p>那么一般情况服务端应该怎么做来处理大量的 socket 连接请求？</p><p>前面说过，服务端可以新开一个进程来处理客户端连接，但每次 <code>fork()</code> 创建新进程（包括完整的虚拟内存空间、CPU 寄存器、内核数据结构如文件描述符等等）、进程间上下文切换的时间开销非常大。并且父进程需要通过 <code>wait/waitpid</code> 来回收子进程资源。更重要的是，内存空间资源也不一定足够，这在大量快速并发的场景并不切实际。</p><p>如果服务端新开一个线程来处理客户端连接，性能和其他资源紧张情况应该好于多进程实现。同一进程的线程间会共享文件描述符表、页表、所在进程的所有信息、全部的用户态空间等等，因此同进程间线程上下文开销大大减小。</p><p>为了应对线程频繁创建和销毁的情况，我们还可以通过维护线程池来缓解这个情况。</p><p>但本质上，过多的进程 / 线程最终会把压力交给操作系统。OS 想要同时管理、调度上万个进程/线程，势必会导致 OS 不堪重负（考虑调度）。</p><p>在这种场景下我们就需要使用 I/O 多路复用技术，让一个进程能够维护多个 socket。</p><h4 id="select-amp-poll"><a href="#select-amp-poll" class="headerlink" title="select &amp; poll"></a>select &amp; poll</h4><p>我们最先了解，也是最简单的 I/O 多路复用是 <code>select</code> 方法：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/select.h&gt;</span></span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 该函数会阻塞监听指定的 readfds 列表中所有文件，直至列表中任一个文件能够被 read（不是 EOF）</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param[in] maxfd 最大监听的数量（`readfds` 有几个 bit 位）</span></span><br><span class="line"><span class="comment"> * @param[in/out] readfds 所有要监听的 file descriptor 列表（每个 bit 代表对对应的 fd 进行监听），最大默认 `FD_SET_SIZE` bits；</span></span><br><span class="line"><span class="comment"> *      也是结束监听时能读 fd 的返回值。</span></span><br><span class="line"><span class="comment"> * 只需要了解前两个参数，后面 3 个配置参数一般填 NULL</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">select</span><span class="params">(<span class="type">int</span> nfds, fd_set *_Nullable <span class="keyword">restrict</span> readfds,</span></span><br><span class="line"><span class="params">          fd_set *_Nullable <span class="keyword">restrict</span> writefds,</span></span><br><span class="line"><span class="params">          fd_set *_Nullable <span class="keyword">restrict</span> exceptfds,</span></span><br><span class="line"><span class="params">          <span class="keyword">struct</span> timeval *_Nullable <span class="keyword">restrict</span> timeout)</span>;</span><br><span class="line"><span class="comment">/* clear all bits in fdset. */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">FD_ZERO</span><span class="params">(fd_set *fdset)</span>;</span><br><span class="line"><span class="comment">/* clear bit fd in fdset */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">FD_CLR</span><span class="params">(<span class="type">int</span> fd, fd_set *fdset)</span>;</span><br><span class="line"><span class="comment">/* turn on bit fd in fdset */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">FD_SET</span><span class="params">(<span class="type">int</span> fd, fd_set *fdset)</span>;</span><br><span class="line"><span class="comment">/* Is bit fd in fdset on? */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">FD_ISSET</span><span class="params">(<span class="type">int</span> fd, *fdset)</span>;</span><br></pre></td></tr></table></figure><p>处理已连接 socket 的线程，将已连接的并且感兴趣的 socket 放到文件描述符集合（FD set，也就是上面的 bitmap）中，然后调用 <code>select</code> 函数将文件描述符集合<strong><u>复制</u></strong>到到内核里，让内核来检查是否有网络事件产生。</p><p>内核检查的方式很 naive，就是遍历这个文件描述符集合，当内核发现有网络事件发生后（例如客户端回复），在将对应的 socket 改为可读/可写，把更新状态的文件描述符表再次复制回用户态，用户态再通过遍历方式找到可读/可写的 socket 再进行对应操作。</p><p>我们发现 <code>select</code> 有几个问题：</p><ul><li>整个过程比较低效（<strong><u>两次遍历、两次复制</u></strong>），涉及多次 kernel 和 user 间的 memory copy 以及上下文切换；</li><li>并且访问文件描述符表的时间复杂度是线性的（$O(n)$）；</li><li>由于使用固定大小的 bitmap，受到内核中的 <code>FD_SETSIZE</code> 限制， 默认最大值为 1024，只能监听 0~1023 的文件描述符。</li></ul><p>那么 <code>poll</code> 函数呢？</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;poll.h&gt;</span></span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span> &#123;</span></span><br><span class="line">    <span class="type">int</span>   fd;         <span class="comment">/* file descriptor */</span></span><br><span class="line">    <span class="type">short</span> events;     <span class="comment">/* requested events */</span></span><br><span class="line">    <span class="type">short</span> revents;    <span class="comment">/* returned events */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="type">int</span> <span class="title function_">poll</span><span class="params">(<span class="keyword">struct</span> pollfd *fds, <span class="type">nfds_t</span> nfds, <span class="type">int</span> timeout)</span>;</span><br></pre></td></tr></table></figure><p>同样是在 <code>fds</code> 中任意一个文件描述符准备完成 / 超时 / 信号打断。只不过 <code>poll</code> 支持精确到事件类别的控制（<code>events/revents</code>）。</p><p>它和 <code>select</code> 一样访问模式类似，但是不一样的是，<code>poll</code> 不再用 bitmap 来存储所关注的文件描述符，取而代之<strong><u>用动态数组（以链表形式）来组织</u></strong>，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。不过仍然是线性访问时间、低效的检查过程。</p><p>因此在高并发的情况 <code>select</code> 和 <code>poll</code> 的性能还是不足够的。</p><h4 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h4><p>这个系统调用是 Unix 专属的，一般情况下它的使用涉及接口：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> &#123;</span></span><br><span class="line">   <span class="type">uint32_t</span>      events;  <span class="comment">/* Epoll events */</span></span><br><span class="line">   <span class="type">epoll_data_t</span>  data;    <span class="comment">/* User data variable */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">union</span> <span class="title">epoll_data</span> &#123;</span></span><br><span class="line">   <span class="type">void</span>     *ptr;</span><br><span class="line">   <span class="type">int</span>       fd;</span><br><span class="line">   <span class="type">uint32_t</span>  u32;</span><br><span class="line">   <span class="type">uint64_t</span>  u64;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">union</span> <span class="title">epoll_data</span>  <span class="title">epoll_data_t</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个新的 epoll 实例</span></span><br><span class="line"><span class="comment"> * @param[in] size 原本用作给内核一个分配数据结构大小的提示。现在已不需要，主要是保持兼容性</span></span><br><span class="line"><span class="comment"> * @return 属于新的 epoll 实例的文件描述符。是接下来对 epoll 接口操作指代该 epoll 的符号（epfd）</span></span><br><span class="line"><span class="comment"> * @warning 所有 epoll_create 返回的 epfd 都需要手动回收（close()）！</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">epoll_create</span><span class="params">(<span class="type">int</span> size)</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * epoll 实例的感兴趣的 socket fd 列表维护在 Kernel 中。用户态需要这个控制函数来增添/修改/删除对指定 socket 文件的监听。</span></span><br><span class="line"><span class="comment"> * @param[in] epfd 当前 epoll 实例对应的文件描述符</span></span><br><span class="line"><span class="comment"> * @param[in] op 可选操作：EPOLL_CTL_ADD / EPOLL_CTL_MOD / EPOLL_CTL_DEL</span></span><br><span class="line"><span class="comment"> * @param[in] fd 需要被操作的 socket 文件描述符</span></span><br><span class="line"><span class="comment"> * @param[in] event.events 可选事件：EPOLLIN (readable) / EPOLLOUT (writable) / EPOLLRDHUP (peer close conn)</span></span><br><span class="line"><span class="comment"> *                                     / EPOLLET (边缘触发。不指定则默认水平触发)</span></span><br><span class="line"><span class="comment"> * @return 0 if sucess (otherwise -1 + set errno)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">epoll_ctl</span><span class="params">(<span class="type">int</span> epfd, <span class="type">int</span> op, <span class="type">int</span> fd,</span></span><br><span class="line"><span class="params">             <span class="keyword">struct</span> epoll_event *_Nullable event)</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 等待 epoll 实例中指定发生事件类型的可用文件描述符</span></span><br><span class="line"><span class="comment"> * @param[out] events 返回当前事件信息和对应的文件描述符列表</span></span><br><span class="line"><span class="comment"> * @param[in] maxevents 传入 events buffer 能盛放的最大 epoll_event 结构体的个数</span></span><br><span class="line"><span class="comment"> * @return 返回 ready 的文件描述符数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">epoll_wait</span><span class="params">(<span class="type">int</span> epfd, <span class="keyword">struct</span> epoll_event events[.maxevents],</span></span><br><span class="line"><span class="params">              <span class="type">int</span> maxevents, <span class="type">int</span> timeout)</span>;</span><br></pre></td></tr></table></figure><p>一般使用方法如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> s = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line">bind(s, ...);</span><br><span class="line">listen(s, ...);</span><br><span class="line"><span class="type">int</span> epfd = epoll_create(...);</span><br><span class="line"><span class="comment">// 将所有需要监听的 socket 添加到 epfd 中</span></span><br><span class="line">epoll_ctl(epfd, ...);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="type">int</span> n = epoll_wait(...);</span><br><span class="line">    <span class="keyword">for</span> (接收到数据的 socket) &#123;</span><br><span class="line">        <span class="comment">// do something</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>epoll 相较于 select 和 poll 有重要的优势：</p><ul><li><p>epoll 在内核中使用<strong><u>红黑树</u></strong>来跟踪进程所有已注册（通过 <code>epoll_ctl</code>）的文件描述字。</p><ul><li>少两次文件描述符 copy，减少内存分配：不需要整体对文件描述符表进行复制（放在内核管理）；</li><li>管理性能增强：增删改复杂度 $O(\log n)$，一般<strong><u>不需要查找</u></strong>、取出平均复杂度可以到达常数时间（因为事件驱动）！</li></ul></li><li><p>使用<strong><u>事件驱动</u></strong>机制：</p><ul><li><p>在内核中维护链表记录就绪事件。有网络事件就把 ready sockets 放到 kernel space 的链表中（因此是常数时间的）；</p></li><li><p>调用 <code>epoll_wait</code> 时，如果链表非空，直接复制给 user space 提供的 buffer（抱歉，没法使用共享内存，还是需要 copy）；</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// epoll wait 部分源码</span></span><br><span class="line"><span class="keyword">if</span> (revents) &#123;</span><br><span class="line">    <span class="keyword">if</span> (__put_user(revents, &amp;uevent-&gt;events)</span><br><span class="line">       || __put_user(epi-&gt;event.data, &amp;uevent-&gt;data)) &#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>这大大增强了 <code>epoll</code> API 的并发能力。</p><h4 id="epoll-的边缘触发和水平触发（ET-amp-LT）"><a href="#epoll-的边缘触发和水平触发（ET-amp-LT）" class="headerlink" title="epoll 的边缘触发和水平触发（ET &amp; LT）"></a>epoll 的边缘触发和水平触发（ET &amp; LT）</h4><p>在学习数字电子电路的时候老师一定和你说过，某些电子元件的触发方式，其中就讨论过边缘触发和水平触发。</p><ul><li>水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件通知用户（例如保持某个全局 flag 一直有效）；</li><li>边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。</li></ul><p><code>epoll_ctl</code> 可以默认使用水平触发，向 <code>event.events</code> 追加 <code>EPOLLET</code> 则表示使用边缘触发。在 epoll 中，考虑一个场景：一个文件描述符上有数据可读（EPOLLIN 触发），线程开始处理数据，而在处理过程中又有新数据加入。那么：</p><ul><li><p>如果是边缘触发：<strong>当旧数据开始处理时，文件描述符仍然保持在就绪状态。但当有新的数据写入时，文件描述符会从就绪状态变为未就绪状态，然后再次变为就绪状态，触发一次新的 EPOLLIN 事件</strong>；</p><p>这种模式下我们应该：使用循环 <code>read</code> 这个 fd 中的内容直至这个 read 返回错误 <code>(errno == EAGAIN) || (errno == EWOULDBLOCK)</code>。</p><blockquote><p>这样可以确保即使在旧数据处理过程中有新的数据写入，应用程序也能及时地得到通知，并读取新的数据。</p><p>考虑一个问题，多线程场景下，使用边缘触发可能有问题：因为存在唤醒多个线程的问题。如果不希望多个线程同时操作 socket，就应该使用 <code>EPOLLONESHOT</code>，表示 one-shot，即特定的 socket fd 事件只会触发一次，然后立即移除。如果获得消息的线程以后还想接收这个 socket 的事件，需要使用 <code>epoll_ctl</code> 的 <code>EPOLL_CTL_MOD</code> 重新注册。</p></blockquote><p>如果使用边缘触发，则不能使用阻塞 I/O，并且一个信号必须读到不能再读为止（<code>EAGAIN/EWOULDBLOCK</code>），因为：</p><p>如果没有读完所有内容，则会导致下次调用 <code>epoll_wait</code> 时不会再收到之前消息的通知，通知信息会丢失！</p><p>如果使用了阻塞 I/O，那么在没有通知的情况下会永远等待下去！</p></li><li><p>如果是水平触发：当某个文件描述符上有数据可读，应用程序可以不立即处理完毕该事件。这样，因为当程序下一次调用 <code>epoll_wait</code> 时，<code>epoll_wait</code> 还会向应用程序通知此事件，直到事件被处理完毕。即：<strong>如果文件描述符上有数据可读，它的状态码会一直保持就绪状态，直到所有的数据都被读取完毕才会变为未就绪</strong>；</p><p>这种模式性能会略差于边缘触发。</p></li></ul><h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>尝试一下，用 C 写一个简单的 epoll 驱动的 server：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;netinet/in.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX_EVENTS 64</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PORT 8888</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> BUFFER_SIZE 1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置文件描述符为非阻塞模式</span></span><br><span class="line"><span class="type">static</span> <span class="type">void</span> <span class="title function_">set_nonblocking</span><span class="params">(<span class="type">int</span> fd)</span> &#123;</span><br><span class="line">    <span class="type">int</span> flags = fcntl(fd, F_GETFL, <span class="number">0</span>);</span><br><span class="line">    fcntl(fd, F_SETFL, flags | O_NONBLOCK);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="type">void</span> <span class="title function_">die</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* msg)</span> &#123;</span><br><span class="line">    perror(msg);</span><br><span class="line">    <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="type">size_t</span> <span class="title function_">mread</span><span class="params">(<span class="type">int</span> client_fd, <span class="type">char</span> *buf, <span class="type">size_t</span> n)</span> &#123;</span><br><span class="line">    <span class="type">size_t</span> total_read = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="type">ssize_t</span> count = recv(client_fd, buf, n, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (count == <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (errno == EAGAIN || errno == EWOULDBLOCK) &#123;</span><br><span class="line">                <span class="comment">// 数据已读完</span></span><br><span class="line">                <span class="keyword">if</span> (total_read &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="built_in">printf</span>(<span class="string">&quot;Received %zd bytes from fd %d: %.*s\n&quot;</span>,</span><br><span class="line">                           total_read, client_fd, (<span class="type">int</span>)total_read, buf);</span><br><span class="line">                    send(client_fd, <span class="string">&quot;fsck\n&quot;</span>, <span class="number">5</span>, <span class="number">0</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                perror(<span class="string">&quot;recv&quot;</span>);</span><br><span class="line">                close(client_fd);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Connection closed by client: fd %d\n&quot;</span>, client_fd);</span><br><span class="line">            close(client_fd);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        total_read += count;</span><br><span class="line">        <span class="built_in">memcpy</span>(bufferi, buf, count);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> total_read;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> listen_sock = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (listen_sock == <span class="number">-1</span>) die(<span class="string">&quot;socket&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置地址复用</span></span><br><span class="line">    <span class="type">int</span> optval = <span class="number">1</span>;</span><br><span class="line">    setsockopt(listen_sock, SOL_SOCKET, SO_REUSEADDR, &amp;optval, <span class="keyword">sizeof</span>(optval));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 绑定地址</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span> =</span> &#123;</span><br><span class="line">        .sin_family = AF_INET,</span><br><span class="line">        .sin_port = htons(PORT),</span><br><span class="line">        .sin_addr.s_addr = INADDR_ANY</span><br><span class="line">    &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (bind(listen_sock, (<span class="keyword">struct</span> sockaddr*)&amp;addr, <span class="keyword">sizeof</span>(addr))) die(<span class="string">&quot;bind&quot;</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 设置为非阻塞模式</span></span><br><span class="line">    set_nonblocking(listen_sock);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (listen(listen_sock, SOMAXCONN)) die(<span class="string">&quot;listen&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> epoll_fd = epoll_create1(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (epoll_fd == <span class="number">-1</span>) die(<span class="string">&quot;epoll_create1&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">event</span> =</span> &#123;</span><br><span class="line">        .events = EPOLLIN | EPOLLET,  <span class="comment">// 边缘触发模式</span></span><br><span class="line">        .data.fd = listen_sock</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">if</span> (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_sock, &amp;event)) die(<span class="string">&quot;epoll_ctl&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 事件循环</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">events</span>[<span class="title">MAX_EVENTS</span>];</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Server started on port %d\n&quot;</span>, PORT);</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="type">int</span> n = epoll_wait(epoll_fd, events, MAX_EVENTS, <span class="number">-1</span>);</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">-1</span>) die(<span class="string">&quot;epoll_wait&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="comment">// 处理新连接</span></span><br><span class="line">            <span class="keyword">if</span> (events[i].data.fd == listen_sock) &#123;</span><br><span class="line">                <span class="keyword">while</span> (<span class="number">1</span>) &#123;  <span class="comment">// 必须循环accept直到EAGAIN</span></span><br><span class="line">                    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">client_addr</span>;</span></span><br><span class="line">                    <span class="type">socklen_t</span> addrlen = <span class="keyword">sizeof</span>(client_addr);</span><br><span class="line">                    <span class="type">int</span> client_fd = accept(listen_sock,</span><br><span class="line">                                        (<span class="keyword">struct</span> sockaddr*)&amp;client_addr,</span><br><span class="line">                                        &amp;addrlen);</span><br><span class="line">                    <span class="keyword">if</span> (client_fd == <span class="number">-1</span>) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (errno == EAGAIN || errno == EWOULDBLOCK) &#123;</span><br><span class="line">                            <span class="keyword">break</span>; <span class="comment">// 已接受所有连接</span></span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            die(<span class="string">&quot;accept&quot;</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="built_in">printf</span>(<span class="string">&quot;New connection: fd %d\n&quot;</span>, client_fd);</span><br><span class="line">                    set_nonblocking(client_fd);  <span class="comment">// 必须设置为非阻塞</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 注册客户端socket到epoll（边缘触发）</span></span><br><span class="line">                    event.events = EPOLLIN | EPOLLET | EPOLLRDHUP;</span><br><span class="line">                    event.data.fd = client_fd;</span><br><span class="line">                    <span class="keyword">if</span> (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &amp;event)) &#123;</span><br><span class="line">                        close(client_fd);</span><br><span class="line">                        die(<span class="string">&quot;epoll_ctl client&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 处理客户端事件</span></span><br><span class="line">                <span class="type">int</span> client_fd = events[i].data.fd;</span><br><span class="line">                </span><br><span class="line">                <span class="comment">// 处理连接关闭或错误</span></span><br><span class="line">                <span class="keyword">if</span> (events[i].events &amp; (EPOLLERR | EPOLLHUP | EPOLLRDHUP)) &#123;</span><br><span class="line">                    <span class="built_in">printf</span>(<span class="string">&quot;Connection closed: fd %d\n&quot;</span>, client_fd);</span><br><span class="line">                    close(client_fd);</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 处理可读事件</span></span><br><span class="line">                <span class="keyword">if</span> (events[i].events &amp; EPOLLIN) &#123;</span><br><span class="line">                    <span class="type">char</span> buf[BUFFER_SIZE];</span><br><span class="line">                    <span class="type">ssize_t</span> total_read = <span class="number">0</span>;</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment">// 必须循环读取直到EAGAIN</span></span><br><span class="line">                    mread(client_fd, buf, BUFFER_SIZE);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    close(listen_sock);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以及一个向指定 Server 发送指定二进制数据的 Go 程序：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;bufio&quot;</span></span><br><span class="line">    <span class="string">&quot;bytes&quot;</span></span><br><span class="line">    <span class="string">&quot;encoding/hex&quot;</span></span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;net&quot;</span></span><br><span class="line">    <span class="string">&quot;os&quot;</span></span><br><span class="line">    <span class="string">&quot;os/signal&quot;</span></span><br><span class="line">    <span class="string">&quot;strings&quot;</span></span><br><span class="line">    <span class="string">&quot;syscall&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(os.Args) &lt; <span class="number">3</span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">&quot;Usage: go run main.go &lt;host&gt; &lt;port&gt;&quot;</span>)</span><br><span class="line">        os.Exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    host := os.Args[<span class="number">1</span>]</span><br><span class="line">    port := os.Args[<span class="number">2</span>]</span><br><span class="line">    address := net.JoinHostPort(host, port)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Connect to the server</span></span><br><span class="line">    conn, err := net.Dial(<span class="string">&quot;tcp&quot;</span>, address)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        fmt.Printf(<span class="string">&quot;Error connecting to server: %v\n&quot;</span>, err)</span><br><span class="line">        os.Exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> conn.Close()</span><br><span class="line"></span><br><span class="line">    fmt.Printf(<span class="string">&quot;Connected to %s\n&quot;</span>, address)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Channel to handle graceful shutdown</span></span><br><span class="line">    exitChan := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">    signal.Notify(exitChan, os.Interrupt, syscall.SIGTERM)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Goroutine to listen for server messages</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        reader := bufio.NewReader(conn)</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            message, err := reader.ReadString(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">                fmt.Println(<span class="string">&quot;Connection closed by server.&quot;</span>)</span><br><span class="line">                exitChan &lt;- syscall.SIGTERM</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            &#125;</span><br><span class="line">            fmt.Printf(<span class="string">&quot;Server: %s&quot;</span>, message)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Goroutine to handle user input and send data to server</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        scanner := bufio.NewScanner(os.Stdin)</span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            fmt.Print(<span class="string">&quot;Enter data to send (\\x?? for hex bytes): &quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> scanner.Scan() &#123;</span><br><span class="line">                input := scanner.Text()</span><br><span class="line">                data, err := parseInput(input)</span><br><span class="line">                <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">                    fmt.Printf(<span class="string">&quot;Invalid input: %v\n&quot;</span>, err)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                &#125;</span><br><span class="line">                _, err = conn.Write(data)</span><br><span class="line">                <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">                    fmt.Printf(<span class="string">&quot;Error sending data: %v\n&quot;</span>, err)</span><br><span class="line">                    exitChan &lt;- syscall.SIGTERM</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                fmt.Println(<span class="string">&quot;Input closed.&quot;</span>)</span><br><span class="line">                exitChan &lt;- syscall.SIGTERM</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Wait for interrupt signal</span></span><br><span class="line">    &lt;-exitChan</span><br><span class="line">    fmt.Println(<span class="string">&quot;Exiting program.&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// parseInput converts user input with \x?? hex sequences into a byte slice</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">parseInput</span><span class="params">(input <span class="type">string</span>)</span></span> ([]<span class="type">byte</span>, <span class="type">error</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> buffer bytes.Buffer</span><br><span class="line"></span><br><span class="line">    parts := strings.Split(input, <span class="string">&quot;\\x&quot;</span>)</span><br><span class="line">    buffer.WriteString(parts[<span class="number">0</span>]) <span class="comment">// Add any text before the first \x</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">1</span>; i &lt; <span class="built_in">len</span>(parts); i++ &#123;</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts[i]) &lt; <span class="number">2</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;incomplete hex byte: \\x%s&quot;</span>, parts[i])</span><br><span class="line">        &#125;</span><br><span class="line">        hexByte := parts[i][:<span class="number">2</span>]</span><br><span class="line">        rest := parts[i][<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line">        b, err := hex.DecodeString(hexByte)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;invalid hex byte: \\x%s&quot;</span>, hexByte)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        buffer.Write(b)</span><br><span class="line">        buffer.WriteString(rest) <span class="comment">// Add any remaining text after the hex byte</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> buffer.Bytes(), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近总结了一些 OS I/O 多路复用的知识。之前对 I/O Multiplexer 的认知还停留在 &lt;code&gt;select&lt;/code&gt; 系统调用，现在是时候扩展一下视野了。&lt;/p&gt;
&lt;h3 id=&quot;1-从-Socket-模型开始&quot;&gt;&lt;a href=&quot;#1-从-Sock</summary>
      
    
    
    
    <category term="review" scheme="https://blog.sjtuxhw.top/categories/review/"/>
    
    
    <category term="OS" scheme="https://blog.sjtuxhw.top/tags/OS/"/>
    
    <category term="IO" scheme="https://blog.sjtuxhw.top/tags/IO/"/>
    
  </entry>
  
  <entry>
    <title>具身智能论文速读3篇 2025年3月</title>
    <link href="https://blog.sjtuxhw.top/technical/embodied-3-papers-202503/"/>
    <id>https://blog.sjtuxhw.top/technical/embodied-3-papers-202503/</id>
    <published>2025-03-02T15:36:58.000Z</published>
    <updated>2025-07-27T10:29:00.494Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HumanUP-Learning-Getting-Up-Policies-for-Real-World-Humanoid-Robots"><a href="#HumanUP-Learning-Getting-Up-Policies-for-Real-World-Humanoid-Robots" class="headerlink" title="HumanUP: Learning Getting-Up Policies for Real-World Humanoid Robots"></a>HumanUP: Learning Getting-Up Policies for Real-World Humanoid Robots</h2><p>主旨：如何通过强化学习（RL）和仿真到现实（Sim-to-Real）的方法，为人形机器人开发能够从不同跌倒姿势和不同地形中自主起身的控制策略；</p><p>背景：人形机器人在实际应用中容易跌倒，而手动设计控制器来处理各种跌倒姿势和复杂地形非常困难。现有的控制器通常只能处理有限的跌倒情况，缺乏泛化能力。因此，论文提出了一种基于学习的框架，通过仿真训练生成能够在真实世界中应对多种跌倒姿势和地形的起身策略。目前的挑战有：</p><ul><li><strong>非周期性行为</strong>：起身任务不像行走那样有固定的周期性接触模式，接触序列需要动态调整。</li><li><strong>丰富的接触</strong>：起身过程中，机器人不仅依靠脚部接触地面，还可能利用身体其他部位（如手臂、躯干）来施加力。</li><li><strong>稀疏奖励</strong>：起身任务的奖励信号较为稀疏，机器人需要在长时间内做出正确的动作才能获得奖励。</li></ul><p><img src="imgs/HumanUP.png" /></p><p>论文的解决方案 <strong>HumanUP</strong>：</p><ul><li><p><strong>第一阶段（Stage I）</strong>：在仿真中发现一个有效的起身轨迹，不考虑动作的平滑性或速度/扭矩限制。这一阶段的目标是找到能够完成任务的轨迹，即使动作可能不够平滑或安全。</p><blockquote><p>under minimal constraints on smoothness or speed / torque limits</p></blockquote></li><li><p><strong>第二阶段（Stage II）</strong>：在第一阶段发现的轨迹基础上，训练一个 deployable 的策略，确保动作平滑、速度适中，并且能够适应不同的初始姿势和地形。</p><blockquote><p>Stage II is optimized to track the state trajectory discovered in the first stage to tackle easier motion tracking with dense tracking rewards, which is under strict Sim2Real control regularization for ensuring Sim2Real transfer. From Stage I to Stage II, we employ a Sim2Real learning curriculum that progresses from simplified → full collision mesh, canonical → random initial lying posture, and weak to strong control regularization and domain randomization. This two-stage approach integrates a hard-to-easy task-solving curriculum with an easy-to-hard Sim2Real curriculum, both of which are crucial for successful learning, as demonstrated in our experiments.</p><p>第二阶段的优化目的是跟踪第一阶段发现的状态轨迹，以密集的跟踪奖励解决更容易的运动跟踪问题，这是在严格的 Sim2Real 控制正则化下进行的，以确保 Sim to Real 的效果。从第一阶段到第二阶段，我们采用 Sim2Real Learning，从简化→全碰撞网格、规范→随机初始卧姿、弱控制正则化和域随机化到强控制正则化。正如我们的实验所证明的那样，这种两阶段方法将由难变易的任务解决方案与由易变难的 Sim2Real 方案整合在一起，两者对于成功学习都至关重要。</p></blockquote></li></ul><p>实验操作：</p><ul><li>仿真和真实世界都使用 Unitree G1 平台；机器人自由度等信息请参见论文的 Page 5；</li><li>Isaac Gym for simulated training and evaluation.</li><li>从 task success、smoothness、safety 几个方面打分、构造强化学习的奖励函数；</li></ul><p>实验结果：</p><ul><li><strong>仿真实验</strong>：HumanUP 能够在多种地形和初始姿势下成功完成起身任务，表现优于其他 baseline 方法；</li><li><strong>真实世界实验</strong>：HumanUP 能够在多种复杂地形（如草地、雪地、斜坡等）上成功起身，且成功率显著高于 G1 机器人自带的控制器。</li></ul><p>不足：</p><ul><li>It relies on high performance physics platforms like IsaacGym to simulate contact-rich tasks such as getting up and rolling over. 当前的仿真平台在接触动力学模拟上仍有不足，未来需要更精确的仿真工具；</li><li>学习到的动作可能不够拟人化，未来可以通过引入人类动作捕捉数据来改进；</li></ul><h2 id="DexTrack-Towards-Generalizable-Neural-Tracking-Control-for-Dexterous-Manipulation-from-Human-References"><a href="#DexTrack-Towards-Generalizable-Neural-Tracking-Control-for-Dexterous-Manipulation-from-Human-References" class="headerlink" title="DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References"></a>DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References</h2><p>主要目标是开发一种通用的神经跟踪控制器（neural tracking controller），用于灵巧手从人类参考中学习并进行复杂的物体操作。</p><p>背景目前的问题：当前的强化学习（RL）和轨迹优化（TO）方法通常依赖于任务特定的奖励或精确的系统模型，限制了其通用性和适应性；</p><blockquote><p>他们大多需要对单独的任务进行针对性的设计，例如专门对某一种特定的任务设计对应的奖励函数，之后根据这样的奖励函数训练策略网络来解决对应的问题；</p></blockquote><p>数据采集：两个公开的人类-物体交互数据集（GRAB 和 TACO）上进行实验，分别包含日常操作和功能性工具使用操作。</p><blockquote><p>仿真：use the Allegro hand, with URDF adapted from Isaac Gym Envs</p><p>真实世界：LEAP hand</p></blockquote><p>训练怎么表示任务？</p><ul><li>比如将物体转动一个角度，我们可以先规划出来物体的运动轨迹，之后将这个任务转化为跟踪物体运动轨迹的轨迹跟踪任务。</li><li>在每个时刻，给定机器手和物体当前的状态，以及下一步想要达到的状态，轨迹跟踪控制器的任务是给出机器手当前应该执行的动作，从而通过执行该动作，机器手可以运动且和物体进行交互，使得机器手以及物体实际达到的状态与下一步想要达到的状态吻合。</li></ul><p><img src="imgs/DexTrack.png" /></p><p>训练的指标：包括物体旋转误差、物体平移误差、手腕位置和旋转误差、手指位置误差以及成功率；</p><p>训练 controller：强化学习 + 模仿学习。</p><ul><li><p>RL 用于处理复杂的动态环境；</p></li><li><p>IL 则通过模仿高质量的机器人跟踪演示来提高控制器的性能（to distill successful, abundant, and diverse “tracking knowledge” into the tracking controller）。</p><blockquote><p>在 RL 训练的同时引入监督信号来降低 policy 学习的难度。通过交替地使用高质量的轨迹跟踪数据辅助通用轨迹跟踪控制器的学习，以及借助通用轨迹跟踪器来提高单一轨迹跟踪演示的质量；</p></blockquote></li></ul><p>优化算法（如何优化 “单一轨迹跟踪演示的质量”）：</p><ul><li>借助通用轨迹跟踪器（之前 train 的）来初始化单一轨迹跟踪策略的学习；</li><li>借助 homotopy optimization 的方式，通过解决一系列的优化任务来降低特定轨迹跟踪任务优化的难度；</li></ul><p>baseline：与现有的模型无关方法（如 DGrasp 和 PPO）进行比较；</p><p>结果：DexTrack 在模拟和现实世界中的表现均优于基线方法，成功率提高了 10% 以上。特别是在处理复杂操作、薄物体和动态接触时表现出色。</p><h2 id="DexGraspVLA-A-Vision-Language-Action-Framework-Towards-General-Dexterous-Grasping"><a href="#DexGraspVLA-A-Vision-Language-Action-Framework-Towards-General-Dexterous-Grasping" class="headerlink" title="DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping"></a>DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</h2><p>实现 <strong>通用灵巧抓取</strong>（General Dexterous Grasping）！</p><p>论文设计了一个 VLA 框架，结合了预训练的视觉-语言模型（VLM）作为高层任务规划器（Planner），以及基于扩散模型（Diffusion-based Policy）的低层动作控制器（Controller）。</p><p><strong><u>模仿学习 + pretrained VLMs</u></strong>！</p><ul><li><strong>高层规划器（Planner）</strong>：使用预训练的视觉-语言模型（如 Qwen-VL-Chat）来解析用户指令，规划抓取任务，并为低层控制器提供监督信号。规划器能够处理多模态输入，执行视觉定位（Visual Grounding），并根据用户指令生成抓取任务的分解。</li><li><strong>低层控制器（Controller）</strong>：基于扩散模型的动作控制器，负责生成闭环的动作序列。控制器通过分割模型（如 SAM 和 Cutie）获取目标物体的掩码（Mask），并使用预训练的视觉编码器（如 DINOv2）提取图像特征。这些特征与机器人本体感知状态（Proprioception）结合，通过扩散模型生成多步动作序列。</li></ul><p>数据采集：为了训练 DexGraspVLA 的低层控制器，研究团队手动收集了2094 个成功的抓取演示数据，涵盖了36 种家庭用品，涉及不同的尺寸、重量、几何形状、纹理和材料。每个演示记录了机器人手腕和头部摄像头的图像、本体感知状态、物体掩码以及动作序列。</p><blockquote><p>我好像没找到采集的具体方法？</p></blockquote><p><img src="imgs/DexGraspVLA.png" width="550px" /></p><p>训练：</p><ul><li>输入包括手腕摄像头图像、头部摄像头图像、机器人本体感知状态（如关节角度）以及目标物体的 Mask；</li><li>DINOv2 提取图像特征 + 机器人本体感知（7 个手臂关节角度 + 6 个手关节角度），输出的动作也是 13 元组；</li><li>最小化预测动作与真实动作之间的差异来训练模型（binary reward，数学模型可以看懂）；</li></ul><p>结果：</p><ul><li><strong>泛化能力</strong>强：DexGraspVLA 在数千种未见过的物体、光照和背景组合下的抓取成功率超过 90%，展示了其在“零样本”（Zero-Shot）环境中的强大泛化能力。</li><li>与不使用视觉编码器的基线相比，DexGraspVLA 在单物体抓取任务中的成功率显著更高，证明了其在领域不变特征上进行模仿学习的有效性；</li><li>Planner 在复杂环境下的目标物体边界框预测准确率超过 99%，展示了其在视觉定位任务中的可靠性。</li></ul><p>不足和展望：</p><ul><li>First, due to the time limit, our training dataset does not encompass very small objects or extremely cluttered environments; performance on these more challenging cases could improve with dedicated data collection.（训练数据集中未包含非常小的物体或极度杂乱的环境）；</li><li>Additionally, we have not yet explored functional grasping for subsequent object usage, which is a promising direction for future work.（未来的工作可以探索功能性抓取 (Functional Grasping) 以及更复杂的环境设置）；</li></ul><h2 id="News-amp-Papers"><a href="#News-amp-Papers" class="headerlink" title="News &amp; Papers"></a>News &amp; Papers</h2><ul><li>灵初智能Psi R0.5：两小时数据解锁全面泛化，具身智能再突破！<ul><li>(2025.3.3) 新闻：<a href="https://www.msn.cn/zh-cn/news/other/%E7%81%B5%E5%88%9D%E6%99%BA%E8%83%BDpsi-r0-5-%E4%B8%A4%E5%B0%8F%E6%97%B6%E6%95%B0%E6%8D%AE%E8%A7%A3%E9%94%81%E5%85%A8%E9%9D%A2%E6%B3%9B%E5%8C%96-%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%86%8D%E7%AA%81%E7%A0%B4/ar-AA1A8aMg?ocid=BingNewsSerp">MSN</a></li><li>Project &amp; Paper: <a href="https://github.com/Psi-Robot/DexGraspVLA">https://github.com/Psi-Robot/DexGraspVLA</a></li></ul></li><li>机器人安灯泡、切东西都能拿捏，可操控轨迹跟踪的DexTrack来了！<ul><li>(2025.3.2) 新闻：<a href="https://www.jiqizhixin.com/articles/2025-03-02-5">机器之心</a></li><li>[ICLR’25] Project &amp; Paper:  <a href="https://github.com/Meowuu7/DexTrack">https://github.com/Meowuu7/DexTrack</a></li></ul></li><li>“我”在 UIUC 学起立？当代博士生不想自己动手扶机器人，竟然训练了….<ul><li>(2025.2.18) 新闻：<a href="https://www.bilibili.com/video/BV1pJA8epExA">bilibili</a></li><li>Project &amp; Paper:  <a href="https://humanoid-getup.github.io/">https://humanoid-getup.github.io/</a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;HumanUP-Learning-Getting-Up-Policies-for-Real-World-Humanoid-Robots&quot;&gt;&lt;a href=&quot;#HumanUP-Learning-Getting-Up-Policies-for-Real-World-H</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="AI" scheme="https://blog.sjtuxhw.top/tags/AI/"/>
    
    <category term="Robot" scheme="https://blog.sjtuxhw.top/tags/Robot/"/>
    
    <category term="Paper" scheme="https://blog.sjtuxhw.top/tags/Paper/"/>
    
    <category term="IL" scheme="https://blog.sjtuxhw.top/tags/IL/"/>
    
  </entry>
  
  <entry>
    <title>如何理解 PyTorch 函数的 dim 参数</title>
    <link href="https://blog.sjtuxhw.top/technical/pytorch-dim/"/>
    <id>https://blog.sjtuxhw.top/technical/pytorch-dim/</id>
    <published>2025-02-18T12:17:05.000Z</published>
    <updated>2025-02-28T12:42:57.462Z</updated>
    
    <content type="html"><![CDATA[<p>之前很长的一段时间内，我都不太清楚如何感性地理解 PyTorch 中的 <code>dim</code> 参数。最近琢磨到了一个还算比较好理解的方法，故简单记录在这里。</p><p><code>dim</code> 在 PyTorch 的很多函数中都可以指定，例如 <code>sum / mode / unsqueeze / topk</code> 等等，主要是告诉函数应该针对张量的哪个特定维度操作。</p><p>这在输入张量维度很高的时候就不那么直观了。虽说不理解问题不大，最多手写循环就能达到目的。但如果我们想尽量避免使用 python 的显式循环，或者还想要利用广播机制来更快的完成计算任务，就不得不总结一下了。</p><ul><li><p><strong><u>聚合类函数</u></strong>（<strong><u>减小维度数的运算</u></strong>，reduction operations），例如 <code>sum / mean / max / min / mode / topk</code> 等等；</p><ul><li><p><code>dim</code> 通常的语义是 “沿这个维度进行消除”，如果有指定 <code>keepdim=True</code>，则这个维度 size 压缩为 1；</p></li><li><p><code>dim</code> 的值就对应张量 shape 的索引；</p></li><li><p>被操作的每个元素的 shape 就是 原张量的 shape 在 <code>dim</code> 索引之后组成的新的 shape，即 <code>shape[dim+1:]</code>；</p></li></ul><blockquote><p>例如对于 <code>a = torch.tensor([ [ [[1],[2],[3]], [[2],[3],[4]] ], [ [[3],[4],[5]], [[4],[5],[6]] ] ])</code>，它的 shape 是 <code>(2, 2, 3, 1)</code>；</p><p>tips. <strong><u>对于高维张量的形状，请从外向里读，这样更清楚一点</u></strong>；</p><p>那么 <code>sum(a, dim=2)</code> 的含义就是沿着 size 为 3 的维度（shape 索引是 2）相加，被求和的元素的 shape 就是原 shape 中索引为 2 向后的组成的。不难发现 size 为 3 的维度的元素是 shape 是 <code>(1,)</code> 的子元素，把子元素加起来就行，答案应该是 <code>[ [ [[6]], [[9]] ], [ [[12]], [[15]] ] ]</code>；</p><p>再来个简单的：<code>b = torch.tensor([[1,2,3], [4,5,6]])</code>，这个 <code>(2,3)</code> 的张量是不是就非常清楚了？我们计算 <code>sum(b, dim=0)</code> 就是对 size 为 2 的维度求和，也就是元素是 shape <code>(3,)</code> 的张量求和，答案显然是 <code>[[5,7,9]]</code>；</p></blockquote></li><li><p><strong><u>拼接类函数</u></strong>（<strong><u>不改变维度数的运算</u></strong>），例如 <code>cat</code> 等，<code>dim</code> 通常的语义是 “拼接的方向”；</p><ul><li>“拼接的方向” 是指，拼接后 size 有变化的维度；例如 <code>a = torch.zeros(2, 3); b = torch.zeros(2, 4)</code>，<code>torch.cat([a,b], dim=1)</code> 就是让行对齐、列增大的拼接方式；</li><li>拼接得到的张量的 shape 和原张量的 shape 的维度数相同，只是某个维度上的 size 有所不同；</li></ul></li><li><p><strong><u>扩展类函数</u></strong>（<strong><u>增加维度数的运算</u></strong>，expansion operations），例如 <code>unsqueeze / stack</code> 等，<code>dim</code> 通常的语义是在原张量的指定维度下添加 size 为 N ($N\ge1$) 的维度；</p><ul><li><p>对于 <code>unsqueeze</code>，没有向张量引入其他信息，只是在原张量 shape 索引为 <code>dim</code> 的位置插入 1 来扩展；</p></li><li><p>比较难以理解的是 <code>stack</code>，很多人会把 <code>stack</code> 和 <code>cat</code> 的作用搞混。但我们只需要搞清楚本质上 <code>stack</code> 是维度扩展类函数，而 <code>cat</code> 则是拼接类函数，就行了！<code>cat</code> 不改变张量的维度，只是将两个或以上张量在已有维度上拼接；而 <code>stack</code> 则是通过新增一个维度来连接两个张量。</p><p>像 <code>a = torch.zeros(2,3); b = torch.ones(2,3)</code>，如果执行 <code>torch.stack([a,b], dim=2)</code>，则是在原张量 shape 为 <code>(2,3)</code> 的情况下构造一个 shape 为 <code>(2,3,2)</code> 的新张量（在 shape 索引为 <code>dim=2</code> 的位置插入一个 size=2 的维度，分别装 <code>a</code> 和 <code>b</code>）。至于最终如何表示输出的张量，很简单，就是在第三个维度把两个张量排在一起，这么表示：<code>[[[0, 1], [0, 1], [0, 1]], [[0, 1], [0, 1], [0, 1]]]</code>，还好只有 3 维，我们可以感性地画出来：</p><p><img src="imgs/pth_stack.jpg" width="450px" /></p></li></ul></li></ul><p>综上，在维度很高、没法感性理解的时候，可以尝试列出输入的 shape（从外向内读），然后在你要执行的函数中指定 <code>dim</code>，按规则写出输出的 shape，你就能清楚这个操作究竟在做什么了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前很长的一段时间内，我都不太清楚如何感性地理解 PyTorch 中的 &lt;code&gt;dim&lt;/code&gt; 参数。最近琢磨到了一个还算比较好理解的方法，故简单记录在这里。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dim&lt;/code&gt; 在 PyTorch 的很多函数中都可以指定，例如 &lt;co</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="AI" scheme="https://blog.sjtuxhw.top/tags/AI/"/>
    
    <category term="ML" scheme="https://blog.sjtuxhw.top/tags/ML/"/>
    
    <category term="PyTorch" scheme="https://blog.sjtuxhw.top/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Java 进阶（三）：垃圾回收、并发、JDNI &amp; SPI</title>
    <link href="https://blog.sjtuxhw.top/technical/java-adv-3/"/>
    <id>https://blog.sjtuxhw.top/technical/java-adv-3/</id>
    <published>2025-01-06T05:44:06.000Z</published>
    <updated>2025-07-30T13:12:54.728Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Chapter-7-Java-Concurrent"><a href="#Chapter-7-Java-Concurrent" class="headerlink" title="Chapter 7. Java Concurrent"></a>Chapter 7. Java Concurrent</h1><h2 id="7-1-Usage"><a href="#7-1-Usage" class="headerlink" title="7.1 Usage"></a>7.1 Usage</h2><p>读者回忆一下在计算机系统课程中学习的关于 thread 和 process 的知识，最好能够在心中对比一下在 C/C++ 中使用线程和进程。</p><p>我们本节的目的是在 Java 中使用线程。两种方法：</p><ul><li>使用 <code>Runnable</code> Interface：<ol><li>重写 <code>public void run()</code> 方法；</li><li>将这个类的实例作为 <code>Thread</code>  类型的构造参数。构造完成后启动 <code>Thread#start()</code> 即可；</li></ol></li><li>继承于 <code>Thread</code> Class；<ol><li>重写 <code>public void run()</code> 方法；</li><li>直接启动：<code>Thread#start()</code>；</li></ol></li></ul><p>注意，我们需要特别处理 <code>InterruptedException</code>：</p><ul><li><p>Java 多线程程序中，我们应该总是考虑这种 exception。这意味着外部有人正在希望以一种优雅的方式结束当前线程（就是对当前线程对象 <code>Thread#interrupt()</code>），并且可能正在通过 <code>Thread#join()</code> 等待；</p></li><li><p>不应该在捕获这个异常的时候直接抛出另外一种异常（混淆原因），或者直接忽略（外部线程可能正在等待结束！）；</p></li><li><p>根据方法自身的含义，一般有两种解决方案：</p><ol><li>继续向上传播这个异常（当你的方法本身就是一个耗时操作 / 网络操作或者其他情况）；</li><li>捕获这个异常、设置当前线程被 interrupted 的 flag（方便 log 溯源）：<code>Thread.currentThread.interrupt()</code>，并且准备结束；</li></ol><p>然后处理当前类中需要回收 / 处理的资源。无论是哪一种方法，都需要遵循当前方法的语义：“调用它出现 <code>InterruptException</code> 这种情况是否合理？”</p></li></ul><h2 id="7-2-Synchronized-Methods"><a href="#7-2-Synchronized-Methods" class="headerlink" title="7.2 Synchronized Methods"></a>7.2 Synchronized Methods</h2><p>Java 线程中的设定和 C/C++ 是类似的，它也会共享线程间的资源，不过 Java 没有指针，只是通过引用共享的。因此会遇到和 C/C++ 一样的问题。</p><p>就以共享静态变量为例，多线程同时操作共享静态变量会导致未定义的行为（race condition）。</p><p>在 C/C++ 中，一般会通过设立临界区（信号量 semaphore）或互斥锁（mutex）来锁定共享变量，确保同一时间只有一个/指定数量的线程可以访问。</p><p>在 Java 中，提供了一种修饰方法的关键字 <code>synchronized</code>，其作用是：</p><ol><li><p>被该关键字修饰的方法，其所在的类型的任意一个对象，只能被一个线程调用被这个关键字修饰的方法。</p><p>也就是说，相当于在这个方法的类上设一个互斥锁（被称为 intrinsic lock 固有锁，或者 monitor lock），把这个类中所有被 <code>synchroized</code> 修饰的方法锁住；</p></li><li><p>当一个线程退出了一个对象的 synchronized 方法，则会与这个对象其他的 synchronized 方法建立一个 happens-before relationship，以确保对象被使用的状态能被所有线程知道；</p></li></ol><p>如果 <code>synchronized</code> 修饰在静态方法上，那么锁住的就是与 intrinsic lock 关联的 class 实例，而不是它的实例的实例。也就是对这个类中静态域的访问会被控制，需要与实例方法的 <code>synchronized</code> 区分开。</p><p>Java 甚至支持到 statement 细粒度的 <code>synchronized</code>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addName</span><span class="params">(String name)</span> &#123;</span><br><span class="line">    <span class="comment">// 这里保护实例属性的并发访问（this）</span></span><br><span class="line">    <span class="comment">// 如果需要保护静态成员，则需要将关键字定义在静态方法上</span></span><br><span class="line">    <span class="comment">// 或者括号内使用 Class 元类型的实例</span></span><br><span class="line">    <span class="keyword">synchronized</span>(<span class="built_in">this</span>) &#123;</span><br><span class="line">        lastName = name;</span><br><span class="line">        nameCount++; </span><br><span class="line">    &#125; </span><br><span class="line">    nameList.add(name); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7-3-Reentrant-Synchronization"><a href="#7-3-Reentrant-Synchronization" class="headerlink" title="7.3 Reentrant Synchronization"></a>7.3 Reentrant Synchronization</h2><p>Java 中提供了一类可重入锁，可以让获得锁的同一个线程多次访问临界资源：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意，如果需要保护类的静态成员，则应该将锁也定义为静态成员</span></span><br><span class="line"><span class="type">Lock</span> <span class="variable">lock</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ReentrantLock</span>();</span><br><span class="line"></span><br><span class="line">lock.lock();</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">//更新对象状态</span></span><br><span class="line">    <span class="comment">//捕获异常，并在必须时恢复不变性条件</span></span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">   e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">   lock.unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7-4-Atomic-Access-amp-Keyword-volatile"><a href="#7-4-Atomic-Access-amp-Keyword-volatile" class="headerlink" title="7.4 Atomic Access &amp; Keyword volatile"></a>7.4 Atomic Access &amp; Keyword <code>volatile</code></h2><p>Java 中原生的单步原子访问操作包含：</p><ul><li><p>针对引用变量的读写、大多数基本类型的读 <strong><u>或</u></strong> 写（除了 <code>long</code> / <code>double</code>）；</p></li><li><p>被 <code>volatile</code> 关键字修饰的所有变量的读 <strong><u>或</u></strong> 写（包括 <code>long</code> 和 <code>double</code>）；</p><blockquote><p><code>volatile</code> 的本质是，程序在访问一个被它修饰的变量后，会<strong><u>直接进入 main memory 读取，而不会使用寄存器 / 线程本地缓存</u></strong>；相当于告诉 JVM 这个变量可能会在当前线程的控制流以外的地方被更改。</p><p>它会确保当一个线程修改了一个变量时，其他线程能够立即看到这个修改。</p><p>底层是通过<u>禁止指令重排序和 memory barrier（如 x86-64 的 <code>fence</code> 族指令）</u> 等机制来实现的。</p></blockquote></li></ul><p>Java 的原子访问操作可以：</p><ul><li>保证多线程操作一个数据时值不会错误的改变（写操作字节码指令会一次性执行完），降低 memory inconsistency 的风险；</li><li>保证各个线程总是能读到关于这个值最新情况（读操作字节码指令会读到最新的情况并且一次性执行完）；</li></ul><p>那么，<strong><u>为什么 Java 既然有内置 <code>Atomic</code> Classes、锁、synchronous 关键字等等同步机制，为什么还需要 volatile 关键字</u></strong>？</p><p>问出这个问题就说明你将 “多线程原子操作”（也称 “同步”）和这里的 “单步原子访问操作” 的概念混淆了。</p><ul><li>单步原子操作是指，Java 中的一条基本字节码指令（例如读一次内存、写一次内存）会不会被 CPU（其他线程）从中间打断、打乱；</li><li>多线程原子操作通常讨论的是一条/一系列 Java 代码的执行（例如从获取内存中的共享变量值到自增写回的这个流程）会不会被 CPU（其他线程）从中间打断；</li></ul><p>我们发现，满足单步原子访问操作是满足多线程原子操作的<u>必要不充分条件</u>。因此底层在实现多线程同步机制时，一定已经实现了单步原子访问的操作（及时刷到内存中）。</p><p>单步的原子操作是<strong><u>由语言本身以及硬件指令的特征决定</u></strong>。在 Java 中，为了保证灵活性还向上提供了 <code>volatile</code> 关键字，相当于告诉 JVM 和 CPU，被它修饰的变量一是不能放在寄存器中/缓存下来，二是它需要满足单步原子操作（不能乱序执行）。</p><p><img src="imgs/volatile.png" width="350px" /></p><p>而多线程原子操作则需要开发者按照程序语义，利用同步机制手动指定代码片段的临界区，即同一时刻的访问控制。</p><p>总结一下，你只需要记住这些：</p><ul><li><code>volatile</code> 保证 Java 多线程对某个变量的读、写是及时的（不使用寄存器、不使用线程缓存、禁止指令乱序），一定能被下一条指令 / 其他线程感知到；</li><li>其他的同步操作，保证 Java 代码片段执行期间的临界特性（不会有另一个线程同时执行相同代码）；</li><li>为共享变量加锁（或者其他同步机制）之后，就不再需要 <code>volatile</code> 关键字了（后者是前者的必要不充分条件）；</li></ul><p>因此，只有在<u>没有多线程同步的需求</u>（<code>volatile</code> 不保证同一线程对变量的一系列操作是原子的），但是又要保证对某一个变量的读和写是准确、及时的时候，可以使用 <code>volatile</code> 关键字，例如状态标志、简单的布尔变量等，这样不需要加锁，规避了死锁以及性能问题。</p><blockquote><p>注意：C/C++ 中的 <code>volatile</code> 关键字的含义与 Java 有些差异。</p><p>它只是告诉 Compiler 不要优化被修饰的变量，并且把它放在内存中，每次读写都直接对内存操作。常用在嵌入式 / 绕过编译器进行内存映射等场景中。</p><p>这里没有单步原子操作的说法，因为 C/C++ 编译结束后直接就是机器码。</p></blockquote><h2 id="7-5-Dead-Lock-Starvation-Live-Lock"><a href="#7-5-Dead-Lock-Starvation-Live-Lock" class="headerlink" title="7.5 Dead Lock, Starvation, Live Lock"></a>7.5 Dead Lock, Starvation, Live Lock</h2><p>无论是死锁还是活锁，都是指多个线程之间因互相请求访问资源而导致程序无法继续执行的情况。</p><p>它们的不同点是：</p><p>对于死锁，它发生的情况是多个线程或进程在互相等待对方释放资源时，自己又不会主动释放自己占有的资源，导致程序永远无法继续的情况。</p><blockquote><p>例如，假设一个程序的两个线程 A 和 B，A 先获得了一个资源 X 并给它上锁，B 获得了另一个资源 Y 也给它上了锁。但是接下来 B 需要资源 X 才能继续、A 又需要 Y 才能继续。所以二者相互等待对方释放资源锁，造成了死锁；</p></blockquote><p>对于活锁，线程并不会阻塞在原地，而是反复地在释放资源和获取资源间横跳，这主要是因为程序有处理资源访问冲突的机制，但是两个存在活锁的线程相互处理访问冲突的时候又造成了访问冲突，也无法继续下去。</p><blockquote><p>例如一个程序的线程 A 和 B，假设 A 先获得了一个资源 X 并给它上锁，B 获得了另一个资源 Y 也给它上了锁。A 想要获取资源 Y 的时候发现 B 占用了，于是 A 主动释放了资源 X 给 B，自己去获取资源 Y；但是此时 B 也主动释放了 Y 资源，去获取 X 资源，双方只是调换了资源持有的顺序，仍然无法继续执行。</p></blockquote><p>线程饥饿是指，因为共享资源调度策略的问题，造成某些线程一直无法获得执行的机会而近乎停止执行，而另一些线程则一直占用共享资源不释放。</p><h2 id="7-6-Condition-Variable-in-Java-Guarded-Blocks"><a href="#7-6-Condition-Variable-in-Java-Guarded-Blocks" class="headerlink" title="7.6 Condition Variable in Java: Guarded Blocks"></a>7.6 Condition Variable in Java: Guarded Blocks</h2><p>在 Java 中，和 C/C++ 的 condition variable 对应的、在 blocks 前通过某些方式检查（例如轮询）一些条件再决定执行的，线程动作同步的术语被称为 guarded block。</p><p>Java 中被 <code>synchrounous</code> 修饰的方法可用 <code>Object#wait()</code> 和 <code>Object#notify*() / notifyAll()</code> 来实现与 condition variable 相似的效果。</p><ul><li><code>Object#wait()</code>：此时会设置等待的信息、放锁并且挂起当前线程（不是 spin lock）；</li><li><code>Object#notify / notifyAll()</code>：另一个线程可以通过访问当前对象的这两个方法来唤醒等待在 intrinsic lock 上的线程，把锁交给它们；</li></ul><p>我们可以利用 guarded blocks 模仿 condition variable 的做法实现生产者消费者模式。</p><h2 id="7-7-Immutable-Objects"><a href="#7-7-Immutable-Objects" class="headerlink" title="7.7 Immutable Objects"></a>7.7 Immutable Objects</h2><p>在很多实际情况下，不可变数据类型的好处：</p><ul><li><p>复制构造时，不是引用传递，因此是深拷贝。这样使用起来和基本类型一样方便，但是又不用担心改错源数据（非引用链接）；</p></li><li><p>确保数据在多线程情况下无需同步，线程安全！</p></li></ul><p>我们在 Java Bean &amp; Record 一节已经讨论过。不可变类的定义：一个类满足如下三个条件：</p><ul><li>类型中的每个数据域都是 <u>私有的、常量的</u>（<code>private</code>，<code>final</code>）；</li><li>每个数据域都只能通过 <code>getter</code> 方法获取，不能有任何 <code>setter</code> 方法，并且没有“返回值是指向可变数据域的引用”的 <code>getter</code> 方法；</li><li>必须存在公有构造函数，并且构造函数内初始化各个数据域（常量只能这么做）；</li><li>Object 基类继承函数 <code>equals</code> 返回 <code>true</code> 当且仅当类中的每个数据域都相等；</li><li>Object 基类继承函数 <code>hashCode</code> 在类中的每个数据域都相等时，一定返回一样的值；</li><li>Object 基类继承函数 <code>toString</code> 最好包含 类名 和 每个数据域的名称和值； </li></ul><p><strong>因此如果有一个类数据域都私有、没有修改器方法，但有一个方法：返回内部一个可变数据域的引用（例如数组），则这个类也是可变类</strong>；</p><h2 id="7-8-High-Level-Concurrency-Objects"><a href="#7-8-High-Level-Concurrency-Objects" class="headerlink" title="7.8 High Level Concurrency Objects"></a>7.8 High Level Concurrency Objects</h2><p>Java 中包装了一些高级并发对象：</p><h3 id="7-8-1-Lock-Objects"><a href="#7-8-1-Lock-Objects" class="headerlink" title="7.8.1 Lock Objects"></a>7.8.1 Lock Objects</h3><p>Lock Objects：对常见的并发场景提供了简单的保护；</p><blockquote><p>例如 <code>ReentrantLock</code>（可重入锁），</p><p>可以使用 <code>tryLock()</code> 获取锁、<code>unlock()</code> 释放锁。</p><p>和 Intrinsic Lock 机制很相似（包括持有规则、通过关联的 Condition 对象 notify/wait）相比更好的一点是 “允许 try”，也就是获取锁不成功的话还可以回到获取锁前的执行状态。</p></blockquote><h3 id="7-8-2-Executors"><a href="#7-8-2-Executors" class="headerlink" title="7.8.2 Executors"></a>7.8.2 Executors</h3><blockquote><p>创建一个新的线程可以通过继承 Thread 类或者实现 Runnable 接口来实现，这两种方式创建的线程在运行结束后会被虚拟机销毁，进行垃圾回收，如果线程数量过多，频繁的创建和销毁线程会浪费资源，降低效率。而线程池的引入就很好解决了上述问题，线程池可以更好的创建、维护、管理线程的生命周期，做到复用，提高资源的使用效率，也避免了开发人员滥用 new 关键字创建线程的不规范行为。</p><p>在实际生产中，一般企业内部会规定编码规范。例如 Aliyun 指出，线程资源必须通过线程池提供，不允许在应用中显式的创建线程。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。</p></blockquote><p>Executors：为启动、管理线程提供了更高级的 API，可以使用线程池机制为大规模并发应用提供支持；</p><p>将线程创建、管理的工作从应用业务逻辑中剥离。Java 中的 Executor 就是来包装这个的接口。</p><p>其中，有一些框架 / 库可以实现 Executor 接口。例如：</p><ul><li>Thread Pools：线程池，最常见的对于 Executor 的 implementation；</li><li>Fork/Join：一个利用多处理器资源的 Executor 实现框架。</li></ul><p><code>Executor</code> 接口只有一个：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">execute</span><span class="params">(java.lang.Runnable runnable)</span>;</span><br></pre></td></tr></table></figure><p>不需要自行创建 <code>Thread</code>，而是将 Runnable 类放到 Executor 中，让它帮你启动和管理。</p><p>类似地，还有 <code>ExecutorService</code> 接口，提供了比 <code>Executor</code> 更灵活的线程提交方式：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ExecutorService</span> <span class="keyword">extends</span> <span class="title class_">java</span>.util.concurrent.Executor, java.lang.AutoCloseable;</span><br></pre></td></tr></table></figure><p>类似 <code>Executor</code>，不过它不仅仅允许你提交 <code>Runnable</code> 对象，还允许使用 <code>Callable</code>，并使用 <code>Future&lt;T&gt;</code> 来异步获取返回值，可以通过返回的 <code>Future</code> 对象了解、管理 Runnable/Callable 的执行状态：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Future&lt;?&gt; submit(Runnable runnable);</span><br><span class="line">Future&lt;T&gt; <span class="title function_">submit</span><span class="params">(Runnable runnable, T t)</span>;</span><br><span class="line">Future&lt;T&gt; <span class="title function_">submit</span><span class="params">(Callable&lt;T&gt; callable)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 同时启动多个 callable 对象</span></span><br><span class="line">List&lt;Future&lt;T&gt;&gt; <span class="title function_">invokeAll</span><span class="params">(Collection&lt;? extends Callable&lt;T&gt;&gt; collection)</span> <span class="keyword">throws</span> InterruptedException;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 等待终止</span></span><br><span class="line"><span class="type">boolean</span> <span class="title function_">awaitTermination</span><span class="params">(<span class="type">long</span> l, TimeUnit timeUnit)</span> <span class="keyword">throws</span> InterruptedException;</span><br></pre></td></tr></table></figure><p>在 <code>ExecutorService</code> 基础上继续包装 <code>ScheduledExecutorService</code>，允许对线程启动提供调度 delay 的时间：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ScheduledFuture&lt;V&gt; <span class="title function_">schedule</span><span class="params">(Callable&lt;V&gt; callable, <span class="type">long</span> l, TimeUnit timeUnit)</span>;</span><br><span class="line">ScheduledFuture&lt;?&gt; schedule(Runnable runnable, <span class="type">long</span> l, TimeUnit timeUnit);</span><br></pre></td></tr></table></figure><h4 id="A-Implementation-ThreadPoolExecutor"><a href="#A-Implementation-ThreadPoolExecutor" class="headerlink" title="A. Implementation: ThreadPoolExecutor"></a>A. Implementation: <code>ThreadPoolExecutor</code></h4><p>Thread Pool 线程池，是 <code>Executor</code> 的一种实现，用的最多的是 <code>ThreadPoolExecutor</code> 类型。</p><ul><li><p><code>ThreadPoolExecutor</code> 类型继承：</p><p><img src="imgs/concurrent-executor-hier.png" width="350px" /></p></li><li><p><code>ThreadPoolExecutor</code> 状态维护：运行状态 (<code>runState</code>) 和线程数量 (<code>workerCount</code>) 放在同一个 Atomic Integer 中，高 3 位保存 <code>runState</code>，低 29 位保存 <code>workerCount</code>，二者同时取出避免数据不一致或者频繁占用锁资源。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">AtomicInteger</span> <span class="variable">ctl</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">AtomicInteger</span>(ctlOf(RUNNING, <span class="number">0</span>));</span><br></pre></td></tr></table></figure></li><li><p><code>runState</code>：共有 5 种：</p></li></ul><div class="table-container"><table><thead><tr><th>运行状态</th><th>状态描述</th></tr></thead><tbody><tr><td>RUNNING</td><td>运行状态。能接收新提交的任务，也能处理阻塞队列中的任务</td></tr><tr><td>SHUTDOWN</td><td>准备关闭状态。不接受新提交的任务，但能处理阻塞队列中的任务</td></tr><tr><td>STOP</td><td>停止状态。所有正在执行的任务会被终止，不接受新提交任务、队列中存在的任务</td></tr><tr><td>TIDYING</td><td>空闲状态。所有任务都已经结束，并且当前有效线程数（<code>workerCount</code>）为 0</td></tr><tr><td>TERMINATED</td><td>终止状态。在空闲状态下才能终止，标识不再使用</td></tr></tbody></table></div><p><img src="imgs/concurrent-tpe-runstate.png" width="550px" /></p><ul><li><p><code>execute</code> Control Flow：</p><ol><li>首先检测线程池运行状态，如果不是 <code>RUNNING</code>，则直接拒绝；</li><li>如果 <code>workerCount &lt; corePoolSize</code>，则创建并启动一个线程来执行新提交的任务；</li><li>如果 <code>workerCount &gt;= corePoolSize</code>，且线程池内的阻塞队列未满，则将任务添加到该阻塞队列中；</li><li>如果 <code>workerCount &gt;= corePoolSize &amp;&amp; workerCount &lt; maximumPoolSize</code>，且线程池内的阻塞队列已满，则创建并启动一个线程来执行新提交的任务；</li><li>如果 <code>workerCount &gt;= maximumPoolSize</code>，并且线程池内的阻塞队列已满，则根据拒绝策略来处理该任务，默认的处理方式是直接抛异常。</li></ol></li><li><p>拒绝策略（饱和策略）：</p><ul><li><strong><code>AbortPolicy</code></strong>：默认策略，抛出异常 <code>RejectedExecutionException</code>，拒绝执行；</li><li><strong><code>CallerRunsPolicy</code></strong>：调用执行自己的线程运行任务，也就是直接在调用 <code>execute</code> 方法的线程中运行 (run) 被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且要求任何一个任务请求都要被执行的话，你可以选择这个策略；</li><li><strong><code>DiscardPolicy</code></strong>：不处理新任务，直接丢弃掉；</li><li><strong><code>DiscardOldestPolicy</code></strong>：此策略将丢弃最早的未处理的任务请求。</li></ul></li></ul><p>使用 <code>ThreadPoolExecutor</code> 一般配合 fixed thread pool 的策略。好处是可以让应用 degraded gracefully。</p><p>不过 <code>Executor</code> 本身也提供了一些快速创建的工厂方法，帮助在一些场景下简化代码逻辑：</p><ul><li><p>使用 <code>newFixedThreadPool</code> 创建固定的线程数的线程池（同时最多只有指定的线程数正在执行）；</p></li><li><p>使用 <code>newSingleThreadExecutor</code> 单个线程实例的 executor，一次执行一个线程；</p></li><li><p>使用 <code>newCachedThreadPool</code> 创建可动态调整线程数的线程池，可以应对多个短期 tasks；</p><blockquote><p>创建一个可缓存的线程池，调用 <code>execute</code> 将重用以前构造的线程（如果线程可用）。如果没有可用的线程，则创建一个新线程并添加到池中。终止并从缓存中移除那些已有 60 秒钟未被使用的线程；</p></blockquote></li><li><p><code>newScheduledThreadPool</code> 创建一个支持定时及周期性的任务执行的线程池，多数情况下可用来替代 Timer 类；</p></li></ul><blockquote><p>[!WARNING]</p><p>但是在实际生产实践中，我们不建议使用 <code>Executors</code> 来创建线程池。建议使用 <code>ThreadPoolExecutor</code> 构造函数的方式，因为后者的处理方式让开发者更加明确线程池的运行规则，规避资源耗尽的风险。</p><p>再强调一次。项目中创建多线程时，使用上面的方法线程池创建方式，无论是单一、可变、定长都有一定问题，原因是：</p><ul><li><code>FixedThreadPool</code> 和 <code>SingleThreadExecutor</code> 底层使用有界阻塞队列 <code>LinkedBlockingQueue</code>；</li><li><code>CachedThreadPool</code>：底层使用的是同步队列 <code>SynchronousQueue</code>；</li><li><code>ScheduledThreadPool</code> 和 <code>SingleThreadScheduledExecutor</code>：使用的无界的延迟阻塞队列<code>DelayedWorkQueue</code>；</li></ul><p>这些队列的最大长度为都是 <code>Integer.MAX_VALUE</code>，可能会堆积大量请求导致 OOM（为什么<strong><u>网络请求场景下，队列越长越有可能 OOM</u></strong>，请参见 “计算机系统工程 - 拥塞控制”）。</p></blockquote><p>所以实际生产环境中开发者会根据需求手动定制 <code>ThreadPoolExecutor</code> 的 7 个参数，来自定义线程池。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 线程池的核心线程数</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">corePoolSize</span> <span class="operator">=</span> <span class="number">30</span>;</span><br><span class="line"><span class="comment">// 能容纳的最大线程数</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">maximumPoolSize</span> <span class="operator">=</span> <span class="number">200</span>;</span><br><span class="line"><span class="comment">// 空闲线程存活时间</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">long</span> <span class="variable">keepAliveTime</span> <span class="operator">=</span> <span class="number">0L</span>;</span><br><span class="line"><span class="comment">// 空闲线程存活时间 单位</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">TimeUnit</span> <span class="variable">unit</span> <span class="operator">=</span> TimeUnit.MILLISECONDS;</span><br><span class="line"><span class="comment">// 创建线程的工厂类,自定义线程名称</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">String</span> <span class="variable">threadName</span> <span class="operator">=</span> <span class="string">&quot;thread-local-pool-%d&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">ThreadFactory</span> <span class="variable">namedThreadFactory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ThreadFactoryBuilder</span>().setNameFormat(threadName).build();</span><br><span class="line"><span class="comment">// 存放提交但未执行任务的队列</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> BlockingQueue&lt;Runnable&gt; threadFactory = <span class="keyword">new</span> <span class="title class_">LinkedBlockingQueue</span>&lt;&gt;(<span class="number">1024</span>);</span><br><span class="line"><span class="comment">// 等待队列满后的拒绝策略</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">RejectedExecutionHandler</span> <span class="variable">handler</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ThreadPoolExecutor</span>.AbortPolicy();</span><br><span class="line"><span class="comment">// 定义线程池</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">ExecutorService</span> <span class="variable">pool</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ThreadPoolExecutor</span>(corePoolSize, maximumPoolSize, keepAliveTime, unit, threadFactory, namedThreadFactory, handler);</span><br></pre></td></tr></table></figure><h4 id="B-Implementation-ForkJoinTask"><a href="#B-Implementation-ForkJoinTask" class="headerlink" title="B. Implementation: ForkJoinTask"></a>B. Implementation: <code>ForkJoinTask</code></h4><p>而 Fork/Join 框架是针对 <code>ExecutorService</code> 接口的实现。它可以充分利用多处理器的优势，为那些可以拆成小块递归的任务设计，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if (my portion of the work is small enough) </span><br><span class="line">    do the work directly </span><br><span class="line">else </span><br><span class="line">    split my work into two pieces </span><br><span class="line">    invoke the two pieces and wait for the results </span><br></pre></td></tr></table></figure><p>在 <code>ForkJoinTask</code> 子类（<code>RecursiveTask</code> 有返回值、<code>RecursiveAction</code> 无返回值）中定义这些任务。</p><h3 id="7-8-3-Other-Utilities"><a href="#7-8-3-Other-Utilities" class="headerlink" title="7.8.3 Other Utilities"></a>7.8.3 Other Utilities</h3><h4 id="Concurrent-Collections"><a href="#Concurrent-Collections" class="headerlink" title="Concurrent Collections"></a>Concurrent Collections</h4><p>Concurrent Collections：更容易地管理大规模数据，减少 <code>synchronization</code> 次数；</p><h4 id="Atomic-Variables"><a href="#Atomic-Variables" class="headerlink" title="Atomic Variables"></a>Atomic Variables</h4><p>Atomic Variables：针对变量粒度的同步机制，可以在一定程度上避免 data inconsistency；</p><p>All classes have get and set methods that work like reads and writes on volatile variables</p><h4 id="Virtual-Threads"><a href="#Virtual-Threads" class="headerlink" title="Virtual Threads"></a>Virtual Threads</h4><p>Java 中是一类轻量级线程解决方案。让线程创建、调度、管理的开销最小化。</p><p>Virtual Threads 是 Java Thread 的实例，这与任何 OS thread 是相互独立的。</p><p>当 virtual threads 内部调用了阻塞的 I/O 操作后，会立即被 JVM 挂起；</p><p>virtual threads 有一个有限的 call stack，并且只能执行一个 HTTP client 请求 / JDBC 查询。这对一些异步的耗时任务比较合适，但是不适合 CPU intensive tasks；</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Thread</span> <span class="variable">virtualThread</span> <span class="operator">=</span> Thread.ofVirtual().start(() -&gt; &#123;</span><br><span class="line">    <span class="comment">// Code to be executed by the virtual thread</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>所以 Virtual Threads 不是说会比普通线程更快，而是说比普通线程更具可扩展性（provide scale），这在高并发、每次请求处理耗时的服务器网络应用中能提升吞吐量。</p><p><code>ThreadLocalRandom</code>：为多线程提供高效的伪随机数生成方案；</p><h1 id="Chapter-8-Java-Garbage-Collection"><a href="#Chapter-8-Java-Garbage-Collection" class="headerlink" title="Chapter 8. Java Garbage Collection"></a>Chapter 8. Java Garbage Collection</h1><p>本章，我们将先从 “为什么需要垃圾回收”、“垃圾回收的思路是什么”（8.1）出发，先介绍现存的主流语言（Python/JavaScript 等）甚至操作系统（Android）究竟采用了哪些垃圾回收方式（8.2 ~ 8.7），然后再来看看 Java 语言自己是怎么做垃圾回收的（8.8 ~ 8.9）。</p><h2 id="8-1-Problem-Definition"><a href="#8-1-Problem-Definition" class="headerlink" title="8.1 Problem Definition"></a>8.1 Problem Definition</h2><h3 id="8-1-1-Why-amp-How"><a href="#8-1-1-Why-amp-How" class="headerlink" title="8.1.1 Why &amp; How"></a>8.1.1 Why &amp; How</h3><p>通常情况下，无论是什么语言，在运行时想要分配空间，要么放在栈上、要么放在堆上。</p><p>栈上分配的变量全权由编译器管理（如果你写过编译器，请回想一下令人讨厌的 global frame size），这也是绝大多数语言的（解释型语言则是解释器）共性；</p><p>但是部分语言的有一部分空间是分配在堆上的。例如 tiger 语言中，我们初始化数组/结构体，它调用的 runtime function 底层由 C++ 实现，最终就是分配在堆上的。</p><p>如果这类使用堆空间的语言，在运行时不及时释放堆空间，可能会导致堆溢出的问题。由于像 tiger 这样的语言不涉及底层的结构（包括 Python、Java 等等），没有指针的概念，自然也没办法自己释放，或者这种语言的定位就是不需要开发者来释放（所谓 “内存安全”），那么还需要借助<strong><u>运行时机制</u></strong>来管理堆空间。</p><p>垃圾回收的基本原理就是，当一个被分配的地址没办法被当前的任何指针/变量访问到（not reachable），那么就需要运行时工具帮助释放这片空间，使得它能够被重新分配和使用。</p><blockquote><p>更准确一点，其实应该进行 liveness analysis（就像前面设计编译器时做的），但是运行时显然不方便做这种静态分析（运行时难以看出）。因此人们通常使用可达性（reachability）来代替 liveness 进行分析，只不过 reachability 可能没有 liveness 那么及时 / 精确（还可能出现一些问题，后面讨论）。</p><p>也就是说，如果存活，一定可达（这是由各个语言的编译器保证的）。</p></blockquote><p>于是，垃圾回收的过程就是，从当前已知存活的变量开始（例如当前栈帧上、寄存器中正在使用的变量），递归地去搜索可达的内存区域，再把分配过、但不可达的内存释放即可。</p><blockquote><p>我们通常使用有向图去描述 “两个存活变量间的可达关系”：结点表示程序当前栈上/寄存器中正在使用的变量 和 堆上分配的记录，边表示地址指针；</p></blockquote><p>所以，<strong>几乎所有自动 GC 都遵循一个理念：按照某个策略<u>预定的时间</u>（定时策略），释放<u>不再继续使用的</u>（标记策略）<u>引用类型变量</u>所占用的空间</strong>；</p><blockquote><p>为什么加上了 “定时”：运行时是动态的，总不能只回收一次，或者一直回收；</p></blockquote><h3 id="8-1-2-GC-Metrics"><a href="#8-1-2-GC-Metrics" class="headerlink" title="8.1.2 GC Metrics"></a>8.1.2 GC Metrics</h3><p>另外还有一点需要注意的是，GC 通常会伴随一段时间的 STW（stop-the-world，时停），此时段间，无论这个语言是单线程还是多线程的，都需要全部暂停等待 GC 的处理。</p><p>这样的 STW 在大多数 GC 算法中都是必要的，不过有长有短（取决于具体算法）。这主要是因为 GC 在运行时总有一些数据需要确保 synchronization，防止并发的回收造成数据不安全的问题。我将 STW 的时长称为 <strong>GC 算法的时延（GC latency）</strong>；</p><p>还有一点需要明确的是，我们引入 GC 是为了防止内存泄漏。而 “内存泄漏” 这个概念本身<strong><u>并不是</u></strong>说没有回收完所有的不再使用的空间就是泄漏了，而是<strong><u>没有及时的回收不再使用的堆空间，引发的一系列问题，包括堆溢出</u></strong>。</p><p>所以重点在于 “及时” 而不是 “全部”。也就是说，一个 GC 算法可以不需要在一次回收过程清理掉全部的垃圾，而是只需要确保及时就行。</p><p>于是我们还可以定义 <strong>GC throughput</strong>，即一次 GC 操作中，单位时间内回收记录的数量，这个指标能间接反映这个 GC 算法的效率，以及它究竟是否 “及时”。</p><h2 id="8-2-Mark-amp-Swap"><a href="#8-2-Mark-amp-Swap" class="headerlink" title="8.2 Mark &amp; Swap"></a>8.2 Mark &amp; Swap</h2><p>一种 GC 策略是 “标记清除”（Mark-And-Swap，标记策略）+ 溢出清理（定时策略）：</p><ul><li><p>Mark：从可达有向图的根结点（已知存活的变量）开始遍历，将遇到的所有结点全部标记一遍；</p></li><li><p>Swap：将整个堆扫描一遍，把没有标记的结点放到 free list 中（相当于释放），并且清空所有标记（为下一轮 GC 准备）；</p></li><li>程序在 GC 开始时 freeze，在 GC 结束时 resume，从 free list 中分配堆空间；</li><li>程序只有在 free list 为空时才进行 GC；</li></ul><blockquote><p>目前正在使用这个策略的语言有 JavaScript；</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Mark phase:</span><br><span class="line">    for each root v</span><br><span class="line">        DFS(v)</span><br><span class="line"></span><br><span class="line">Sweep phase:</span><br><span class="line">    p &lt;- first address in heap</span><br><span class="line">    while p &lt; last address in heap</span><br><span class="line">        if record p is marked</span><br><span class="line">            unmark p</span><br><span class="line">        else let f1 be the first field in p</span><br><span class="line">            p.f1 &lt;- freelist</span><br><span class="line">            freelist &lt;- p</span><br><span class="line">        p &lt;- p + (size of record p)</span><br><span class="line"></span><br><span class="line">function DFS(x)</span><br><span class="line">    if x is a pointer and points to record y</span><br><span class="line">        if record y is not marked</span><br><span class="line">            mark y</span><br><span class="line">        for each field fi of record y</span><br><span class="line">            DFS(y.fi)</span><br></pre></td></tr></table></figure><p>这样做的优缺点：</p><ul><li><p>优点：算法简单便于实现，很多情况下确实是有效的（满了就回收全部的垃圾）；</p></li><li><p>缺点：性能不佳（扫描全堆，throughput 不大），而且需要经常打断程序执行流先做垃圾回收，程序效率不佳（总体 STW 很长）。</p></li></ul><p>分析一下：</p><p>$T=c_1R+c_2H$（$R$ 为可达变量数、$H$ 为堆大小），每次增长 free list entry 数量 $H-R$，因此总体均摊时间：$\overline{T}=\dfrac{c_1R+c_2H}{H-R}$；</p><p>于是我们知道：当 $H$ 和 $R$ 很接近的时候，GC 的性能会很差。因此我们的改进是，在 $\dfrac{R}{H}\gt0.5$ 的时候建议 OS 增大当前进程的堆的大小；</p><p><u>第二个改进</u>，是考虑到如果只使用 DFS 函数调用递归很可能导致栈溢出（因为堆本身是很大的），考虑引入 显式的栈来实现 DFS；</p><blockquote><p>更有技巧一点的话，还有 pointer reversal 这类优化的方法。但是本文的主题不是讨论这些算法，因此略过，感兴趣的读者可以自行搜索。</p></blockquote><p><u>第三个改进</u>，对于 free list，我们可以模仿 Memory Allocation 的 aggregation list，管理多个 free list 并按照列表的大小来分类；</p><h2 id="8-3-Reference-Count"><a href="#8-3-Reference-Count" class="headerlink" title="8.3 Reference Count"></a>8.3 Reference Count</h2><p>还有一类常见的 GC 策略是 “引用计数”（标记策略）+ 分配时清理（定时策略）；</p><blockquote><p>目前使用这种 GC 策略的语言有 Python、Swift 等等；</p></blockquote><p>这里我们对每一个在堆上的 record 维护一个额外的字段（<code>ref_cnt</code>）表明当前有多少变量指向它。</p><p>然后在赋值、作用域变化等等情况时更新变量对应的值就行。</p><blockquote><p>举个例子，例如 record <code>x</code> 的某个 field <code>x.fi</code> 原本是 <code>z</code>（是堆上的 record）赋值为 <code>y</code>（另一个堆上的 record）此时：</p><ul><li>读写 <code>y</code> 的 reference count 使其增加；</li><li>读写 <code>z</code> 的 reference count 使其减少；</li><li>检查 <code>z</code> 的 reference count 如果是 0，则将 <code>z</code> 加入 freelist；</li><li>而 <code>z</code> 中的字段（例如 <code>z.fi</code>）如果是堆上的指针，则<u>推迟到 <code>z</code> 所在的地址被从 free list 分配出去时再减小 reference count</u>；</li></ul></blockquote><p>于是相较于同步的 Mark-and-Swap，引用计数的好处是：</p><ul><li>避免了批量、递归的检查回收操作，将部分的更新引用计数操作推迟到分配时（批处理提升了 GC throughput）；</li><li>不需要频繁进行全堆扫描，提升了程序执行效率（压缩了 STW 的时长）；</li></ul><p>只不过引用计数还引入了一些问题：</p><ul><li><p>循环引用：相互引用的变量无法让 <code>ref_cnt</code> 减为 0（信息不充分会导致回收不充分）；</p><blockquote><p>解决方法 1：引入语义层面的语法特性，例如<strong><u>弱引用</u>（Weak References）</strong>，当内存压力比较大的时候，将这种弱引用视为没有引用。</p><p>但是这相当于把难题丢给了开发者，容易导致程序运行时错误的出现（例如使用一个被释放了的弱引用变量）；</p><p>解决方法 2：进行简单的 cycle detection，对某些容易成环的地方进行特殊检查，及时除环；</p><p>解决方法 3：与 mark &amp; swap 结合（occasional），补上全局信息，但 mark 操作很昂贵；</p></blockquote></li><li><p>内存访问性能问题：例如 <code>x.fi &lt;- p</code> 这个语句在加上引用计数的 GC 后，会变成 3 次内存访问，性能可能更差些：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">z &lt;- x.fi</span><br><span class="line">c &lt;- z.count</span><br><span class="line">c &lt;- c – 1</span><br><span class="line">z.count &lt;- c</span><br><span class="line">if c = 0 call putOnFreelist</span><br><span class="line">x.fi &lt;- p</span><br><span class="line">c &lt;- p.count</span><br><span class="line">c &lt;- c + 1</span><br><span class="line">p.count &lt;- c</span><br></pre></td></tr></table></figure></li></ul><p>正因为这两个问题，我们常常在一些 GC 学术原型中才能见到它，实际使用这种方法会出现一些难以避免的性能问题。但是不可否认的是，这种方法（思路）真的很简单和显然，所以它也被常常应用到其他的领域和方面。例如 file table 维护文件打开状态时使用、虚拟页和物理页的映射时物理页维护引用计数、C++ 的智能指针 <code>shared_ptr</code> 等等。</p><h2 id="8-4-Copy-Collection"><a href="#8-4-Copy-Collection" class="headerlink" title="8.4 Copy Collection"></a>8.4 Copy Collection</h2><p>这类 GC 策略比较新，有些现代的应用（例如 Android 10+）就在使用这种 GC 策略。内容如下：</p><ul><li><p>将 heap 拆成两个部分：from-space &amp; to-space；</p><p>from-space 专门存放分配的内容，to-space 专门管理回收的内容；</p></li><li><p>两个 space 都有一个 <code>limit</code> 指针表示该区域结尾；</p></li><li><p><code>next</code> 表示该区域接下来要插入的位置，分配内存就是向 <code>next</code> 后面追加可达的 entry；</p></li><li><p>如果 <code>next</code> 到 <code>limit</code> 的位置，证明当前的 semi-space 满了，我们从 <code>root</code> 根结点（目前肯定存活的）开始，将所有可达结点 copy 到另一个 semi-space 中（<code>next</code> 也移过去了），相当于丢弃了不可达结点、变相进行了一次 GC；</p></li></ul><p>这样做有几点好处：</p><ul><li>不需要特地进行 mark + sweep 了，操作更简单，降低了时间复杂度，一次 copy 足矣（进一步降低 STW 长度）；</li><li>每次 copy 都相当于整理出了连续的空闲堆空间，方便分配、减小了 external fragments，最大化 memory utilization；</li></ul><p>相比于 mark-and-sweep 也有坏处：</p><ul><li>如果大部分变量存活时间很长（$R\sim H$），会导致内存拷贝过多，overhead 很大；</li><li>一半的空间利用率低下；</li></ul><p>实现 Copy Collection 的算法是 Cheney’s algorithm：</p><p><img src="imgs/gc-cc-cheney-algo.png" width="400px" /></p><blockquote><p>解释一下算法：</p><p>把当前准备切走的一边称为 from-space，另一边称为 to-space；</p><p>每个被分配的 entry（$p$）的第一个字段（$f_1$）保存<u>指向当前分配对象自己</u>的指针（只有在移动更新时指向新的对应的 entry），其他字段（$f_i$）存放正常被分配的数据；</p><p><code>Forward(p)</code> 函数的含义是将 <code>p</code> 指针对应的结点数据完全移动到 to-space（如果已经完成移动则什么也不做）；</p><p>主函数的算法就是从根结点开始（先 forward 根结点）BFS 遍历结点、更新 <code>scan</code>，遍历到的直接调用 <code>Forward</code> 转移。同时需要更新拿在手上的指针，保证上层应用无感；</p></blockquote><p>这种算法有些优缺点：</p><ul><li>优点：不引入额外数据结构（没有额外的栈、不需要 reversal pointers）；</li><li>缺点：影响原来程序的 locality！无法充分利用局部性（不相干的 record 可能被复制到一起），这会降低每次 GC 后的程序执行效率；</li></ul><p>因此我们作出优化：<strong><u>Semi-depth-first Algorithm</u></strong>，这也是 Copy Collection 最常见的实现方法：</p><p><img src="imgs/gc-semi-df.png" width="700px"/></p><p>这个算法的思路是 DFS，所以主函数省略了（就是对每个 root 结点调用 <code>Forward</code> 然后结束）。这个 <code>Forward</code> 函数我们在 Cheney’s Algorithm 中见过类似的结构，就是如果当前 entry（由 <code>p</code> 指向）完全移到 to-space（第一字段已经在 to-space 了），那么就直接返回，否则调用 <code>chase</code> 迁移；</p><p>主要看 <code>Chase(p)</code> 的做法。<code>q</code> 和 <code>next</code> 有点像算法里的左右指针，<code>q</code> 表示当前正在迁移的 entry 的开头，<code>next</code> 则表示正在迁移的 entry 的结尾；</p><p>而 <code>r</code> 和 <code>p</code> 又像一对前后指针，<code>p</code> 表示当前正在 copy 的节点，<code>r</code> 则是一轮 semi-DFS 从根到叶遍历的指针；</p><p>中间 <code>for</code> 循环的作用是把要 copy 的 <code>p</code> 指向的 entry 的每个字段都复制到 to-space 中。同时 <code>Chase</code> 还需要考虑 <code>p</code> entry 中指向 from-space（分配在堆上但没迁移的 entry）的指针。</p><blockquote><p>为什么 <code>Chase</code> 需要考虑 <code>p</code> 中的指向 from-space 的指针？因为这里是 DFS，不能直接结束对 <code>p</code> 的迁移，否则就变成 BFS 了（Cheney’s Algorithm）；应该像这样一直沿树边递归到底；</p></blockquote><p>一轮 semi-DFS 后，如果 <code>p</code> 有多个 field 都指向堆，那么默认是先 copy 当前 <code>p</code> entry 中的最后一个 from-space 分配的地址，方便 <code>repeat</code> 循环递归（DFS）地移动 from-space 中分配的 entries；</p><blockquote><p>为啥是 copy 最后一个？因为递归过程一次只能保存一个，这也是算法称为 “semi-DFS” 的原因（没有完全遵循 DFS 的遍历方法）。更清楚一点，我们可以看下面的比较图（注意到 4、6 是最后才访问的）：</p><p><img src="imgs/gc-semi-dfs.png" width="350px" /></p><p>例如 <code>1 -&gt; 2 -&gt; 5</code> 后，一轮 semi-DFS 结束，虽然看到了 <code>4</code> 对应的 record，但也只是更新了 <code>2</code> 的 record 的指针，并没有 copy <code>4</code> 的 record 到 to-space；随后递归回溯到 <code>1</code> 再继续；</p></blockquote><h3 id="Optimizations"><a href="#Optimizations" class="headerlink" title="Optimizations?"></a>Optimizations?</h3><p>注意到，copy collection 对于堆空间的浪费还是很严重的，因为每次只使用一半的堆空间（另一边必须是无效的）。于是一个很简单的优化是，底层使用 <code>mmap</code> 来处理对堆空间的分配。我们可以让另一半（不在使用的 to-space）在 copy 前是 unmap 的状态（未分配物理页），这样能更充分地利用机器资源。</p><blockquote><p>但这么做也有点问题，在 GC collector 进行 copy collection 的途中可能出现物理内存猛增的情况；</p><p>但这确实能缓解在除了 GC 阶段以外的其他时间里内存不够用的情况。</p></blockquote><p>注意到 Copy Collection 的均摊开销主要在 $\dfrac{c_3R}{\dfrac{H}{2}-R}$ 左右（$c_3\sim10$，$R$ 为存活记录的大小）；那么我们如何降低上述内存使用，并且继续提升整体 GC 性能呢？</p><h2 id="8-5-General-Collection-Generations"><a href="#8-5-General-Collection-Generations" class="headerlink" title="8.5 General Collection (Generations)"></a>8.5 General Collection (Generations)</h2><h3 id="8-5-1-Design"><a href="#8-5-1-Design" class="headerlink" title="8.5.1 Design"></a>8.5.1 Design</h3><p>一般情况下，GC 的 throughput 和 latency 是相互制约的，例如我想要确保 throughput 很大，一般需要扫描更多的信息来进行批处理，但扫描更多的信息会导致 latency 的延长。</p><p>但是这种 GC 的策略相较于前面几种方法，可以同时提升 throughput、降低 latency。主要是人们有以下的特性的观察（不是准确和普适的定律，而是经验假设）：</p><ul><li>堆上新创建的对象通常更容易先死去；</li><li>堆上存活的时间越长的对象通常更不容易死去；</li></ul><blockquote><p>堆上存活时间长的越长，短的越可能更短。也许可以用马太效应解释；</p></blockquote><p>基于这个特性，人们提出了 general collection 的 GC 策略（分代）：</p><ul><li><p>将 heap 分为若干区域 $G_0,\space G_1,\space G_2,\ldots$，编号从小到大存放的是创建对象的由新到旧（新生代 ~ 持久代）。被分配的对象首先被放入 $G_0$；</p></li><li><p>$G_0,G_1,\ldots$  这些代（generations）一般使用 Marking 的思路（给数据标记）；</p></li><li><p>GC collector 重点回收 $G_0$ 区域的对象。同时需要关注 $G_1,G_2,\ldots$ 中指向 $G_0$ 区域的指针（如果存在，$G_0$ 的这个区域就不能释放）；</p><blockquote><p>其实 $G_1,G_2,\ldots$ 中有指向 $G_0$ 区域的指针 这件事本身就很罕见。主要是因为根据经验假设，$G_0$ 只有很少一部分新生对象会进入 $G_1$ 以及更旧的代（10%-20%），而 $G_1$ 中的指针更普遍的是指向自己代或者更旧的代（因为先定义再使用嘛），更少有指向新一代的指针。</p><p>相反的情况，如果 $G_0$ 中的数据有些指向 $G_1,G_2,\ldots$ 区域的指针，那么它们可以立即被释放；</p></blockquote></li><li><p>每次 $G_0$ 未能清空 N 次（N 一般是 2~3 左右，应该看不同应用场景）的部分会转移到 $G_1$，并且更旧的代同理，依次进行；也正因如此，$G_0,G_1,\ldots$ 各代间的大小差距最好是指数级的；</p></li></ul><p>这样做带来的好处是，每次回收只需要扫描原先 20% 左右的区域，但是能回收率能达到 80%，不仅减小了 latency（扫描的区域少了），而且增大了 throughput（单位时间回收的量增多了），极大提升 GC 效率。</p><p>这种策略的均摊时间开销：$\dfrac{c_3R}{H-R}$（和 Copy Collection 相比，$\dfrac{H}{2}$ 变为 $H$）；并且通常情况下，由于 $G_0$ 本身不大，因此很多情况都有 $H\gt10R$，也就是说一般情况的均摊复杂度 $\dfrac{1}{9}c_3$（相对于前面的策略效率提升了 10 倍，如果只考虑 $G_0$ 的话）！</p><p>可是，如果涉及更旧代的回收，时间开销还是很大的（例如如果只有两代，并且做一次 $G_0,G_1$ 回收，就相当于扫描了整个堆）。不过好在需要回收旧代的频率并不是很高。</p><h3 id="8-5-2-Patch-Ways-of-Remembering"><a href="#8-5-2-Patch-Ways-of-Remembering" class="headerlink" title="8.5.2 Patch: Ways of Remembering"></a>8.5.2 Patch: Ways of Remembering</h3><p>现在回过头看一下，如果使用 general collection，有个问题是我们 “同时需要关注 $G_1,G_2,\ldots$ 中指向 $G_0$ 区域的指针”。</p><p>虽然就像前面说的，这种情况很罕见，但还是需要考虑的，例如堆上分配的结构体在它被分配的很久以后突然更新了一个字段，这种情况就可能出现上述罕见的现象。于是我们<u>需要单独保存一些旧代中有指针指向新代的信息</u>（不然 GC collector 很难判断 $G_0$ 中会不会出现上述罕见情况，而且全表扫描太慢了）。</p><p>为了解决这个问题，人们首先想出了借助一个 Remembered List 的方法：每次更新被分配 entry 的某个字段时，向这个 list 中加入一条更新的记录。这种方法有个问题，就是实际上可用的堆的空间一般很大，平均需要记录的更新数据可能会达到数个 MB，这对于一个内存中的列表而言开销已经比较大了。</p><p>于是人们想使用 Remembered Set 来存放，因为它的去重性质，我们可以在分配的 object $b$ 中使用 1 个 bit 来记录它是否已在 remembered set 中；</p><p>以上两种方法的粒度都是对象（很细），免不了占用比较大的内存资源。</p><p>于是人们提出了粗粒度的信息存放方案（<strong><u>Card Marking</u></strong>，比较主流的实现方案）：</p><ul><li>将内存分为 $2^k$ bytes 大小的内存区域，称为 logical cards（逻辑卡片）；</li><li>一个对象可以占据一个卡片的一部分，或者从卡片中间区域开始占据到后面的卡片；</li><li>当 $b$ 地址在分配后被更新时，包含这个地址的卡片会被标记；</li><li>可以用更小的 list（byte index 向右移动了 $k$ 位）来保存 mark 的情况；</li></ul><p>具体实现就是在每次更新 object 时打桩（在 <code>obj.f = a</code> 的后面加上代码，判断如果确实是 <code>G1</code> 指向 <code>G0</code> 的情况，则从 card 对应的 list 中取出并更新）；</p><p>坏处：不清楚在这片区域的 cross generation pointer 具体在哪里，还需要在这片区域内继续查找（空间换时间）；</p><p>另一种方法是用 Page 的粒度来管理这个信息（<strong>Page Marking</strong>）：</p><ul><li>就像 card marking，不过 $2^k$ 是页表大小；</li><li>更新一个分配的 object 相当于 makes the page dirty；</li><li>用户态程序可以在每次 GC 后标记 write-protected；</li></ul><p>这样相当于把 card marking 的打桩操作变成了用户态的 fault handler。但实际上开销也不小（毕竟需要进出内核以及用户态 handler）；</p><h2 id="8-6-Incremental-Collection"><a href="#8-6-Incremental-Collection" class="headerlink" title="8.6 Incremental Collection"></a>8.6 Incremental Collection</h2><p>这里还有个更加复杂的回收策略，和实际应用结合得比较紧密。</p><p>这个策略着重关注于优化 STW 时长（latency）这个指标，希望 GC 让应用停止的时间尽可能短（尤其是在涉及 UI 前台类型/端侧的语言中，不希望用户感受到卡顿）；</p><p>比如对于数十 GB 的内存而言，同步的 Mark &amp; Swap 的 STW 可能在百毫秒级别，这对后台程序 / 服务器应用而言不那么关心，但是如果是端侧设备 / 浏览器 / 游戏设备 / 精确的嵌入式设备呢？这个问题就很突出了。</p><p>这个时候，两种思路：</p><ul><li>让 GC 回收程序和应用并发执行（从根本上极大缩短甚至基本消灭 STW）；</li><li>让 GC 回收程序按应用执行的情况增量地回收（通过减少需要扫描的数据量，也能极大缩短 STW）；</li></ul><p>这时我们需要引入一些概念：</p><ul><li><p>Collector：专门收集不再使用的分配的空间并释放；</p></li><li><p>Mutator：改变 reachable data 的关联图；</p><blockquote><p>将应用中改变 reachable 关系的逻辑抽象出来，它就是之前阻碍 GC collector 并发，也是 STW 出现的主要原因；</p></blockquote></li><li><p>Incremental Collection：只有 Mutator 需要 Collector 回收时才进行回收（调用式关系）；</p></li><li><p>Concurrent Collection：在 Mutator 执行期间 Collector 并发执行（后台服务关系）；</p></li></ul><p>现在介绍一种 Incremental Collection 的实现模型：<strong><u>Tricolor Marking</u>（三色标记模型）</strong>；</p><p>任意一个被分配的记录只会处于 3 种颜色中：</p><ul><li>White: not visited by GC；</li><li>Grey: visited by GC, but its children are not；</li><li>Black: visited by GC, so as its children；</li></ul><p>算法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">procedure tricolor-marking:</span><br><span class="line">set root object gray // visit</span><br><span class="line"></span><br><span class="line">while they are any gray objects:</span><br><span class="line">    select a gray record p</span><br><span class="line">    for each field fi of p:</span><br><span class="line">        if fi.p is white:</span><br><span class="line">            color fi.p gray// visit first time (可以用栈压入来实现)</span><br><span class="line">    color record p black// visit second time (可以用栈弹出来实现)</span><br></pre></td></tr></table></figure><blockquote><p>这里是 BFS 性质的遍历所以使用栈数据结构。可以类比，Copy Collection 中的 Cheney’s Algorithm，可以使用队列数据结构来实现；</p></blockquote><p>这里注意几个<strong><u>算法不变量</u>（Invariants）</strong>，它们是构建 Incremental Collection 的根本原理：</p><ul><li>不存在黑结点直接指向白结点的情况。如果有就说明黑色结点对应的对象并没有处理完全，出现了数据不一致的问题；</li><li>灰结点一定在 collector 的数据结构（fringe）中，意味着正在处理；</li></ul><p>现在，我们希望在 tricolor marking 的基础上引入 incremental collection，就意味着，被分配的 object 在 GC 结束后继续留有标记，交给 mutator 执行。</p><p>过一段时间后，再次进入 GC 执行算法时，可能就会出现上述算法不变量的违反。</p><p>例如，已经染黑的结点（GC 完全扫描的结点）在上一轮 GC 结束后，被 mutator 追加了白色结点（上一轮 GC 结束后才分配的），这个时候就违反了第一个 invariant。</p><p>于是！所谓的 incremental collection 做增量回收，就是通过在 runtime 分配堆空间时，采取措施保证 invariants 的成立（在 mutator 运行时，而非 collector 运行时），对变化的部分进行 GC 检查。</p><p>基本的思路还是，针对基本的读写操作进行打桩（barriers）：</p><ul><li><p>Method 1 (Dijkstra, Lamport)：向黑结点 object 写入指针 field （插入白结点）时染灰（纳入 GC 管理中）；</p></li><li><p>Method 2 (Steele)：向黑结点 object 写入指针 field （插入白结点）时将父黑结点染灰（dirty，让之前的结点重新放入 GC fringe、表示需要重新扫描）；</p><p>(Boehm, Demers, Shenker) 改进是把 black 结点的页改成 write-protected，page fault handler 更改权限为 writable，并且标记为灰色（但这么做也有性能问题）；</p></li><li><p>Method 3 (Baker)：在读<u>灰结点</u> object 的白子结点时提前将该白结点染灰（纳入 GC 管理），这样 mutator 永远无法获得一个不受 GC 管理的指针了！因此就不会出现 invariant 的违反问题；</p><p>(Appel, Ellis, Li) 改进还是利用 page fault，但这个时候直接从灰结点开始进行一次 GC，染成黑色-灰色的结点；</p></li></ul><p>于是，我们基于 Copy Collection 的 Cheney’s Algorithm 实现一个 Incremental Collection 算法 (read)：<strong><u>Baker’s Algorithm</u></strong>；</p><p>基本前提：应用（mutator）和 GC（collector）处于同一线程中；</p><p>定时策略：仅 allocate memory 时交互进行；</p><p><img src="imgs/gc-baker-algo.png" width="250px" /></p><p>仍然把 heap 分为 from-space 和 to-space 两个部分；</p><p>GC 过程仍然从堆内存满了后开始，但不是一次性做完，而是每次 allocation 时增量地做一点；步骤如下：</p><ol><li><p>当堆（from-space）满后，交换二者角色，现在 from-space 变成空的一边；并且立即 forward root 结点到 from-space；</p><p><img src="imgs/gc-baker-flip.png" width="250px" /></p></li><li><p>第一步结束后，不继续复制，而是先退出 GC 例程，回到应用（有点像协程 coroutine）；当应用在 GC 例程暂停期间 allocate 堆时，会再次唤醒 GC，此时：</p><ul><li><p>会直接分配在空的 from-space 的末端（减小 limit 的指针位置）；</p><p><img src="imgs/gc-baker-alloc.png" width="250px" /></p></li><li><p>对应的扫描并从 to-space 中复制已分配记录（注意，如果分配了 N bytes 空间，那么从 to-space 扫描的大小不小于 N bytes，为了 copy 的过程不慢于应用分配的速度）。注意 forwarding pointers；</p><p><img src="imgs/gc-baker-incr-scan.png" width="250px" /></p></li></ul></li><li><p>第一步结束后，如果应用在 GC 例程暂停期间 load references，创建了指向 to-space entry（这个 entry 是没有被复制到 from-space 过的）的 root 结点，那么根据 tricolor marking 理论，此时 to-space 中未被复制到 from-space 的 entries 全是白结点（没有被 GC 遍历过），这相当于直接将黑结点指向白结点（违反 invariant 1）。我们立即做一次 forward（但不关心它内部的 forwarding pointers）：</p><p><img src="imgs/gc-baker-load1.png" width="550px" /></p><p>如果指向的 to-space entry 是已被复制的，那么立即 forward：</p><p><img src="imgs/gc-baker-load2.png" width="550px"/></p></li><li><p>当 <code>scan</code> 碰上 <code>next</code> 时，allocation 结束，本轮 GC 再次暂停，控制权又回到应用；</p><p>只要保证 $R\lt\dfrac{1}{4}H$，在上述过程中就不会出现 GC 扫描时 from-space 用完的情况；</p></li></ol><p>总结一下 incremental collection 的优缺点：</p><ul><li>优点：时延低，把复制的操作均摊到每次 allocation 时，减小了单次的 STW 时长，对 real-time app（例如浏览器/端侧设备渲染、游戏应用等等）友好；</li><li>缺点：总体开销很大，每次 read 都多两条指令（compare &amp; jump）；性能开销高出 20%（而且还没有考虑 locality）；</li></ul><h2 id="8-7-Concurrent-Collection"><a href="#8-7-Concurrent-Collection" class="headerlink" title="8.7 Concurrent Collection"></a>8.7 Concurrent Collection</h2><p>之前我们讨论的是同步 GC，有没有异步 GC 呢？有的。就像前面说的，无论如何都需要 STW 来同步信息，只不过 STW 的长短罢了。在并行 GC 中，STW 的大部分工作都在 synchronization 中；</p><p>这里全部展开的话篇幅过长，我们按下不表，在后面讨论 Java 的并行 GC 的算法时再作介绍。</p><h2 id="8-8-Parallel-Scanvenge-Java"><a href="#8-8-Parallel-Scanvenge-Java" class="headerlink" title="8.8 Parallel Scanvenge (Java)"></a>8.8 Parallel Scanvenge (Java)</h2><p>一个内存密集型设计比较成功的语言。因此 Java 的 GC 设计的比较成熟。</p><p>2010 年左右的时间，Java 8 默认使用的是 <strong><u>Parallel Scavenge (PS)</u></strong> 这种 GC 策略。</p><p>定时策略：某个 semi-space 满了后；</p><p>它其实也是一种 Generation GC 的策略（stop-copy-compact，也有 STW 存在）。不过 Parallel Scavenge 对于新代和旧代的 GC 算法是不同的。总体结构如下：</p><p><img src="imgs/gc-modern-java-arch.png" width="350px" /></p><p>注意到，Young Generation（新代）中存在 3 个 semi space:   Eden、From、To，使用 Minor GC 算法如下：</p><ol><li>Application allocate 空间时向 Eden 区域直接插入；</li><li>Eden-space 满了后，从 root 遍历复制 reachable records 到 to-space；</li><li>当 Eden-space 再次满了/ to-space 满了后，同时从 root 遍历复制 reachable records 到 from-space；<ul><li>其中在 to-space 中存活 <code>K</code> 轮的 records，会被 promote 到 old generation(s) 中；</li></ul></li><li>在一轮复制结束后，如果 from-space 快满了，则交换 to-space 和 from-space 的身份；</li></ol><p><img src="imgs/gc-java-minor.png" /></p><p>Minor GC 如何并行与 Application 执行（parallel execution）？</p><ul><li>Reference Tracking；</li><li>Copy Race（Queue &amp; CAS）；</li><li>Work Stealing；</li></ul><p>对于旧代（Old Generation）使用 Full/Major GC 算法：</p><blockquote><p>一般要执行 Full GC，那么这时可能剩余内存已经不多了。</p></blockquote><ul><li><p>Marking:   Mark all live objects;</p></li><li><p>Summary:   Calculate new address for live objects;</p><blockquote><p>先算出地址，而不是上来就 copy，方便后面的 compaction；</p></blockquote></li><li><p>Compact:   Move objects and update references;</p><blockquote><p>需要考虑一个问题，如果 destination 也是 source 呢？</p></blockquote></li></ul><p><img src="imgs/gc-java-major.png"/></p><p>PS 这种策略也有些缺陷，例如空间是连续的（难以 split/adjust/reorganize），并且比例固定（在 VM 启动后固定）：</p><p>而且 Minor GC（4 GB ~）一般在 100ms 左右，Major GC （16 GB ~）一般多于 1s。</p><p>为了改进这个问题，我们可以：</p><ul><li><p>将 heap 的结构从 space 改成 regions（分块，smaller, segregated regions）：</p></li><li><p>Also including young and old spaces:   Adding a middleground between old and young，这被称为 <strong><u>Mixed GC</u></strong>；</p><p><img src="imgs/gc-java-mixed.png" width="350px" /></p></li><li><p>Collection set: including all regions to be collected；</p><p><img src="imgs/gc-java-collection-set.png" width="250px" /></p></li></ul><p>当然，Parallel Scanvenge 有很强的特点，就是它是吞吐量垃圾收集器（Throughput Collector），它的设计目标是最大化应用程序的吞吐量。也就是说它适用于<strong><u>对吞吐量要求较高</u></strong>的应用程序，例如适合批处理任务或后台处理任务。</p><h2 id="8-9-G1GC-Java"><a href="#8-9-G1GC-Java" class="headerlink" title="8.9 G1GC (Java)"></a>8.9 G1GC (Java)</h2><p>Garbage First GC（G1GC）主要也是采用了 Generation GC 的思想，不过具体细节有些改动。</p><p>它将 Heap 分成 3 个部分，有点类似之前的 Parallel Scanvenge + Mixed GC 改进 + Collection Set 改进：</p><ul><li><strong>Eden:</strong> This is where newly created objects are allocated.</li><li><strong>Survivor:</strong> There are typically two Survivor regions (S0 and S1), and they are used to  hold objects that have survived one or more garbage collection cycles.</li><li><strong>Old Generation:</strong> This region is used to hold long-lived objects that have survived multiple garbage collection cycles.</li></ul><p>这里这篇文章描述的比较好，可以看看：<a href="https://medium.com/@perspectivementor/how-g1-garbage-collector-work-in-java-e468a94ebed6">How G1GC Work in Java</a>；</p><p>算法则主要分为 3 个阶段：</p><ul><li>标记阶段，即从 Roots References 开始，标记活跃对象；<ul><li>此阶段一开始寻找 root 时需要 STW，不过因为数量很少，因此时间较短；</li><li>然后需要遍历 reachable graph，此时是 concurrent mark，因此不需要 STW；</li></ul></li><li>转移阶段，即把活跃对象复制到新的内存地址上；</li><li>重定位阶段，因为转移导致对象的地址发生了变化，在重定位阶段，所有指向对象旧地址的指针都要调整到对象新的地址上。</li></ul><p>这里我们重点关注 G1GC 针对 Young GC 和 Mixed GC 的算法上，其主要流程如下：</p><p><img src="imgs/gc-java-g1gc.png" width="550px" /></p><p>此外，除了 Mixed GC 调整年轻代和年老代的回收阈值、heap 的分区收集结构，以及更强的并发能力以外，G1GC 还提供了<strong>预测性暂停时间</strong> 的特性，可以通过设置目标暂停时间（Pause Time Goal）来控制垃圾收集的暂停时间。</p><p>最后，G1GC 是<u>为多核处理器和大内存系统设计的</u>，旨在替代 CMS（Concurrent Mark-Sweep，注意到历史原因）的并发垃圾收集器（也是 Concurrent GC 的一种）。</p><h2 id="8-10-ZGC-Java"><a href="#8-10-ZGC-Java" class="headerlink" title="8.10 ZGC (Java)"></a>8.10 ZGC (Java)</h2><p>这是 Java 中的一个更新的 GC 机制。它在 Java 11 中实验性使用，在 Java 15 及以后可以正式作为生产环境的一种选择了。</p><p>ZGC 致力于提供尽可能短的 STW 时间。它的目标是：</p><ul><li>STW 时间不超过 10 ms；</li><li>STW 时间不会随着堆的大小，或者活跃对象的大小而增加；</li><li>支持 8 MB~4 TB 级别的堆（未来支持 16 TB）。</li></ul><p>这主要是想匹配服务器应用程序的使用场景。因为在服务器应用程序中，大堆很常见（通常程序运行时间更长、运行的守护进程更多），而且需要快速的应用程序响应时间。</p><p>ZGC 有两种算法，一种是 Generational 的（分代），这个和前面 G1GC 以及 Parallel Scanvenge 比较相似，能够利用到分代的好处（吞吐量上升和更低的时延）；也有一种是 non-generational 的，主要是考虑到禁用分代后可以对某些使用场景进行运行时性能优化。这里我们为了简便，就讨论 non-generational 的实现。下面我们将不分代的 ZGC 单独称为 “ZGC”；</p><p>与 G1GC 的 Mixed GC 和 Young GC 类似，ZGC 也是采用标记然后复制的算法，不过 ZGC 对这部分的算法做了重大改进：ZGC 在标记、转移和重定位阶段几乎都是并发的。ZGC 垃圾回收周期如下图所示：</p><p><img src="imgs/gc-java-zgc.png" width="550px" /></p><p>注意到 “初始标记” 就是我们在 G1GC 中讨论的，标记 root references，需要 STW、不能并发，不过时间很短。</p><h3 id="Tricky-Things"><a href="#Tricky-Things" class="headerlink" title="Tricky Things"></a>Tricky Things</h3><p>ZGC通过着色指针和读屏障技术，解决了转移过程中准确访问对象的问题，实现了并发转移。大致原理描述如下：并发转移中“并发”意味着GC线程在转移对象的过程中，应用线程也在不停地访问对象。假设对象发生转移，但对象地址未及时更新，那么应用线程可能访问到旧地址，从而造成错误。而在ZGC中，应用线程访问对象将触发“读屏障”，如果发现对象被移动了，那么“读屏障”会把读出来的指针更新到对象的新地址上，这样应用线程始终访问的都是对象的新地址。那么，JVM是如何判断对象被移动过呢？就是利用对象引用的地址，即着色指针。下面介绍着色指针和读屏障技术细节。</p><h4 id="References-Coloring"><a href="#References-Coloring" class="headerlink" title="References Coloring"></a>References Coloring</h4><blockquote><p>着色指针是一种将信息存储在指针中的技术。</p></blockquote><p>ZGC 仅支持 64 位系统，它把 64 位虚拟地址空间划分为多个子空间，如下图所示：</p><p><img src="imgs/gc-java-zgc-area.png" width="550px" /></p><p>其中，<code>[0~4TB)</code> 对应 Java 堆，<code>[4TB ~ 8TB)</code> 称为 <code>M0</code> 地址空间，<code>[8TB ~ 12TB)</code> 称为 <code>M1</code> 地址空间，<code>[12TB ~ 16TB)</code> 预留未使用，<code>[16TB ~ 20TB)</code> 称为 <code>Remapped</code> 空间。</p><p>当应用程序创建对象时，首先在堆空间申请一个虚拟地址，但该虚拟地址并不会映射到真正的物理地址。ZGC 同时会为该对象在 M0、M1 和 Remapped 地址空间分别申请一个虚拟地址，且这三个虚拟地址对应同一个物理地址，但这三个空间在同一时间有且只有一个空间有效。ZGC 之所以设置三个虚拟地址空间，是因为它使用“空间换时间”思想，去降低 GC 停顿时间。“空间换时间”中的空间是虚拟空间，而不是真正的物理空间。后续章节将详细介绍这三个空间的切换过程。</p><p>与上述地址空间划分相对应，ZGC 实际仅使用 64 位地址空间的第 0~41 位，而第 42~45 位存储元数据，第 47~63 位固定为 0。</p><p><img src="imgs/gc-java-zgc-pointer.png" width="550px" /></p><p>ZGC 将对象存活信息存储在 42~45 位中，这与传统的垃圾回收（例如 Reference Count）并将对象存活信息放在对象头中的策略是不相同的。</p><h4 id="Load-Barriers"><a href="#Load-Barriers" class="headerlink" title="Load Barriers"></a>Load Barriers</h4><blockquote><p>读屏障是 JVM 向应用代码插入一小段代码的技术。当应用线程从堆中读取对象引用时，就会执行这段代码。需要注意的是，仅“从堆中读取对象引用”才会触发这段代码。</p></blockquote><p>读屏障示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Object</span> <span class="variable">o</span> <span class="operator">=</span> obj.FieldA;    <span class="comment">// 从堆中读取引用，需要加入屏障</span></span><br><span class="line">&lt;Load barrier&gt;</span><br><span class="line"><span class="type">Object</span> <span class="variable">p</span> <span class="operator">=</span> o;            <span class="comment">// 无需加入屏障，因为不是从堆中读取引用</span></span><br><span class="line">o.dosomething();        <span class="comment">// 无需加入屏障，因为不是从堆中读取引用</span></span><br><span class="line"><span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span>  obj.FieldB;    <span class="comment">//无需加入屏障，因为不是对象引用</span></span><br></pre></td></tr></table></figure><p>ZGC 中读屏障的代码作用：在对象标记和转移过程中，用于确定对象的引用地址是否满足条件，并作出相应动作。</p><h2 id="8-11-Summary"><a href="#8-11-Summary" class="headerlink" title="8.11 Summary"></a>8.11 Summary</h2><p>总的来说，虽然垃圾收集器的技术在不断进步，但直到现在还没有最好的收集器出现，因为不存在“万能”的收集器，<u>只有适合某些使用场景的垃圾回收器</u>。</p><h1 id="Chapter-9-JDNI-amp-SPI"><a href="#Chapter-9-JDNI-amp-SPI" class="headerlink" title="Chapter 9. JDNI &amp; SPI"></a>Chapter 9. JDNI &amp; SPI</h1><p>Java Oracle Doc 一目了然：</p><p>Java Naming and Directory Interface (JNDI) 是一个应用程序编程接口 (API)，为使用 Java 编程语言编写的应用程序提供命名和目录功能。它的定义独立于任何特定的目录服务实现。因此，各种目录，无论新的、正在出现的和已经部署的，都可以用一种通用的方式访问。</p><p>而 JNDI 体系结构包括一个 API 和一个服务提供商接口 (SPI)。Java 应用程序使用 JNDI API 访问各种命名和目录服务。SPI 使各种命名和目录服务能以透明方式插入，从而允许使用 JNDI API 的 Java 应用程序访问它们的服务。</p><p><img src="imgs/jndi-arch.gif" /></p><p>补充：什么是 SPI？</p><p>SPI 即 Service Provider Interface ，字面意思就是：“服务提供者的接口”，我的理解是：专门提供给服务提供者或者扩展框架功能的开发者去使用的一个接口。</p><p>SPI 将服务接口和具体的服务实现分离开来，将服务调用方和服务实现者解耦，能够提升程序的扩展性、可维护性。修改或者替换服务实现并不需要修改调用方。</p><p>SPI 和 API 有什么区别吗？下面一个图就能弄清楚：</p><p><img src="imgs/spi-vs-api.png" width="350px" /></p><ul><li>当实现方提供了接口和实现，我们可以通过调用实现方的接口从而拥有实现方给我们提供的能力，这就是 <strong>API</strong>。这种情况下，接口和实现都是放在实现方的包中。调用方通过接口调用实现方的功能，而不需要关心具体的实现细节。</li><li>当接口存在于调用方这边时，这就是 <strong>SPI</strong> 。由接口调用方确定接口规则，然后由不同的厂商根据这个规则对这个接口进行实现，从而提供服务。</li></ul><p>补充：SPI 出现的原因是？</p><p>面向对象设计鼓励模块间基于接口而非具体实现编程，以降低模块间的耦合，遵循依赖倒置原则，并支持开闭原则（对扩展开放，对修改封闭）。然而，直接依赖具体实现会导致在替换实现时需要修改代码，违背了开闭原则。为了解决这个问题，SPI 应运而生，它提供了一种服务发现机制，允许在程序外部动态指定具体实现。这与控制反转（IoC）的思想相似，将组件装配的控制权移交给了程序之外（IoC 比较著名的例子就是 Spring Framework）。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Chapter-7-Java-Concurrent&quot;&gt;&lt;a href=&quot;#Chapter-7-Java-Concurrent&quot; class=&quot;headerlink&quot; title=&quot;Chapter 7. Java Concurrent&quot;&gt;&lt;/a&gt;Chapter 7.</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="Programming" scheme="https://blog.sjtuxhw.top/tags/Programming/"/>
    
    <category term="Java" scheme="https://blog.sjtuxhw.top/tags/Java/"/>
    
    <category term="GC" scheme="https://blog.sjtuxhw.top/tags/GC/"/>
    
    <category term="Concurrent" scheme="https://blog.sjtuxhw.top/tags/Concurrent/"/>
    
  </entry>
  
  <entry>
    <title>算法设计知识点自查表</title>
    <link href="https://blog.sjtuxhw.top/review/algo-desgin-table/"/>
    <id>https://blog.sjtuxhw.top/review/algo-desgin-table/</id>
    <published>2024-12-31T14:47:03.000Z</published>
    <updated>2025-01-18T15:03:36.201Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>四则运算、$\mathbf{Z}_N$ 下四则运算+幂运算复杂度；</p></li><li><p>欧几里得 GCD 递归复杂度证明；</p></li><li><p>拓展欧几里得算法求乘法逆元；</p></li><li><p>利用同余性质推算大数能否被整除；</p></li><li><p>费马小定理完整证明；</p></li><li><p>非 Carmichael 合数的费马测试证明；</p></li><li><p>对称加密、非对称加密、证书；</p></li><li><p>大师定理；</p></li><li><p>比较排序的时间下界证明（$n!$ 如何确定两边界？）；</p></li><li><p>快选算法在 25%-75% 判据下的时间复杂度；</p></li><li><p>矩阵算法、计数逆序（及拓展）时间复杂度推导、算法设计；</p></li><li><p>有向图中，有自环等价于存在回边的证明；</p></li><li><p>DAG 中，最大 post number 意味着源点、最小 post number 意味着汇点；</p><blockquote><p>证明 DAG 中，至少有一个源点和一个汇点；</p></blockquote></li><li><p>普通有向图中，最大 post number 意味着位于源点强连通部件内。但最小 post number 没有特性；</p></li><li><p>记忆：任何有向图的嵌图都是 DAG；</p></li><li><p>$G$ 中的源点强连通部件是 $G^R$ 中的汇点强连通部件；</p></li><li><p>证明：</p><ul><li><p>DFS explore 子过程若从 $u$ 开始，必然以 $u$ 及其所有可达结点遍历结束后结束（反证）；</p></li><li><p>$G$ 的两个不同的强连通部件 $C_1,C_2$，如果有从 $C_1$ 到 $C_2$ 的边，则 $C_1$ 的最大 post number 大于 $C_2$ 的最大 post number（不是所有），证明时讨论遍历开始的位置；</p></li><li>同第 14 条；</li></ul></li><li><p>线性时间查找有向图强连通部件算法；</p></li><li><p>问题：</p><ul><li>线性时间找到有向图中能到达其他所有顶点的点（如果不存在需要指出）；</li><li>线性时间判断无向图是否为二部图：证明使用着色法；</li><li>线性时间判断有向图是否存在奇数个顶点的环：<strong><u>每个强连通子图</u></strong>上 DFS 二染色。对回边（好理解）/ 前向边 / 交叉边（如果强连通部件中从 $u$ 到 $v$ 间有两条道路，并且这两条道路奇偶数不同，因为违反染色规则了所以不同。则必然存在一个奇数环和一个偶数环）违反规则的情况认为是含有奇数顶点的环；</li><li>线性时间找 DAG 中从指定顶点 $u$ 到 $v$ 的路径数目：从 $u$ DFS 遍历时，遇到无论什么边，只要是指向 $v$ 的，都让计数加一；</li><li>线性时间判断 DAG 中的 Hamilton 路径的存在性：修改线序化算法。每次检查是否有多于一个入度为 0 的结点，如果是就不存在。</li></ul></li><li><p>lemma：BFS 过程中，<strong>对每个 $d\in\mathbf{N}$，都总存在一个时刻，使得</strong>：</p><ul><li>所有与 $s$ 间距小于等于 $d$ 的顶点都被<strong><u>正确地</u></strong>设置了距离；</li><li>其他顶点的距离都还未被设置（$+\infty$）;</li><li>队列中只含有与 $s$ 间距为 $d$ 的顶点；</li></ul></li><li><p>分析 d-堆、二叉堆、数组作为 Dijkstra 算法的优先级队列时的时间复杂度情况；</p></li><li><p>Dijkstra 算法：</p><ul><li>所有结点初始化距离为 $\infty$，只有源点是 0；然后对所有结点构建优先级队列；</li><li>在队列不空时循环出队：<ul><li>出队 $u$ 时，更新相邻结点距离。如果更小就覆盖（此举会改变相邻点在优先级队列中的位置），并且维护最短路径列表 <code>pre[]</code>（如果需要）；</li></ul></li></ul><p>时间复杂度：$|V|$ 次删除、$|V|+|E|$ 次 <code>decreaseKey</code>；</p></li><li><p>证明 Dijkstra 算法正确性：归纳假设</p><p><img src="imgs/dijkstra-proof.png" width="250px" /></p><ul><li><p>预设：对于一个图 $G$，设定源点 $s$，希望测量的目标点 $v$，假定算法运行时已经设定距离（完整更新一轮邻居后的结果）的结点的集合为 $S$。下面对 $S$ 的大小与算法正确性归纳；</p></li><li><p>奠基：$|S|=1$ 的情况显然算法正确（$s$ 的最短距为 0）；</p></li><li><p>假设：假设对于 $|S|\ge1$ 的情况算法都是正确的；</p></li><li><p>归纳：进行下一轮邻居更新时，设更新到的结点 $v$ 将被加入 $S$，设 $(u,v)$ 是最后一条边。则 $s$ 至 $u$ 的最短路径一定位于 $S$ 中（算法正确性保证），记为 $dist[u]$，我们只需要证明 $dist[u]+l(u,v)$ 比其他任何 $s$ 到 $v$ 间的路径都短就行。</p><ul><li><p>假设 $(x,y)$ 是新一轮迭代中第一条边（最有可能更短），并且 $y$ 到 $v$ 右边。我们记 $s$ 到 $x$ 的路径为 $p^\prime$，则：</p><p>$l(p^\prime)+l(x,y)\ge dist[u]+l(x,y)\ge dist[y]\ge dist[v]$；</p><blockquote><p>注意到，如果存在负边权，$dist[y]\ge dist[v]$ 不再成立，因为可能 $l(y,v)$ 是个绝对值很大的负数。</p><p>这个时候如果需要更新 $v$，发现 $v$ 已经被弹出去了，没法更新了，因此 Dijkstra 算法在负权下不再是正确的了。</p></blockquote></li></ul></li></ul></li><li><p>Bellman-Ford 算法：</p><ul><li>从源点开始，<u>同时地更新一次</u>（不能把这轮更新的信息用来更新其他结点）每条边的两端点的距离信息；</li><li>重复上述动作 $|V|-1$ 次；</li></ul><p>时间复杂度 $O(|V||E|)$；</p></li><li><p>证明为何 Bellman-Ford 算法只需要重复更新 $|V|-1$ 次：</p><ul><li>因为从源点 $s$ 到任意一点 $t$ 的最短路径 $s\rightarrow v_1\rightarrow v_2\rightarrow\cdots\rightarrow v_k\rightarrow t$ 最多包含了 $|V|-1$ 个顶点（这条用反证法）。</li><li>更新了 $|V|-1$ 次后，即便再多的更新都是安全的（$\min$ 不会变大），并且不会缺漏（顶点不会移除）；</li></ul></li><li><p>如何检测负环？让 Bellman-Ford 算法在运行 $|V|-1$ 次后再进行一次，如果还有顶点的距离在变化，则说明有负环。</p></li><li><p>可以含负权的 DAGs 的单元最短路径算法：先拓扑排序，再按线序顺序更新结点的距离信息；</p><p>时间复杂度：$O(|V|)$；</p></li><li><p>问题：</p><ul><li><p>强连通图的某点 $v_0$ 通过的边全部是正权的。找任意两点间的最短路，要求必须经过 $v_0$；</p><blockquote><p>先求 $v_0$ 为源的到各点的最短距离，然后 $dist(u,v_0)+dist(v_0,v)$；</p></blockquote></li><li><p>无向正权图中有一条边 $e_0$，求含 $e_0$ 的最短环的长度；</p><blockquote><p>设 $e_0=(u,v)$，在 $G-e_0$ 中执行 Dijkstra 算法找到 $u$ 到 $v$ 的最短距离，加上 $l(e_0)$ 即可；</p></blockquote></li><li><p>有向正权图的最短环？</p><blockquote><p>环是由 $u$ 到 $v$ 和 $v$ 到 $u$ 的路径拼接而成。</p><p>先求每两个点间的最短路径（数组数据结构 $O(|V|^3)$），在遍历查找 $dist(u,v)+dist(v,u)$ 最小值；</p></blockquote></li><li><p>有向正权图中最简最长路径？</p><blockquote><p>NP Hard 问题；可以将 Hamilton 图问题规约到这个问题。</p></blockquote></li><li><p>无向正权图的最短环？</p><blockquote><p>遍历所有边，每轮循环时删除当前边，并且选取一端运行 Dijkstra 算法。如果另一端是可达的（不是 $\infty$，记为 $c_i$），那么说明原图中存在一个长度为 $c_i+l(e)$ 的环。遍历找长度最小的那个。</p></blockquote></li><li><p>无向等权图中指定两点间最短路径数量？</p><blockquote><p>BFS 使用两个队列。类似 copy collection，最终在队列 1 中找到与 $u$ 相邻的结点终止。然后统计队列中还有哪些与 $u$ 相邻的结点。其数量即为所求。</p><p>这个方法的原理是利用 BFS 的性质（参见第 20 条 lemma）：在每轮结束后队列中放的是相同长度的结点（分开放能确保下一轮的结点不会混入当前队列中）。</p></blockquote></li></ul></li><li><p>证明 $|V|-1=|E|$ 的<u>无向连通图</u>等价于树数据结构；</p><ul><li><p>必要性：归纳。对顶点归纳的话，就是 $k$ 个顶点推 $k+1$ 个；对边归纳的话，就是向 $n$ 个顶点中加边看连通部件数是否减为 1；</p></li><li><p>充分性：反证。假设不是树，则必然存在环，重复去环得到 $|E^\prime|\lt|E|$，由必要性可知 $|E^\prime|=|V|-1=|E|$，矛盾；</p></li></ul></li><li><p>证明一个无向图是一棵树 当且仅当 任意两结点间有一条唯一路径；</p><ul><li>充分性：反证。如果不是就有环，违反定义；</li><li>必要性：先判断是连通图，再反证。假设存在环，则存在两个结点间路径不唯一，矛盾；</li></ul></li><li><p>Kruskal：向顶点中加尽可能小的边，成环就舍去。直至加够 $|V|-1$ 条；</p></li><li><p>割定理：割集间的最短边一定在最小生成树中：分类讨论 + 反证法；</p></li><li><p>Prim：顶点割集 + 顶点的策略。正确性由割定理保证；</p></li><li><p>上色算法求 MST：破圈（染红没有红边的环）、闭圈（染蓝没有蓝边的割边集合中最短边）；</p></li><li><p>证明贪婪算法作为近似算法求解 set cover 问题的近似比为 $\ln n$（设 $n_t$ 是 $t$ 轮选取后剩下的顶点数）；</p></li><li><p>01 背包和完全背包问题的状态转移方程、边界条件（出口）、时空复杂度；</p></li><li><p>多重背包问题（不含二进制优化）算法设计以及时间复杂度分析；</p></li><li><p>硬币找零问题最优解算法；</p></li><li><p>子序列系列问题：</p><ul><li>最长递增子序列：构造 DAG 再动归边；</li><li>最长公共子序列、最长公共子串；</li><li>分治法和动态规划分别求解 最大连续子序列（即子串）和（为什么讨论以 $a_i$ 结尾的子串？子串更适合这种方法！）；</li></ul></li><li><p>三分问题的状态转移方程；</p></li><li><p>Shortest Reliable Path Lite：经过不超过指定边数的最短路径问题；</p><blockquote><p>和 Shortest Reliable Path 的 NP-Complete 问题不一样；</p><p>详情参见第 50 条；</p></blockquote></li><li><p>矩阵乘法最优分配：化归为二叉树，动归二叉树代价；</p></li><li><p>动态规划尝试解决 TSP 问题算法；</p></li><li><p>线性时间找出树的 independent set 大小：分类讨论，孙子结点可以在、儿子结点只能替代；</p></li><li><p>棋子放置问题：模式、相容 与动态规划；</p></li><li><p>DP 难题：</p><p><img src="imgs/dp-nb.png" /></p><p>假设 $M[i,j,p,q]$ 表示对子串 $s[i]\sim s[j]$ 分割，涉及分割点在数组中 $d[p]\sim d[q]$，分割后的最小代价；</p><p>状态转移方程：$M[i,j,p,q]=j-i+1+\min\limits_{p\le k\le q}\{M[i,d[k],p,k]+M[d[k]+1,j,k+1,q]\}$；</p><p>算法复杂度 $O(m^2n^2)$；</p><p><img src="imgs/dp-nb2.png" /></p><p>设在 $x\times y$ 的布料上制作前 $i$ 个产品的最大价值为 $P(i,x,y)$，则状态转移方程为：</p><script type="math/tex; mode=display">\left\{\begin{aligned}P_h(i,x,y)&=\max\{P(i-1,x,y),c_i+P(i,x-a_i,y)+P(i,x,y-b_i)+P(i,x-a_i,y-b_i)\}\\P_v(i,x,y)&=\max\{P(i-1,x,y),c_i+P(i,x-b_i,y)+P(i,x,y-a_i)+P(u,x-b_i,y-a_i)\}\\P(i,x,y)&=\max\{P_h(i,x,y),P_v(i,x,y)\}\end{aligned}\right.</script><p>主要注意到布料是可以旋转使用的！因此分为旋转后再裁剪（$P_v$）和不旋转直接裁剪（$P_h$）两种手段。</p></li><li><p>规范型线性规划转对偶式；</p></li><li><p>单纯形算法时间复杂度推导 $O(n(m+n)C_{m+n}^n)$；</p></li><li><p>单纯形算法求解多元线性规划；</p></li><li><p>最大流问题变式：</p><ul><li>含有多个源点和汇点的最大流问题：定义新的源点和汇点；</li><li>每个顶点有流量限制：将每个顶点拆成两个，这两个顶点间用一条有向边连接，容量就是限制。转换成普通最大流问题；</li><li>每条边在容量的基础上添加限制，要求流量不得低于某个限制。或者某些节点间存在流量损耗：直接规约成一般线性规划问题；</li></ul></li><li><p>瓶颈边（增大会导致总流增大）、临界边（减小会导致总流减小）：注意它们俩是不一样的！！</p></li><li><p>查找临界边：</p><ul><li><p>剩余图中只有反向边的；</p></li><li><p>但也不全是。可能某条在最大流中流满的边，其容量下降后，最大流中缺少的流量可以从其他边的流量得到补充。通过在剩余图中 DFS 能通过的这些反向边就不是临界边，排除它们即可。</p></li></ul></li><li><p>证明：二部图的最小顶点覆盖能够被规约为最大流问题。</p></li><li><p>边不相交问题（化归为最大流问题，需要证明）；</p></li><li><p>常见 NP-Complete 问题规约：</p><ul><li>稠密子图问题（a 个顶点中至少 b 条边）：$b=\dfrac{a(a-1)}{2}$，从最大团问题规约；</li><li>稀疏子图问题（a 个顶点中至多 b 条边）：$b = 0$，从最大独立集问题规约；</li><li>集合覆盖问题：从顶点覆盖规约（子问题，一个元素只会同时存在于两个集合中）；</li><li>子图同构问题（G 能否只通过删除一些点、边，修改名称来得到 H）：从哈密顿回路规约（构造图 G 是一个 N 元环，N 为输入图的顶点数）；</li><li>最大公共子图（同时删除 G 和 H 一些点和对应边，使二者相同，预算 b）：从最大独立集规约；</li><li>哈密顿回路、哈密顿道路相互规约；</li><li>最长初级路径（simple longest path）：从哈密顿路径规约；</li><li>K-Coloring 问题：从 3-SAT 问题规约（构建 OR-gadget 和 Clause gadget）；</li><li>可靠网络问题（shortest reliable path，给定顶点、距离矩阵(边权)、连接需求(重边数量)、预算，构建一个总代价不超过预算、两点间的不同路径数等于对应连接需求的图）：从 TSP 问题规约（预算 $b=n$，距离矩阵全为 1 或 2，表示有边和没有边，连接需求全为 2，表示在环上）；</li></ul></li><li><p>近似算法解决 NP 问题：</p><ul><li>贪婪算法求解集合覆盖问题。近似比 $ln n$；</li><li>极大匹配算法（贪婪选取尽可能多的 endpoints 的边，然后删除共享 endpoints 的边）求解顶点覆盖问题。近似比 2；</li><li>贪婪算法求解 k-clustering 问题：先任取一个点作为第一个中心，然后每次取离当前点最远的，中垂线分割。近似比为 2；</li><li>MST + skip 近似 metric-space-TSP 问题：近似大小不大于最小生成树长度两倍。而最优解不不小于最小生成树长度。因此近似比为 2；</li><li>rescale value 近似背包问题：缩小 $\hat{v_i}=\dfrac{n}{\varepsilon v_{\max}}$，总价值 $\sum v_i\ge\sum\hat{v_i}=K^\star(1-\varepsilon)$，即近似比为用户指定的任意接近；</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;&lt;p&gt;四则运算、$&#92;mathbf{Z}_N$ 下四则运算+幂运算复杂度；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;欧几里得 GCD 递归复杂度证明；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;拓展欧几里得算法求乘法逆元；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;利用同余性质推算大数</summary>
      
    
    
    
    <category term="review" scheme="https://blog.sjtuxhw.top/categories/review/"/>
    
    
    <category term="Programming" scheme="https://blog.sjtuxhw.top/tags/Programming/"/>
    
    <category term="Algorithms" scheme="https://blog.sjtuxhw.top/tags/Algorithms/"/>
    
  </entry>
  
  <entry>
    <title>Redis 入门：从实践到理论</title>
    <link href="https://blog.sjtuxhw.top/technical/redis-starter/"/>
    <id>https://blog.sjtuxhw.top/technical/redis-starter/</id>
    <published>2024-11-12T13:05:37.000Z</published>
    <updated>2024-11-14T13:13:31.049Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Chapter-1-基本概念和-CLI-使用"><a href="#Chapter-1-基本概念和-CLI-使用" class="headerlink" title="Chapter 1. 基本概念和 CLI 使用"></a>Chapter 1. 基本概念和 CLI 使用</h1><h2 id="1-1-NoSQL"><a href="#1-1-NoSQL" class="headerlink" title="1.1 NoSQL"></a>1.1 NoSQL</h2><p>用于存储非结构化数据，不保证 ACID 事务特性（仅有最终一致性 Weak Consistency Model）。</p><p>Redis（Remote Dictionary Server）就是一类基于内存的键值型 NoSQL，不保证数据一致性，但可以保证性能。</p><ul><li><p>一种 KVStore System，可以方便的存放非结构化数据，这对于缓存各异性数据非常有帮助；</p></li><li><p>Handle 网络请求多线程。处理指令单线程，单个指令具有原子性；</p></li><li>低延迟，利用 I/O Multiplexing 在单线程中处理多个请求；</li><li>支持数据持久化；</li><li>支持主从集群（从备份，读写分离）和分片集群（数据拆分，存储上限提高）；</li></ul><h2 id="1-2-Redis-Data-Structure"><a href="#1-2-Redis-Data-Structure" class="headerlink" title="1.2 Redis Data Structure"></a>1.2 Redis Data Structure</h2><p>Redis Key 一般使用 String，Value 支持：</p><ul><li>基本类型：<code>String</code>、<code>Hash</code>、<code>List</code>、<code>Set</code>、<code>SortedSet</code>；</li><li>特殊类型：<code>GEO</code>（地理位置信息格式）、<code>BitMap</code>（位图）、<code>HyperLog</code>；</li></ul><h2 id="1-3-Basic-Redis-CLI-Commands"><a href="#1-3-Basic-Redis-CLI-Commands" class="headerlink" title="1.3 Basic Redis CLI Commands"></a>1.3 Basic Redis CLI Commands</h2><h3 id="1-3-1-General"><a href="#1-3-1-General" class="headerlink" title="1.3.1 General"></a>1.3.1 General</h3><p>命令行指令较多，建议查询官方文档而不是背诵。介绍常见的几个（General）：</p><ul><li><code>KEYS &lt;pattern&gt;</code>：查询 Key 符合 pattern（R.E.）的键值对；</li><li><code>DEL/EXISTS &lt;KEY&gt;</code>：删除、判断存在性；</li><li><code>EXPIRE/TTL &lt;KEY&gt; [sec]</code>：设置/获取键值的有效期（<code>-1</code> 为永久、<code>-2</code> 为已过期）；</li></ul><h3 id="1-3-2-String-int-float"><a href="#1-3-2-String-int-float" class="headerlink" title="1.3.2 String (+int/float)"></a>1.3.2 String (+int/float)</h3><ul><li><code>SET/MSET &lt;KEY&gt; &lt;VAL&gt;[...(KEY, VAL)]</code>：设置/批量设置键值；</li><li><code>GET/MGET &lt;KEY&gt;[...KEY]</code>：获取/批量获取；</li><li><code>INCR/INCRBY &lt;KEY&gt; [STEP]</code>：让存储数值型 String 的 Value 自增 1 或 <code>STEP</code>；</li><li><code>INCRBYFLOAT &lt;KEY&gt; [incr]</code>：让存储浮点型 String 的 Value 增长指定值；</li><li><code>SETNX &lt;KEY&gt; &lt;VAL&gt;</code>：仅不存在才插入（决不更改已存在的数据）；</li><li><code>SETEX &lt;KEY&gt; &lt;sec&gt; &lt;VAL&gt;</code>：设置并指定有效期；</li></ul><h3 id="1-3-3-The-Hierarchy-Structure-of-Redis-Key"><a href="#1-3-3-The-Hierarchy-Structure-of-Redis-Key" class="headerlink" title="1.3.3 The Hierarchy Structure of Redis Key"></a>1.3.3 The Hierarchy Structure of Redis Key</h3><p>Redis Key 允许使用多个单词形成层级结构，常用格式为 <code>&lt;PROJECT_NAME&gt;:&lt;BUSSINESS_NAME&gt;:[TYPE]:&lt;id&gt;</code>；</p><h3 id="1-3-4-Hash"><a href="#1-3-4-Hash" class="headerlink" title="1.3.4 Hash"></a>1.3.4 Hash</h3><p>String 结构存储时，想要修改其中某个字段不方便。</p><p>现在引入 Hash 数据类型，其 <code>value</code> 作为一个无序字典（多了一层 field-value 关系），类似一个 <code>HashMap</code>；</p><ul><li><code>HSET/HGET &lt;KEY&gt; &lt;FIELD&gt; [VAL]</code></li><li><code>HMSET / HMGET</code></li><li><code>HGETALL &lt;KEY&gt;</code>：获取这个键的 value 中的所有 field 的值；</li><li><code>HKEYS</code>：获取这个键的所有 field；</li><li><code>HVALS</code>：获取这个键的所有 value；</li><li><code>HSETNX &lt;KEY&gt; &lt;sec&gt; &lt;FIELD&gt; &lt;VAL&gt;</code>：仅不存在才插入；</li></ul><h3 id="1-3-5-List"><a href="#1-3-5-List" class="headerlink" title="1.3.5 List"></a>1.3.5 List</h3><p>Redis 中的 value 类型为列表，与双向链表很相似（同时支持正向、反向索引）。特性：</p><ul><li>有序、可重复、增删操作快、查询速度一般；</li></ul><p>操作如下：</p><ul><li><code>LPUSH/RPUSH &lt;KEY&gt; &lt;ELEMENT&gt;[...ELEMENT]</code>：从列表左侧/右侧插入；</li><li><code>LPOP/RPOP &lt;KEY&gt;</code>：弹出；</li><li><code>LRANGE &lt;KEY&gt; &lt;START&gt; &lt;END&gt;</code>：获取列表中的角标范围中所有元素；</li><li><code>BLPOP / BRPOP &lt;KEY&gt;[...KEY] &lt;sec&gt;</code>：和 <code>LPOP/RPOP</code> 类似，但是在没有元素时等待一段时间，而不是直接返回 <code>NIL</code>；</li></ul><h3 id="1-3-6-Set"><a href="#1-3-6-Set" class="headerlink" title="1.3.6 Set"></a>1.3.6 Set</h3><p>Redis 中的 value 类型为集合，与 <code>HashSet</code> 类似。特性：</p><ul><li>无序、元素不可重复、查找快、支持交并差集操作；</li></ul><p>操作如下：</p><ul><li><code>SADD/SREM &lt;KEY&gt; &lt;MEMBER&gt;[...MEMBER]</code>；添加/移除集合中的若干元素；</li><li><code>SCARD &lt;KEY&gt;</code>：获取集合中元素个数；</li><li><code>SISMEMBER &lt;KEY&gt; &lt;MEMBER&gt;</code>：是否在集合内；</li><li><code>SMEMBERS &lt;KEY&gt;</code>：获取集合中所有元素；</li></ul><h3 id="1-3-7-SortedSet"><a href="#1-3-7-SortedSet" class="headerlink" title="1.3.7 SortedSet"></a>1.3.7 SortedSet</h3><p>Redis 中 value 类型为有序集，每个元素携带 <code>score</code> 值（相当于优先级），可以基于 <code>score</code> 排序。</p><p>其底层基于 SkipTable + HashTable 实现。特性如下：</p><ul><li>可排序、元素不重复、查询速度快；</li></ul><p>操作如下：</p><ul><li><code>ZADD &lt;KEY&gt; &lt;score&gt; &lt;MEMBER&gt;</code>：添加一个或多个元素到 sorted set，如果已经存在则更新 <code>score</code> 值；</li><li><code>ZREM &lt;KEY&gt; &lt;MEMBER&gt;</code>：移除一个指定元素；</li><li><code>ZSCORE/ZRANK &lt;KEY&gt; &lt;MEMBER&gt;</code>：获取指定元素 score 值 / 排名；</li><li><p><code>ZCARD &lt;KEY&gt;</code>：获取元素个数；</p></li><li><p><code>ZCOUNT/ZRANGEBYSCORE &lt;KEY&gt; &lt;MIN_SCORE&gt; &lt;MAX_SCORE&gt;</code>：统计 score 值在指定范围内的元素个数 / 元素值；</p></li><li><code>ZRANGE &lt;KEY&gt; &lt;MIN_RANK&gt; &lt;MAX_RANL&gt;</code>：获取指定排名区间内的元素；</li><li><code>ZDIFF / ZINTER / ZUNION</code>：求差集、交集、并集；</li></ul><h1 id="Chapter-2-Using-Redis-With-Spring-Boot"><a href="#Chapter-2-Using-Redis-With-Spring-Boot" class="headerlink" title="Chapter 2. Using Redis With Spring Boot"></a>Chapter 2. Using Redis With Spring Boot</h1><h2 id="2-1-Basic-Usages"><a href="#2-1-Basic-Usages" class="headerlink" title="2.1 Basic Usages"></a>2.1 Basic Usages</h2><p>引入 Redis Client for Java 的 Spring Boot 依赖：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dependencies &#123;</span><br><span class="line">    implementation <span class="string">&#x27;org.springframework.boot:spring-boot-starter-data-redis&#x27;</span></span><br><span class="line">    <span class="comment">// 连接池实现</span></span><br><span class="line">    implementation <span class="string">&#x27;org.apache.commons:commons-pool2&#x27;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置 Redis 连接信息：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">    <span class="attr">redis:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">...</span></span><br><span class="line">        <span class="attr">post:</span> <span class="string">...</span></span><br><span class="line">        <span class="attr">password:</span> <span class="string">...</span></span><br><span class="line">        <span class="comment"># 使用 lettuce 而非 jedis 实现</span></span><br><span class="line">        <span class="attr">lettuce:</span></span><br><span class="line">            <span class="attr">pool:</span></span><br><span class="line">                <span class="attr">max-active:</span> <span class="number">8</span><span class="comment"># 最大连接数</span></span><br><span class="line">                <span class="attr">max-idle:</span> <span class="number">8</span><span class="comment"># 最大空闲连接</span></span><br><span class="line">                <span class="attr">min-idle:</span> <span class="number">0</span><span class="comment"># 最小空闲连接</span></span><br><span class="line">                <span class="attr">max-wait:</span> <span class="string">100ms</span><span class="comment"># 最大连接等待时长</span></span><br></pre></td></tr></table></figure><p><strong>使用时自动装配 <code>RedisTemplate</code> 类型</strong>。</p><p>使用 <code>RedisTemplate</code> 的接口：</p><ul><li><p><code>RedisTemplate#opsForValue(Object key, Object val, [long timeout, TimeUnit unit])</code>：获取 Java 一般 Object 类型的操作，这个方法会将 <code>key, val</code> 全部序列化后存储。</p><blockquote><p>默认 <code>JdkSerializationRedisSerializer</code> 类型作为序列化器，底层使用 <code>ObjectOutputStream#writeObject</code>  完成序列化工作。</p><p>缺点：可读性差、默认序列化器的内存占用大（消息队列中的默认序列化器有同样问题）；</p><p>我们应该少用这种方法，尽量选择下面确定类型的方法。</p><p>还有一种方法是，自定义 Redis 的序列化方式。</p></blockquote></li><li><p><code>RedisTemplate#opsForXXX()</code>：返回 Spring Data Redis 对于指定数据类型 <code>XXX</code>（String/Hash/…）的可能操作集合 <code>XXXOperations</code>；</p></li></ul><p><code>XXXOperations</code> 类的对象可以完成对应类型的 <code>set / get</code> 等方法，并选用合适的序列化器进行存储处理。</p><h2 id="2-2-Self-defined-Serializer-for-Redis"><a href="#2-2-Self-defined-Serializer-for-Redis" class="headerlink" title="2.2 Self-defined Serializer for Redis"></a>2.2 Self-defined Serializer for Redis</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="keyword">public</span> RedisTemplate&lt;String, Object&gt; <span class="title function_">redisTemplate</span><span class="params">(RedisConnectionFactory redisConnectionFactory)</span> &#123;</span><br><span class="line">    <span class="comment">/* Create empty template */</span></span><br><span class="line">    RedisTemplate&lt;String, Object&gt; template = <span class="keyword">new</span> <span class="title class_">RedisTemplate</span>&lt;&gt;();</span><br><span class="line">    <span class="comment">/* Configure connection factory */</span></span><br><span class="line">    template.setConnectionFactory(redisConnectionFactory);</span><br><span class="line">    <span class="comment">/* Set Serializer for POJO */</span></span><br><span class="line">    <span class="type">GenericJackson2JsonRedisSerializer</span> <span class="variable">jsonRedisSerializer</span></span><br><span class="line">            <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">GenericJackson2JsonRedisSerializer</span>();</span><br><span class="line">    <span class="comment">/* Use String serializer for key &amp; hash key (instead of Jdk serializer) */</span></span><br><span class="line">    template.setKeySerializer(RedisSerializer.string());</span><br><span class="line">    template.setHashKeySerializer(RedisSerializer.string());</span><br><span class="line">    <span class="comment">/* Use JSON serializer for value &amp; hash value (instead of Jdk serializer) */</span></span><br><span class="line">    template.setValueSerializer(jsonRedisSerializer);</span><br><span class="line">    template.setHashValueSerializer(jsonRedisSerializer);</span><br><span class="line">    <span class="keyword">return</span> template;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有个问题，<code>GenericJackson2JsonRedisSerializer</code> 对一类数据会反复保存 <code>@class</code> 字段（反序列化后的类型）。这个信息虽然是必要的，但很多情况下存储 <code>@class</code> 字段甚至比原数据还要占空间！</p><p>因此我们一般不会直接使用 <code>JSON</code> 序列化器来序列化我们的 POJO，而是我们开发者<strong><u>对一个层级下的键都约定一个数据类型</u></strong>，然后使用 <strong><u>String 序列化器</u></strong>，最终手动序列化和反序列化，那么可以节省这部分空间。</p><p>Spring 中已经帮我们包装好了序列化、反序列化全是 String 的 <code>RedisTemplate</code>，省得我们配置了，它就是 <code>StringRedisTemplate</code>；</p><blockquote><p>也就是 <code>RedisTemplate&lt;String, Object&gt;</code>，并且设置好全部都是 String 的序列化、反序列化器；</p></blockquote><h1 id="Chapter-3-内存型数据库理论"><a href="#Chapter-3-内存型数据库理论" class="headerlink" title="Chapter 3. 内存型数据库理论"></a>Chapter 3. 内存型数据库理论</h1><p>本章介绍一下以 Redis 为首的 NoSQL 内存型数据库产生、发展的历程，同时介绍其存在的问题以及解决方案。</p><h2 id="3-1-为什么需要内存型数据库？"><a href="#3-1-为什么需要内存型数据库？" class="headerlink" title="3.1 为什么需要内存型数据库？"></a>3.1 为什么需要内存型数据库？</h2><p>持久化在磁盘上的关系型数据库在存储关系数据、处理事务的多数场合下都非常得力，但免不了存在一些问题。</p><p>例如，在电商、文章档案等网页应用中，常常是读请求远多于写请求，即便 MySQL 有 cache buffer pool（InnoDB），在大量数据查询的场合下也会出现频繁的 cache evict，究其原因就是 cache working space 太小了。</p><p>人们发现只是读请求造成的 Disk I/O 是可以避免的——通过将数据托管到一个更大的内存空间（这段内存空间可以不连续、甚至可以不在单个物理节点上，由一个程序来管理它）中缓存起来，可以有效提升这些应用的处理效率和吞吐量。</p><p>结论 1：<strong><u>在庞大数据量的应用场景下，读多写少、数据时间局部性强的应用访问模式可以通过外置的内存缓冲区统一进行缓存，来提升整体性能和接口承载量</u></strong>。这就是 Redis 要解决的需求痛点。</p><h2 id="3-2-缓存策略"><a href="#3-2-缓存策略" class="headerlink" title="3.2 缓存策略"></a>3.2 缓存策略</h2><p>在确定使用内存型数据库作为持久化的关系型数据库的缓存后，接下来，和所有缓存机制一样（不仅仅是数据库领域），内存型数据库会遇到两个问题：</p><ol><li>采取什么样的读/写缓存策略更好？</li><li>当缓冲区占满后，evict 的策略是什么？</li></ol><h3 id="3-2-1-缓存读写策略-和-缓存模式"><a href="#3-2-1-缓存读写策略-和-缓存模式" class="headerlink" title="3.2.1 缓存读写策略 和 缓存模式"></a>3.2.1 缓存读写策略 和 缓存模式</h3><p>对于第一个问题，区分缓存读策略和缓存写策略。</p><p><strong>缓存读策略</strong>：大多数情况下比较显然：</p><ul><li>如果 read cache miss，一定需要从磁盘上读数据，顺带写回缓存；</li><li>如果 read cache hit，则可以考虑记录数据热点情况；</li></ul><p><strong>缓存写策略</strong>：主要会有 4 类策略：</p><ul><li>如果 write cache hit，可以：<ul><li><strong>Write-through</strong>：立即将数据写入缓存（即覆盖当前行）并主动刷新（flush）到磁盘；</li><li><strong>Write-back</strong>：先把数据写入缓存，但不立即刷新，直到下一个数据要覆盖这个数据行的时候，才更新到磁盘中（defer write to disk until replacement of line，<strong>只是尽可能推迟了写入磁盘的时间</strong>）。另外，这个方案需要额外维护 dirty bit 来指示是否与磁盘数据一致；</li></ul></li><li>如果 write cache miss，可以：<ul><li><strong>Write-allocate</strong>：写分配，在 write-miss 后，<strong>先将原数据从磁盘读入缓存，转换为 write-hit 的情况，再 write-back（仅修改缓存 + dirty bit）</strong>；</li><li><strong>No-write-allocate</strong>：直接写入磁盘，不加载到缓存（缓存中没有这个数据所在的数据行，因为本来就是 write-miss）；</li></ul></li></ul><p>显然，在 write cache hit 情况下，write-through 和 write-back 策略的优劣势互补：前者保证内存和磁盘的数据较强的一致性，但是同步写回操作免不了降低了操作性能；后者接受了一定程度上的数据不一致性（推迟刷盘时间），换取了短时间内的高并发性能。</p><p>write-allocate 和 no-write-allocate 之间的优劣势和 write-through、write-back 的优劣势的对比相同，因此人们常常根据实际情况选择 “<strong>write-back + write-allocate</strong>” 或 “write-through + no-write-allocate” 的策略中的其中一对。</p><p>上述读写策略的不同选择，就形成了以 Redis 为首的内存数据库的 3 个主流的<strong><u>宏观缓存模式</u></strong>，每种模式可以应对一些使用场景：</p><ul><li><p>旁路缓存模式（Cache Aside Pattern）：同时维护数据库、缓存，二者中的数据存在强一致性；</p><ul><li><p>read：使用上面统一的缓存读策略；</p></li><li><p>write：不存在 write cache hit + no-write-allocate。立即写回数据库，并拒绝缓存。清空写这个数据的缓存信息（使用不缓存手段消除数据不一致性，<strong><u>注意保证顺序先更新磁盘再删除缓存</u></strong>）；</p><blockquote><p>为什么不采用上述的写缓存策略，而是拒绝缓存？因为考虑到<strong><u>多次盲写</u></strong>的问题。</p></blockquote></li></ul></li><li><p>读写穿透模式（Read/Write Through Pattern）：视缓存为主要存储手段，二者中的数据也存在强一致性；</p><ul><li>read：使用上面统一的读策略；</li><li>write：<strong>write-through + write-allocate</strong>；</li></ul></li><li><p>异步缓存写入模式（Write Behind Pattern）：针对读写穿透模式的改进，牺牲一部分数据一致性换取更高的吞吐量；</p><ul><li>read：使用上面统一的读策略；</li><li>write：write-back（优化，不使用 dirty-bit，而是异步更新到数据库）；</li></ul></li></ul><blockquote><p>后文将以如何实现读写穿透模式为例，展示代码，同时加入缓存击穿和缓存雪崩等等问题的应对措施。代码将在文末以附录形式呈现。</p></blockquote><p>结论 2：<strong><u>常用的缓存读写策略有很多种，不过依赖它们制定的缓存模式常见的有 3 种，分别是 旁路缓存、读写穿透、异步缓存</u></strong>；</p><h3 id="3-2-2-缓存-Evict-策略：以-Redis-为例"><a href="#3-2-2-缓存-Evict-策略：以-Redis-为例" class="headerlink" title="3.2.2 缓存 Evict 策略：以 Redis 为例"></a>3.2.2 缓存 Evict 策略：以 Redis 为例</h3><p>不同内存型数据库的缓存淘汰策略不尽相同。下面以 Redis 为例介绍它的 cache evict 方案：</p><p>首先，Redis 正常不会主动 evict 数据项，而是先通过数据过期的方式腾出内存空间：</p><ul><li>过期时间：对每个数据项可以设置 TTL（Time-To-Live），表示数据过期时间。过期的数据自动被清空；</li><li>定期清理：Redis 可以配置扫描过期数据的频率，扫描过程称为 Garbage Collection（GC）；</li><li>随机选取：由于 Redis 管理的缓冲区很大，因此每次 GC 一般不会扫描全表，而是随机选取一部分进行回收；</li><li>惰性删除：某些键值可能概率原因一直无法被选中删除，因此一旦有查询找到该数据，发现该数据过期后立即删除（被动）；</li></ul><p>在此基础上，如果：</p><ul><li>有些键值始终没被查询，且一直没有被随机选取清理（躲过了定期清理和惰性删除）；</li><li>过多的键值没有设置过期时间；</li><li>数据工作集（working set）进一步增大；</li></ul><p>导致内存空间还是没法及时腾出，那么 Redis 就会采取主动 evict 的方案。</p><p>Redis 主动进行 cache evict 时可以配置 8 种策略（注意，下面的策略都是 “没办法通过数据过期腾出空间” 的主动举措）：</p><ul><li><code>noneviction</code>：缓冲区占满后报错，不会删除任何键值；</li><li><code>allkeys-lru</code>：对所有缓存键使用 LRU 策略 evict；</li><li><code>volatile-lru</code>：从设置了过期时间的数据（不一定过期了）中使用 LRU 策略 evict；</li><li><code>allkeys/volatile-random</code>：对所有缓存键/设置了过期时间的缓存记录使用随机策略 evict；</li><li><code>volatile-ttl</code>：从设置过期时间的缓存记录中选择剩余存活时间最少的记录 evict；</li><li><code>allkeys/volatile-lfu</code>：从所有缓存键/设置了过期时间的缓存记录使用 LFU 策略 evict；</li></ul><blockquote><p>LRU:  Least Recently Used；</p><p>LFU:  Least Frequently Used；</p></blockquote><p>结论 3:   <strong><u>Redis 对于缓存使用率过高的解决方案是 数据过期 + 主动 evict。其中数据过期依赖 “定时清理” 和 “惰性删除”，主动 evict 依赖 8 种 evict 策略</u></strong>。</p><h2 id="3-3-缓存击穿-amp-缓存雪崩-Cache-Penetration-Avalanche"><a href="#3-3-缓存击穿-amp-缓存雪崩-Cache-Penetration-Avalanche" class="headerlink" title="3.3 缓存击穿 &amp; 缓存雪崩 Cache Penetration/Avalanche"></a>3.3 缓存击穿 &amp; 缓存雪崩 Cache Penetration/Avalanche</h2><p>注意到以上方案，从缓存读写策略、缓存模式，到缓存 evict 策略，全部都没有考虑到一个问题，或者说一类独特的访问 pattern：如果<strong><u>一直查询并不存在的数据</u></strong>会发生什么。</p><p>无论按照上面的哪种策略，都会频繁出现 “cache miss - 查找数据库 - 数据库未找到” 这个流程，这会频繁绕过缓存，增大数据库的 disk I/O，影响正常业务逻辑的时延和吞吐量，尤其是大量的 Client 查找一个并不存在的数据的时候，性能影响更为明显。这种现象被称为 “缓存击穿”。</p><p>为了解决缓存击穿的问题，可行的解决方案之一是：根据具体业务逻辑指定一个无效值（Invalid Value），一旦出现一次 read cache miss 并且发现数据库未找到的情况，可以在缓存中写入这个无效值，下次 read cache hit 就知道数据库中没有了。</p><blockquote><p>这种解决方案借鉴了 Bloom Filter 的设计思想。</p><p>Google 的一篇论文曾经介绍使用跳表 + Bloom Filter 为 Log-Structure Merged Tree 提升查询速度。</p></blockquote><p>但是，除了 “一直查询并不存在的数据”，还有一类情况会引发缓存击穿：<strong><u>某个热点数据过期被清理</u></strong>。</p><p>在过期后的短时间内，没有等上缓存恢复就出现大量的对该数据的并发请求。这些请求会 cache miss 并 fallback 到数据库，造成上述问题。</p><p>如果更严重一点，假设<strong><u>一批热点数据同时过期</u></strong>，也就是大批数据出现 cache miss，那么大量请求可能造成拒绝查询甚至宕机的后果，这种现象被称为 “缓存雪崩”。</p><p>解决这类缓存击穿，以及缓存雪崩的方案之一，无非是：</p><ul><li>延长热点数据的 TTL（比如每次访问时增加 TTL），或者热点数据永久驻留缓存；</li><li>批量设置缓存时，在一定时间范围内随机指定 TTL（例如 10~30 分钟内的均匀分布）；</li></ul><p>还有一种情况，如果 Redis 出现宕机，内存缓存数据全部丢失，也会出现缓存雪崩的问题，这个时候仅靠以上的应对方案已经不足以解决了。我们需要对 Redis 中的数据进行适当的持久化（“适当” 指同时保证性能）来尽量避免这个情况。</p><p>结论 4：<strong><u>“一直查询不存在的数据” 或者 “某个热点数据被清理” 都会造成缓存击穿、“一批热点数据同时过期”、“内存数据库宕机” 都可能造成缓存雪崩。对应的解决方案是 “添加无效值缓存”、“延长热点数据 TTL”、“随机化批量缓存 TTL”，以及 “适当的缓存持久化”</u></strong>。</p><h2 id="3-4-缓存持久化：以-Redis-为例"><a href="#3-4-缓存持久化：以-Redis-为例" class="headerlink" title="3.4 缓存持久化：以 Redis 为例"></a>3.4 缓存持久化：以 Redis 为例</h2><p>基于上述原因，我们需要对内存型数据库进行适当的缓存持久化。我们仍然以 Redis 为例说明。</p><p>Redis 提供了一套缓存持久化方案：RDB；</p><p>首先 RDB 支持全量备份，但是如果 Redis 缓存空间很大，一次全量备份刷盘会耗时很久，虽说 NoSQL 不需要支持完整的 ACID 性质，但也会严重影响查询时延和吞吐量，并且可能出现持久化的数据不一致的情况。也就是说：</p><ul><li>RDB 即便支持全量备份也不能太过频繁，对性能影响较大；</li><li>RDB 以分钟级别进行全量备份，可能短时间内会丢失大量新数据（snapshot 数据不一致）；</li></ul><p>因此 RDB 全量备份在多数场合下是不划算的。</p><p>于是人们借鉴了关系型数据库的 Binary Log 的思想，添加了一种 AOF 机制，采用<strong><u>增量备份</u></strong>来备份缓存数据。</p><p><strong><u>AOF 持久化机制</u></strong>（Append Only File），就是一种<strong>增量逻辑备份</strong>，向日志文件中以二进制形式写入修改缓存的指令，并且使用了 AOF Buffer 来批量写入以提升效率。</p><p>我们注意到 AOF Log 随着时间推移也会越来越大、越来越多，加载和存储效率都不高。一种解决方案是，定时对已有的 AOF Log 进行重写压缩（包括删去无效修改、指令重写等等）和轮替。</p><p>此外，为了解决备份时数据不一致现象，Redis 再引入 AOF Rewrite Buffer，可以存放在备份期间修改的数据指令，以便子进程对 AOF Log 重写时加入最新的、遗漏的修改指令，维持了一些数据一致性。</p><p>结论 5:   <strong><u>Redis 提供了 RDB 全量备份和 AOF 增量备份两种缓存持久化方案，和关系型数据库一样，二者配合使用可以一定程度上解决缓存热身和雪崩问题</u></strong>。</p><p>虽然缓存持久化能一定程度上解决缓存雪崩、缓存热身等问题，但是无法提升 Redis 的可用性（availability）。为了实现高可用，我们需要引入 Redis 集群，利用多个物理节点 primary-backup 的方式实现高可用支持。</p><h2 id="3-5-内存数据库副本-与-高可用支持"><a href="#3-5-内存数据库副本-与-高可用支持" class="headerlink" title="3.5 内存数据库副本 与 高可用支持"></a>3.5 内存数据库副本 与 高可用支持</h2><p>一个经典的架构：primary-backup 架构（旧称 “master-slave” 主从架构），可以作为数据 replicas 的模式，在多种分布式系统上都在使用。</p><p>例如，我们可以对 Redis 采用这种架构策略。一个 Redis 节点作为 primary，其他两个 redis 作为 backup server；</p><p>其中：</p><ul><li>primary 负责统一写操作，再利用类似 RSM（Replicated State Machine）之类的机制向 backup 发送数据 / 以指令传播的形式同步数据的修改；</li><li><p>primary 和 backup server 都可以处理读操作，有助于减小 primary 负担。</p></li><li><p>primary 维护 Write Ahead Log，在 backup 宕机时能够从 primary 的 WAL 以及自身的 Log 中迅速恢复；</p></li></ul><p>如果考虑到 primary 也可能宕机，可以引入分布式协调者（coordinator），决定谁作为 primary。</p><blockquote><p>这个协调者在 Redis 的术语中被称为 <strong>“哨兵”（Sentinel）</strong>。</p></blockquote><p>另外，如果还需要保证 coordinator 自身的高可用，可以对 coordinator 进行 replications，可以构建经典 <strong><u>“主从-哨兵” 架构</u></strong>。</p><p><img src="imgs/sentinels-and-replicas.png" width="400px" /></p><p>为了确保不会因为 network partition 而出现多个 primary（“split-brain problem”），可以将 coordinator 中选举 primary、心跳监测的职责分出给唯一的 view server。第一次 coordinator 接受 client 请求时先询问 view server 关于 primary 的信息，然后再向 primary 给出修改请求。</p><p>最终，如果还需要保证 view server 的高可用以及数据一致性，还可以将轻量的 view server 进行 replications 并交由 Paxos 或者 ZooKeeper 或者 KRaft 来做分布式协调管理。</p><p>一般为了架构简单起见，可以不使用 view server，直接在 sentinels 中引入 primary 投票机制（主观下线、客观下线），粗略地模拟 Paxos 的一致性协调管理。</p><blockquote><p> 注：选拔 Primary 的标准可以考虑硬件配置、断开 primary 连接的时间长短等等信息来指定优先级，从而选择。</p></blockquote><h2 id="3-6-内存数据库集群"><a href="#3-6-内存数据库集群" class="headerlink" title="3.6 内存数据库集群"></a>3.6 内存数据库集群</h2><p>前面的例子虽然介绍了，内存数据库可以通过建立 replicas 来提升高可用性，但是单个物理节点的存储量总有上限。</p><p>在极大的 data working set 场景下，我们可能需要通过集群来实现更大规模数据的缓存。</p><p>首先需要解决集群后的缓存位置问题。大多数内存型数据库采用了 <strong><u>一致性哈希（Consistent Hashing）思想</u></strong>：</p><ul><li><p>我们先按照常用的 hash 算法将 Key hash 到 $0\sim2^{32}-1$ 个桶中，并且把它们想象成一个环结构；</p></li><li><p>将机器的唯一标识（例如 <code>MAC/IP/HOSTNAME</code> 等信息）以及需要缓存的 KV 都 hash 到环上；</p></li><li><p>于是就能判断信息究竟放在哪一台服务器上了：按顺时针方向，所有对象 hash 的位置距离最近的机器 hash 点就是要存的机器，如下图所示：</p><p><img src="imgs/consistent-hashing-example.png" width="600px" /></p></li><li><p>当有机器（<code>t4</code>）加入分布式集群后，<code>t3 - t4</code> 间的缓存将转移至 <code>t4</code> 上（少量数据交换）；</p><p>反之，有机器（<code>t4</code>）从分布式集群中离线后，<code>t3 - t4</code> 间的缓存将重新转移至 <code>t2</code>；</p></li></ul><p>此外，hash 的位置可以根据机器的硬件承载能力适当调整。调整方法可以借助下文介绍的 virtual nodes 来完成。</p><p>这样的方案能在分布式场景下尽可能减少缓存失效和变动的比例；</p><p>但这种方案仍然存在问题：当集群中的节点数量较少时，可能会出现<strong><u>节点在哈希空间中分布不平衡</u></strong>的问题（hash 环的倾斜和负载不均），甚至引发雪崩问题（最多数据的 A 故障，全转移给 B，然后 B 故障，并重复下去，造成整个分布式集群崩溃）。</p><p>解决 hash 环倾斜的问题的方案之一就是引入 “<strong><u>虚拟节点</u></strong>”（相当于给机器 hash 点创建 “软链接”），将 virtual nodes 和 real nodes 的映射关系记录在 Hash Ring 中；</p><p>上面解决方案的具体实现被称为 “<strong><u>Chord 算法</u></strong>”；</p><p>我们现在继续以 Redis 为例。在 Redis 集群中，实现的方法略有差别，它一般创建一个超大的数组，例如 <code>struct clusterNode *slots[]</code>，规定哪些 Key Hash 值的范围由指定的 Redis 实例负责。并且只有数组中的所有 entries 都被分配给一个 Redis 实例，整个集群才能认为是上线状态。</p><p>因此真正在 Redis 集群中的查询动作通常会先检查数据是否缓存在当前结点中，如果不是则响应 <code>MOVE(IP:PORT)</code> 来指示 client 应该对哪个实例请求该数据。</p><p>现在，我们把集群配合上之前提到的 replicas，形成经典的 “<strong><u>三主三从 + 哨兵 架构</u></strong>”，以此来提升集群的整体可用性：</p><p><img src="imgs/replicas-dist.png" width="400px" /></p><h1 id="Appendix-Redis-Spring-Boot-Example"><a href="#Appendix-Redis-Spring-Boot-Example" class="headerlink" title="Appendix:  Redis + Spring Boot Example"></a>Appendix:  Redis + Spring Boot Example</h1><p>下面是使用 Spring Boot 框架的 Redis 单物理节点代码示例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">CacheService</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Check if the cache(redis) is used and connected.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">isAlive</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Get the data from the cache and database.</span></span><br><span class="line"><span class="comment">     * If cache misses, it will use fallback function to get the current data and cache then.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyPrefix The key prefix of the stored items</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> id The key ID to identify the specific items from a group of stored items</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> type The type of stored items</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> fallback The fallback database query function used when cache misses</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> time The TTL for the specific cached item</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> unit The time unit of the TTL</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> Always the specific stored item consistent with database</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    &lt;R, ID&gt;Optional&lt;R&gt; <span class="title function_">queryAndCache</span><span class="params">(</span></span><br><span class="line"><span class="params">            String keyPrefix, ID id, TypeReference&lt;R&gt; type,</span></span><br><span class="line"><span class="params">            Function&lt;ID, Optional&lt;R&gt;&gt; fallback,</span></span><br><span class="line"><span class="params">            Long time, TimeUnit unit</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Get the data from the cache and database.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> CacheService#queryAndCache(String, Object, TypeReference, Function, Long, TimeUnit)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    &lt;R, ID&gt;Optional&lt;R&gt; <span class="title function_">queryAndCache</span><span class="params">(</span></span><br><span class="line"><span class="params">            String keyPrefix, ID id, Class&lt;R&gt; type,</span></span><br><span class="line"><span class="params">            Function&lt;ID, Optional&lt;R&gt;&gt; fallback,</span></span><br><span class="line"><span class="params">            Long time, TimeUnit unit</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Get a bunch of data from the cache.</span></span><br><span class="line"><span class="comment">     * If cache misses, it will use fallback function to get the current data and cache then.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> The returned list is ordered.</span></span><br><span class="line"><span class="comment">     *  But R could be null if ID is not found in either cache or database.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> CacheService#queryAndCache(String, Object, TypeReference, Function, Long, TimeUnit)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    &lt;R, ID&gt;List&lt;R&gt; <span class="title function_">queryInBatch</span><span class="params">(</span></span><br><span class="line"><span class="params">            String keyPrefix, List&lt;ID&gt; ids, Class&lt;R&gt; type,</span></span><br><span class="line"><span class="params">            Function&lt;ID, Optional&lt;R&gt;&gt; fallback,</span></span><br><span class="line"><span class="params">            Long time, TimeUnit unit</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Only query the data from the cache.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@apiNote</span> [WARNING] We don&#x27;t suppose you to use this method for the following reasons:</span></span><br><span class="line"><span class="comment">     *  1. It will not cache the results, which may cause cache penetration if you use it heavily;</span></span><br><span class="line"><span class="comment">     *  2. The result may be not consistent with database, for it only query on redis for existence.</span></span><br><span class="line"><span class="comment">     *  So you are advised to:</span></span><br><span class="line"><span class="comment">     *  - Put the result after you finally get the data;</span></span><br><span class="line"><span class="comment">     *  - Retry the query on database if you cannot find it</span></span><br><span class="line"><span class="comment">     *    (because it doesn&#x27;t mean that the data don&#x27;t exist in database).</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> CacheService#put(String, Object, Object, Long, TimeUnit)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    &lt;R, ID&gt;Optional&lt;R&gt; <span class="title function_">_query</span><span class="params">(String keyPrefix, ID id, TypeReference&lt;R&gt; type)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Put the specific item into the cache.</span></span><br><span class="line"><span class="comment">     * It will overwrite the original item with the same key (if exists).</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@implNote</span> Use asynchronous operation to improve the throughput.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    &lt;R, ID&gt;<span class="keyword">void</span> <span class="title function_">put</span><span class="params">(String keyPrefix, ID id, R val, Long time, TimeUnit unit)</span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Put cache items in batch.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@apiNote</span> We don&#x27;t use TTL here to avoid a bunch of data use a same TTL,</span></span><br><span class="line"><span class="comment">     *  which may cause cache avalanche.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@implNote</span></span></span><br><span class="line"><span class="comment">     *  1. Use discrete &amp; random TTL for each item.</span></span><br><span class="line"><span class="comment">     *  2. Use asynchronous operation to improve the throughput.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    &lt;R, ID&gt;<span class="keyword">void</span> <span class="title function_">putInBatch</span><span class="params">(String keyPrefix, List&lt;ID&gt; ids, List&lt;R&gt; values)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Invalidate the specific item from the cache.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@implNote</span> It will actually invalid the cached item by putting an invalid value to it.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    &lt;ID&gt;Boolean <span class="title function_">invalidate</span><span class="params">(String keyPrefix, ID id)</span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Invalidate the stored items with the same key prefix.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@implNote</span> It will actually invalid the cached item by putting an invalid value to it.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    Long <span class="title function_">invalidateByKeyPrefix</span><span class="params">(String keyPrefix)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Purge the stored item from the cache.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    &lt;ID&gt;Boolean <span class="title function_">remove</span><span class="params">(String keyPrefix, ID id)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="meta">@RequiredArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CacheServiceImpl</span> <span class="keyword">implements</span> <span class="title class_">CacheService</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> StringRedisTemplate template;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ObjectMapper mapper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isAlive</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">RedisConnectionFactory</span> <span class="variable">factory</span> <span class="operator">=</span> template.getConnectionFactory();</span><br><span class="line">            <span class="keyword">return</span> factory != <span class="literal">null</span> &amp;&amp; !factory.getConnection().isClosed();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;Exception occurs when checking redis connection: &#123;&#125;&quot;</span>, exception.getMessage());</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;R, ID&gt; Optional&lt;R&gt; <span class="title function_">queryAndCache</span><span class="params">(</span></span><br><span class="line"><span class="params">            String keyPrefix, ID id, Class&lt;R&gt; type,</span></span><br><span class="line"><span class="params">            Function&lt;ID, Optional&lt;R&gt;&gt; fallback,</span></span><br><span class="line"><span class="params">            Long time, TimeUnit unit</span></span><br><span class="line"><span class="params">    )</span> &#123;</span><br><span class="line">        TypeReference&lt;R&gt; typeReference = <span class="keyword">new</span> <span class="title class_">TypeReference</span>&lt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Type <span class="title function_">getType</span><span class="params">()</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> type;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.queryAndCache(keyPrefix, id, typeReference, fallback, time, unit);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;R, ID&gt; Optional&lt;R&gt; <span class="title function_">queryAndCache</span><span class="params">(</span></span><br><span class="line"><span class="params">            String keyPrefix, ID id, TypeReference&lt;R&gt; type,</span></span><br><span class="line"><span class="params">            Function&lt;ID, Optional&lt;R&gt;&gt; fallback,</span></span><br><span class="line"><span class="params">            Long time, TimeUnit unit</span></span><br><span class="line"><span class="params">    )</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> keyPrefix + id.toString();</span><br><span class="line">        <span class="comment">// Stage 1. lookup in redis</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.isAlive()) <span class="keyword">try</span> &#123;</span><br><span class="line">            String jsonResult;</span><br><span class="line">            <span class="comment">// maybe fault</span></span><br><span class="line">            jsonResult = template.opsForValue().get(key);</span><br><span class="line">            <span class="keyword">if</span> (jsonResult != <span class="literal">null</span>) &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;Cache hit when fetching key: &#123;&#125;&quot;</span>, key);</span><br><span class="line">                <span class="comment">// has non-blank value in redis?</span></span><br><span class="line">                <span class="keyword">if</span> (jsonResult.equals(Cache.REDIS_INVALID_VALUE)) &#123;</span><br><span class="line">                    <span class="comment">// empty value indicates non-existence.</span></span><br><span class="line">                    <span class="keyword">return</span> Optional.empty();</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// <span class="doctag">TODO:</span> cache hit 后是否更新 TTL?</span></span><br><span class="line">                    <span class="type">R</span> <span class="variable">v</span> <span class="operator">=</span> mapper.readValue(jsonResult, type);</span><br><span class="line">                    <span class="keyword">return</span> Optional.of(v);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;Failed to fetch data from redis due to: &#123;&#125;&quot;</span>,</span><br><span class="line">                    exception.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Stage 2. fallback to database</span></span><br><span class="line">        Optional&lt;R&gt; dbResult = fallback.apply(id);</span><br><span class="line">        log.debug(<span class="string">&quot;Cache miss when fetching key: &#123;&#125;. Cache it now&quot;</span>, key);</span><br><span class="line">        <span class="comment">// we deliberately write empty value to indicate non-existence.</span></span><br><span class="line">        <span class="built_in">this</span>.put(keyPrefix, id, dbResult.isPresent() ? dbResult.get() : Cache.REDIS_INVALID_VALUE, time, unit);</span><br><span class="line">        <span class="keyword">return</span> dbResult;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;R, ID&gt; List&lt;R&gt; <span class="title function_">queryInBatch</span><span class="params">(</span></span><br><span class="line"><span class="params">            String keyPrefix, List&lt;ID&gt; ids, Class&lt;R&gt; type,</span></span><br><span class="line"><span class="params">            Function&lt;ID, Optional&lt;R&gt;&gt; fallback,</span></span><br><span class="line"><span class="params">            Long time, TimeUnit unit</span></span><br><span class="line"><span class="params">    )</span> &#123;</span><br><span class="line">        List&lt;R&gt; result = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.isAlive()) &#123;</span><br><span class="line">            <span class="comment">// query in redis first</span></span><br><span class="line">            <span class="keyword">for</span> (ID id: ids) &#123;</span><br><span class="line">                R curRes;</span><br><span class="line">                String jsonResult;</span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> keyPrefix + id.toString();</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// maybe fault</span></span><br><span class="line">                    jsonResult = template.opsForValue().get(key);</span><br><span class="line">                    <span class="keyword">if</span> (jsonResult != <span class="literal">null</span>) &#123;</span><br><span class="line">                        log.debug(<span class="string">&quot;Cache hit when doing batch query: &#123;&#125;&quot;</span>, key);</span><br><span class="line">                        curRes = mapper.readValue(jsonResult, type);</span><br><span class="line">                        <span class="keyword">if</span> (!curRes.equals(Cache.REDIS_INVALID_VALUE)) &#123;</span><br><span class="line">                            result.addLast(curRes);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// not exist</span></span><br><span class="line">                            result.addLast(<span class="literal">null</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">                    log.warn(<span class="string">&quot;Batch query failed: &#123;&#125;. Skip current one.&quot;</span>,</span><br><span class="line">                            exception.getMessage());</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// fallback to database</span></span><br><span class="line">                curRes = fallback.apply(id).orElse(<span class="literal">null</span>);</span><br><span class="line">                log.debug(<span class="string">&quot;Cache miss when doing batch query: &#123;&#125;. Cache it now&quot;</span>, key);</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// set cache</span></span><br><span class="line">                    template.opsForValue().set(</span><br><span class="line">                            key, curRes == <span class="literal">null</span> ? Cache.REDIS_INVALID_VALUE</span><br><span class="line">                                    : mapper.writeValueAsString(curRes),</span><br><span class="line">                            time, unit</span><br><span class="line">                    );</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">                    log.warn(<span class="string">&quot;Cache data from database failed when batching query: &#123;&#125;. Skip current one.&quot;</span>,</span><br><span class="line">                            exception.getMessage());</span><br><span class="line">                &#125;</span><br><span class="line">                result.addLast(curRes);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// query in database only</span></span><br><span class="line">            <span class="keyword">for</span> (ID id: ids) &#123;</span><br><span class="line">                result.addLast(fallback.apply(id).orElse(<span class="literal">null</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;R, ID&gt; Optional&lt;R&gt; <span class="title function_">_query</span><span class="params">(String keyPrefix, ID id, TypeReference&lt;R&gt; type)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> keyPrefix + id.toString();</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.isAlive()) <span class="keyword">try</span> &#123;</span><br><span class="line">            String jsonResult;</span><br><span class="line">            <span class="comment">// may be fault even we have already checked the liveness</span></span><br><span class="line">            jsonResult = template.opsForValue().get(key);</span><br><span class="line">            <span class="keyword">if</span> (jsonResult != <span class="literal">null</span>) &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;Cache hit when fetching key with _query: &#123;&#125;&quot;</span>, key);</span><br><span class="line">                <span class="comment">// has non-blank value in redis?</span></span><br><span class="line">                <span class="keyword">if</span> (!jsonResult.equals(Cache.REDIS_INVALID_VALUE)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> Optional.of(mapper.readValue(jsonResult, type));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// Sadly we cannot use empty value to indicate non-existence. :(</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;Failed to do simple query on redis due to: &#123;&#125;&quot;</span>,</span><br><span class="line">                    exception.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        log.debug(<span class="string">&quot;Cache miss when fetching key with _query: &#123;&#125;&quot;</span>, key);</span><br><span class="line">        <span class="keyword">return</span> Optional.empty();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;R, ID&gt; <span class="keyword">void</span> <span class="title function_">put</span><span class="params">(String keyPrefix, ID id, R val, Long time, TimeUnit unit)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">this</span>.isAlive()) &#123;</span><br><span class="line">                template.opsForValue().set(</span><br><span class="line">                        keyPrefix + id.toString(),</span><br><span class="line">                        mapper.writeValueAsString(val),</span><br><span class="line">                        time, unit</span><br><span class="line">                );</span><br><span class="line">                log.debug(<span class="string">&quot;Set cache for key: &#123;&#125;; TTL: &#123;&#125; &#123;&#125;&quot;</span>, keyPrefix + id, time, unit);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                log.warn(<span class="string">&quot;Redis not alive. Skip writing to it.&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;Exception occurs when setting value for redis: &#123;&#125;&quot;</span>,</span><br><span class="line">                    exception.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;R, ID&gt; <span class="keyword">void</span> <span class="title function_">putInBatch</span><span class="params">(String keyPrefix, List&lt;ID&gt; ids, List&lt;R&gt; values)</span> &#123;</span><br><span class="line">        <span class="type">Random</span> <span class="variable">randomGen</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">assert</span> ids.size() == values.size();</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">this</span>.isAlive()) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">idx</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (ID id: ids) &#123;</span><br><span class="line">                    <span class="comment">// center: REDIS_DEFAULT_CACHE_TTL</span></span><br><span class="line">                    <span class="comment">// [center - RANGE, center + RANGE]</span></span><br><span class="line">                    <span class="type">long</span> <span class="variable">randomTime</span> <span class="operator">=</span> randomGen.nextLong(<span class="number">2</span> * Cache.REDIS_BATCH_RANDOM_TTL_RANGE)</span><br><span class="line">                            + Cache.REDIS_DEFAULT_CACHE_TTL - Cache.REDIS_BATCH_RANDOM_TTL_RANGE;</span><br><span class="line">                    template.opsForValue().set(</span><br><span class="line">                            keyPrefix + id.toString(),</span><br><span class="line">                            mapper.writeValueAsString(values.get(idx)),</span><br><span class="line">                            randomTime, Cache.REDIS_TTL_UNIT</span><br><span class="line">                    );</span><br><span class="line">                    log.debug(<span class="string">&quot;Set cache for key in batch: &#123;&#125;; TTL: &#123;&#125; &#123;&#125;&quot;</span>,</span><br><span class="line">                            keyPrefix + id, randomTime, Cache.REDIS_TTL_UNIT);</span><br><span class="line">                    ++idx;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                log.warn(<span class="string">&quot;Redis not alive. Skip batch process.&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;Exception occurs when doing batch process for redis: &#123;&#125;&quot;</span>,</span><br><span class="line">                    exception.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;ID&gt; Boolean <span class="title function_">invalidate</span><span class="params">(String keyPrefix, ID id)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="built_in">this</span>.isAlive()) &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;Redis not alive. Skip invalidating process.&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// return template.delete(key);</span></span><br><span class="line">            <span class="comment">// no delete but set empty value</span></span><br><span class="line">            template.opsForValue().set(</span><br><span class="line">                    keyPrefix + id.toString(), Cache.REDIS_INVALID_VALUE,</span><br><span class="line">                    Cache.REDIS_DEFAULT_CACHE_TTL, Cache.REDIS_TTL_UNIT</span><br><span class="line">            );</span><br><span class="line">            log.debug(<span class="string">&quot;Invalid cache for key: &#123;&#125;; TTL: &#123;&#125; &#123;&#125;&quot;</span>,</span><br><span class="line">                    keyPrefix + id, Cache.REDIS_DEFAULT_CACHE_TTL, Cache.REDIS_TTL_UNIT);</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;Exception occurs when invalidating value from redis: &#123;&#125;&quot;</span>,</span><br><span class="line">                    exception.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">invalidateByKeyPrefix</span><span class="params">(String keyPrefix)</span> &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">res</span> <span class="operator">=</span> <span class="number">0L</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Set&lt;String&gt; keys = template.keys(keyPrefix);</span><br><span class="line">            <span class="comment">// if (keys != null) res = template.delete(keys);</span></span><br><span class="line">            <span class="keyword">if</span> (keys != <span class="literal">null</span>) &#123;</span><br><span class="line">                res = keys.size();</span><br><span class="line">                List&lt;String&gt; empties = Collections.nCopies(keys.size(), Cache.REDIS_INVALID_VALUE);</span><br><span class="line">                log.debug(<span class="string">&quot;--- Start invalid cache by key prefix ---&quot;</span>);</span><br><span class="line">                <span class="comment">// tricky point: use empty key prefix</span></span><br><span class="line">                <span class="built_in">this</span>.putInBatch(<span class="string">&quot;&quot;</span>, keys.stream().toList(), empties);</span><br><span class="line">                log.debug(<span class="string">&quot;--- Successfully invalid cache for keys: &#123;&#125; ---&quot;</span>, keys);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;Exception occurs when invalidateByPrefix: &#123;&#125;&quot;</span>,</span><br><span class="line">                    exception.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;ID&gt; Boolean <span class="title function_">remove</span><span class="params">(String keyPrefix, ID id)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.isAlive()) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;Remove cache for key: &#123;&#125;&quot;</span>, keyPrefix + id.toString());</span><br><span class="line">                <span class="keyword">return</span> template.delete(keyPrefix + id.toString());</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">                log.warn(<span class="string">&quot;Exception occurs when removing cache: &#123;&#125;&quot;</span>,</span><br><span class="line">                        exception.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> log.warn(<span class="string">&quot;Redis not alive. Skip removing process.&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Chapter-1-基本概念和-CLI-使用&quot;&gt;&lt;a href=&quot;#Chapter-1-基本概念和-CLI-使用&quot; class=&quot;headerlink&quot; title=&quot;Chapter 1. 基本概念和 CLI 使用&quot;&gt;&lt;/a&gt;Chapter 1. 基本概念和 CL</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="Programming" scheme="https://blog.sjtuxhw.top/tags/Programming/"/>
    
    <category term="Web" scheme="https://blog.sjtuxhw.top/tags/Web/"/>
    
    <category term="Redis" scheme="https://blog.sjtuxhw.top/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Python 科学计算入门</title>
    <link href="https://blog.sjtuxhw.top/technical/python-sci-starter/"/>
    <id>https://blog.sjtuxhw.top/technical/python-sci-starter/</id>
    <published>2024-11-03T11:08:13.000Z</published>
    <updated>2024-11-10T11:17:11.085Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-1-NumPy"><a href="#0-1-NumPy" class="headerlink" title="0.1 NumPy"></a>0.1 NumPy</h2><p><code>Numpy</code> 库是 Python 科学计算的核心。</p><p>Numpy Array 是一个存放相同数据类型的数组（类型 <code>numpy.ndarray</code>），可以使用非负元组（Non-negative tuple）来索引。</p><p>我们称 Rank 为数组的维数，Shape 表示数组的各个维度的大小，使用整型元组表示。</p><h3 id="0-1-1-Array-Creation"><a href="#0-1-1-Array-Creation" class="headerlink" title="0.1.1 Array Creation"></a>0.1.1 Array Creation</h3><p>初始化 Numpy Array 的方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a))</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>], a[<span class="number">1</span>], a[<span class="number">2</span>])</span><br><span class="line"><span class="comment"># Reference Modification</span></span><br><span class="line">a[<span class="number">0</span>] = <span class="number">5</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">b = np.array(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(b))</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">0</span>, <span class="number">0</span>], b[<span class="number">1</span>, <span class="number">0</span>], b[<span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>类似 Matlab，提供多种方法创建数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">a = np.zeros((<span class="number">2</span>, <span class="number">2</span>))    <span class="comment"># 2x2 0-matrix</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = np.ones((<span class="number">3</span>, <span class="number">3</span>))     <span class="comment"># 3x3 1-matrix</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">c = np.eye(<span class="number">2</span>)           <span class="comment"># 2x2 identical matrix</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line">d = np.full((<span class="number">4</span>, <span class="number">4</span>), <span class="number">5</span>)  <span class="comment"># 4x4 matrix filled with 5</span></span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="comment"># 2x2 matrix filled with uniformed ([0,1]) random value</span></span><br><span class="line">e = np.random.random((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"></span><br><span class="line">f = np.diag((-<span class="number">3</span>, -<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)) <span class="comment"># 4x4 matrix with -3, -4, 5, 6 on diagonal</span></span><br><span class="line"><span class="built_in">print</span>(f)</span><br><span class="line">g_dup = f</span><br><span class="line">g = f.copy()    <span class="comment"># Copy construction</span></span><br><span class="line">f[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(g_dup, g)</span><br><span class="line"></span><br><span class="line">h = np.linspace(<span class="number">0</span>, <span class="number">13</span>, <span class="number">5</span>)  <span class="comment"># start=0, end(included)=13, num=5</span></span><br><span class="line"><span class="built_in">print</span>(h)</span><br><span class="line"></span><br><span class="line"><span class="comment"># args are similar with range(), but return np.array</span></span><br><span class="line">k = np.arange(<span class="number">4</span>, <span class="number">5</span>, <span class="number">0.1</span>, dtype=<span class="built_in">float</span>)</span><br><span class="line"><span class="built_in">print</span>(k)</span><br></pre></td></tr></table></figure><p>更多创建方法用到再说。</p><h3 id="0-1-2-Array-Indexing"><a href="#0-1-2-Array-Indexing" class="headerlink" title="0.1.2 Array Indexing"></a>0.1.2 Array Indexing</h3><p>索引 Numpy Array 的方法和 Python 原生数组类似：</p><ul><li>整型索引 + slice <code>:</code> 切片；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">a = np.array(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">     [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">     [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use slice in each dimension (!!! Reference but NOT copy construction !!!)</span></span><br><span class="line">b = a[:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>]  <span class="comment"># row: 0-1 (2 not included), column: 1-2</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># Reference Modification</span></span><br><span class="line">b[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">99</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We need to copy construct explicitly</span></span><br><span class="line">c = a[:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>].copy()</span><br><span class="line">b[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">88</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get by only one axis</span></span><br><span class="line">c = a[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line">d = a[<span class="number">0</span>, <span class="number">2</span>] <span class="comment"># row=0, column=2</span></span><br><span class="line"><span class="built_in">print</span>(d)</span><br></pre></td></tr></table></figure><p>注意引用传递的问题。</p><p>还可以将 整型索引 和 slice 混合使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = np.array(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">     [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">     [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the whole row 0 (equivalent to a[0])</span></span><br><span class="line">row_r1 = a[<span class="number">0</span>, :]</span><br><span class="line"><span class="built_in">print</span>(row_r1, row_r1.shape)</span><br><span class="line"><span class="comment"># row 0 (but keep dimension)</span></span><br><span class="line">row_r2 = a[<span class="number">0</span>:<span class="number">1</span>, :]</span><br><span class="line"><span class="built_in">print</span>(row_r2, row_r2.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Same with column</span></span><br><span class="line">col_r1 = a[:, <span class="number">0</span>]</span><br><span class="line">col_r2 = a[:, <span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(col_r1, col_r1.shape)</span><br><span class="line"><span class="built_in">print</span>(col_r2, col_r2.shape)</span><br></pre></td></tr></table></figure><p>此外，Numpy 还支持：</p><ul><li>整型数组索引 + 布尔数组索引（更类似 Matlab）：仍然是引用传递。需要 <code>copy()</code> 来 copy construct；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = np.array(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">     [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">     [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Equivalent to: np.array([ a[0, 0], a[1, 1], a[2, 0] ])</span></span><br><span class="line"><span class="built_in">print</span>(a[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable indexing</span></span><br><span class="line">b = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># Select each row, column: 0, 2, 0, 1, respectively.</span></span><br><span class="line"><span class="built_in">print</span>(a[np.arange(<span class="number">4</span>), b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reference modification</span></span><br><span class="line">a[np.arange(<span class="number">4</span>), b] += <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Like Matlab</span></span><br><span class="line">bool_idx = (a &gt; <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(bool_idx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> boolean indexing can only construct rank-1 array</span></span><br><span class="line"><span class="built_in">print</span>(a[bool_idx])  <span class="comment"># [3 4 5 6]</span></span><br><span class="line"><span class="built_in">print</span>(a[a &gt; <span class="number">2</span>])</span><br></pre></td></tr></table></figure><h3 id="0-1-3-Data-Types"><a href="#0-1-3-Data-Types" class="headerlink" title="0.1.3 Data Types"></a>0.1.3 Data Types</h3><p>Numpy 中提供来多种数据类型，可以用来构建数组等操作。默认情况下构建 array，在不指定 <code>dtype</code> 参数时，Numpy 会猜测数组的数据类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(x.dtype)    <span class="comment"># np.int64</span></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>], dtype=np.uint64)</span><br><span class="line"><span class="built_in">print</span>(x.dtype)    <span class="comment"># np.uint64</span></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line"><span class="built_in">print</span>(x.dtype)    <span class="comment"># np.float64</span></span><br></pre></td></tr></table></figure><blockquote><p>Python 原生类型，如 <code>int / bool / float / complex / bytes / str / object</code>，分别对应 <code>int_ / bool_ / float64 / complex128 / bytes_ / str_ / object_</code>；</p></blockquote><p>当数据类型比较复杂时（例如结构体数组），我们可以手动创建 <code>dtype</code> 并指定，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dt = np.dtype([</span><br><span class="line">    (<span class="string">&#x27;name&#x27;</span>, np.str_, <span class="number">16</span>),          <span class="comment"># Like database CHAR(16)</span></span><br><span class="line">    (<span class="string">&#x27;grades&#x27;</span>, np.float64, (<span class="number">2</span>,))    <span class="comment"># length=2, rank-1 array</span></span><br><span class="line">])</span><br><span class="line"><span class="built_in">print</span>(dt)    <span class="comment"># &lt; 表示小端序、U 代表 unicode char 数字就是占用的字节</span></span><br><span class="line">x = np.array([(<span class="string">&#x27;Sarah&#x27;</span>, (<span class="number">8.0</span>, <span class="number">7.0</span>)), (<span class="string">&#x27;John&#x27;</span>, (<span class="number">6.0</span>, <span class="number">7.0</span>))], dtype=dt)</span><br><span class="line"><span class="comment"># Get data like dict</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">1</span>][<span class="string">&#x27;grades&#x27;</span>])</span><br></pre></td></tr></table></figure><p>更多类型的使用、<code>dtype</code> 信息，请参见 <a href="https://numpy.org/doc/stable/reference/arrays.dtypes.html">Array DTypes - Numpy Doc</a>；</p><h3 id="0-1-4-Array-Math"><a href="#0-1-4-Array-Math" class="headerlink" title="0.1.4 Array Math"></a>0.1.4 Array Math</h3><p>数组运算，在 Numpy 中也是向量 / 矩阵运算。</p><p>在 Matlab 中，我们知道这些运算可以是向量整体点积/叉积、矩阵整体乘法/取逆、向量矩阵乘法；也可以是 element-wise（逐元素）的运算。</p><p>逐元素的四则运算、开方运算如下（会产生新对象）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"><span class="built_in">print</span>(np.add(x, y))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x - y)</span><br><span class="line"><span class="built_in">print</span>(np.subtract(x, y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 和 matlab 不一样，* 符号是 element-wise 的</span></span><br><span class="line"><span class="built_in">print</span>(x * y)</span><br><span class="line"><span class="built_in">print</span>(np.multiply(x, y))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x / y)</span><br><span class="line"><span class="built_in">print</span>(np.divide(x, y))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.sqrt(x))</span><br></pre></td></tr></table></figure><p>向量点积（内积）/ 矩阵整体乘法 / 向量矩阵乘法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matrices</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># vectors</span></span><br><span class="line">v = np.array([<span class="number">9</span>, <span class="number">10</span>])</span><br><span class="line">w = np.array([<span class="number">11</span>, <span class="number">12</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inner product of vectors</span></span><br><span class="line"><span class="built_in">print</span>(v.dot(w))</span><br><span class="line"><span class="built_in">print</span>(np.dot(v, w)) <span class="comment"># equivalence</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix / vector product</span></span><br><span class="line"><span class="comment"># They have different answers!</span></span><br><span class="line"><span class="built_in">print</span>(v.dot(x)) <span class="comment"># v is regarded as row vector</span></span><br><span class="line"><span class="built_in">print</span>(x.dot(v)) <span class="comment"># v is regarded as column vector</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix product</span></span><br><span class="line"><span class="built_in">print</span>(x.dot(y))</span><br></pre></td></tr></table></figure><p>我们发现，在 Numpy 中，通过 <code>array()</code> 定义的 <strong><u>rank-1 array 很灵活，既可以作行向量，又可以作列向量</u></strong>。</p><p>也正因如此，我们下面介绍的 <strong><u>转置操作</u></strong> 对 rank-1 array 毫无影响：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)    <span class="comment"># Prints &quot;[[1 2]</span></span><br><span class="line">            <span class="comment">#          [3 4]]&quot;</span></span><br><span class="line"><span class="comment"># 注意：返回引用！</span></span><br><span class="line"><span class="built_in">print</span>(x.T)  <span class="comment"># Prints &quot;[[1 3]</span></span><br><span class="line">            <span class="comment">#          [2 4]]&quot;</span></span><br><span class="line"><span class="comment"># 等价于 x.transpose()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that taking the transpose of a rank 1 array does nothing:</span></span><br><span class="line">v = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(v)    <span class="comment"># Prints &quot;[1 2 3]&quot;</span></span><br><span class="line"><span class="built_in">print</span>(v.T)  <span class="comment"># Prints &quot;[1 2 3]&quot;</span></span><br></pre></td></tr></table></figure><p>此外，除了 element-wise 求和，Numpy 和 Matlab 一样提供了整体求和的方法 <code>sum</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x))    <span class="comment"># 默认全部元素求和</span></span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x, axis=<span class="number">0</span>))  <span class="comment"># axis=0 对第一维度（列）求和</span></span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x, axis=<span class="number">1</span>))  <span class="comment"># axis=1 对第二维度（行）求和</span></span><br></pre></td></tr></table></figure><p>更多运算方法用到再说。</p><h3 id="0-1-5-Broadcasting"><a href="#0-1-5-Broadcasting" class="headerlink" title="0.1.5 Broadcasting"></a>0.1.5 Broadcasting</h3><p>广播（broadcasting）是 Numpy 运算中相当重要的性质之一，它为不同维度的 array 间的运算提供了更高效的方式。下面几种常用的手段：</p><h4 id="将-rank-1-array-加到-matrix-的每一行上"><a href="#将-rank-1-array-加到-matrix-的每一行上" class="headerlink" title="将 rank-1 array 加到 matrix 的每一行上"></a>将 rank-1 array 加到 matrix 的每一行上</h4><p>假设现在有个需求，将 <code>v</code> 向量加到 <code>x</code> 的每一行上，最后放到 <code>y</code> 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">v = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># empty_like() 按照 x.shape 来创建一个空矩阵</span></span><br><span class="line">y = np.empty_like(x)</span><br></pre></td></tr></table></figure><p>如果不用 broadcasting 就应该这么做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">    y[i, :] = x[i, :] + v</span><br></pre></td></tr></table></figure><p>或者使用 <code>tile</code> 来堆叠重复向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vv = np.tile(v, (<span class="number">4</span>, <span class="number">1</span>))   <span class="comment"># Stack 4 copies of v on top of each other</span></span><br><span class="line"><span class="built_in">print</span>(vv)                 <span class="comment"># Prints &quot;[[1 0 1]</span></span><br><span class="line">                          <span class="comment">#          [1 0 1]</span></span><br><span class="line">                          <span class="comment">#          [1 0 1]</span></span><br><span class="line">                          <span class="comment">#          [1 0 1]]&quot;</span></span><br></pre></td></tr></table></figure><p>第一种方法虽然能实现需求，但是如果 <code>x</code> 相当大，那么 Python 自己的循环就非常慢，因此我们需要借助 broadcasting（写在 Numpy Library 中，C/C++ 实现）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不同维度的 array 通过 broadcast 直接运算</span></span><br><span class="line">y = x + v  <span class="comment"># Add v to each row of x using broadcasting</span></span><br></pre></td></tr></table></figure><p>但是如果使用不当会出现 shape mismatch 的情况。因此我们需要知道 broadcast 的算法：</p><ol><li><p>如果参与运算的 arrays 的 rank 不同，那么向 rank 小的 array 的 shape 中 <code>prepend</code> 数字 1（可以在不添加元素的前提下扩展维度），直至它们的 rank（就是 shape 元组的长度）相等；</p><blockquote><p>例如 <code>[1, 2, 3]</code>（shape <code>(3,)</code>）扩展一次维度后变成 <code>[[1, 2, 3]]</code>（shape <code>(1, 3)</code>）；</p></blockquote><p>定义：两个 arrays 在某个维度上 compatible（匹配），当且仅当它们的 shape 在这个维度上的值相同，<u>或者一方为 1</u>；</p><p>因此，第一步就是尝试让两个参与运算的 array 在每个维度上都匹配。数学上<strong>只有两个 array 在每个维度上都匹配，才能进行 broadcasting 操作</strong>。</p></li><li><p>两个 arrays 运算时，如果在某个维度上一方为 1，另一方大于 1，那么说明需要在这个维度上 broadcasting，具体做法就是让这个维度上为 1 的 array，在当前维度上重复多次直至与另一个 array 的相等；</p></li><li><p>如果两个 arrays 是匹配的，那么在最终运算结束时，结果是二者 shapes 的 element-wise 的最大值。</p></li></ol><p><strong>思考 1：为什么 broadcasting 会将向量加到矩阵的每一行，而不是每一列？</strong></p><p>答：由 broadcasting 算法决定。上面提到，会向 shape 中 <code>prepend</code> 数字 1，这就是原因（如果算法是 append 的话就是加到列上去了）。</p><p><strong>思考 2：那么我们如何利用 broadcasting 将向量加到每一列上？</strong></p><p>答：先用转置，然后最后再转置回去就行。或者使用 <code>reshape</code> 来手动预处理 shape（向它 append 1）；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">w = np.array([<span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>((x.T + w).T)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="built_in">print</span>(x + np.reshape(w, (<span class="number">2</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure><h4 id="常量-Element-wise-运算"><a href="#常量-Element-wise-运算" class="headerlink" title="常量 Element-wise 运算"></a>常量 Element-wise 运算</h4><p>这应该是广播最为通俗易懂的形式。</p><p>我们直接用常量（相当于扩展前的 shape <code>(1,)</code>）向矩阵乘：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(x * <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="计算向量叉积"><a href="#计算向量叉积" class="headerlink" title="计算向量叉积"></a>计算向量叉积</h4><blockquote><p>注：可以使用自带方法 <code>numpy.outer(a, b)</code>，这里只是展示如何用 broadcasting 完成叉积（外积）。</p></blockquote><p>本质上就是将被乘向量的每个元素乘到每个列上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">w = np.array([<span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.reshape(v, (<span class="number">3</span>, <span class="number">1</span>)) * w)</span><br></pre></td></tr></table></figure><p>关于 Numpy 的基础用法掌握这么多就够了。如果希望更多的信息，请参见 Numpy 官方文档。</p><h2 id="0-2-SciPy"><a href="#0-2-SciPy" class="headerlink" title="0.2 SciPy"></a>0.2 SciPy</h2><p><code>SciPy</code> 库提供了一些高性能方法来计算和掌控多维数组，对科学计算和工程有很大用处。</p><p>最好入门的方法是阅读这个文档：<a href="https://docs.scipy.org/doc/scipy/reference/index.html">Scipy - Reference</a>。这里我们尽快入门为主，有需要再自行翻阅。</p><h3 id="0-2-1-Image-Operations"><a href="#0-2-1-Image-Operations" class="headerlink" title="0.2.1 Image Operations"></a>0.2.1 Image Operations</h3><p>Scipy 库提供了图像处理的基本函数。例如：</p><ul><li>从磁盘的图片中读入到 Numpy 数组；</li><li>将 Numpy 数组写成图片；</li></ul><p>下面是使用 Scipy resize 图片的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注：这个写法在 scipy 1.2.0 以后被弃用。</span></span><br><span class="line"><span class="comment"># 你应该使用 opencv 来导入图片</span></span><br><span class="line"><span class="comment"># from cv2 import imread, resize, imwrite</span></span><br><span class="line"><span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread, imsave, imresize</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read an JPEG image into a numpy array</span></span><br><span class="line">img = imread(<span class="string">&#x27;assets/cat.jpg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(img.dtype, img.shape)  <span class="comment"># Prints &quot;uint8 (400, 248, 3)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 补充计算机图形学知识：从这里能看出，jpg 格式的位深度为 8x3 = 24（bytes）</span></span><br><span class="line"><span class="comment"># 其中每个值是 uint8 存储，共 3 个通道（RGB）</span></span><br><span class="line"><span class="comment"># 如果是 png 格式，可以是 4 个通道（RGBA），可以存透明度，称为 PNG-32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Broadcasting</span></span><br><span class="line"><span class="comment"># 将图像的 Green Channel 和 Blue Channel 像素值分别广播乘以 0.95 和 0.9</span></span><br><span class="line"><span class="comment"># 图像会微微泛红</span></span><br><span class="line">img_tinted = img * [<span class="number">1</span>, <span class="number">0.95</span>, <span class="number">0.9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize</span></span><br><span class="line">img_tinted = imresize(img_tinted, (<span class="number">300</span>, <span class="number">300</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save</span></span><br><span class="line">imsave(<span class="string">&#x27;assets/cat_tinted.jpg&#x27;</span>, img_tinted)</span><br></pre></td></tr></table></figure><h3 id="0-2-2-Matlab-Files"><a href="#0-2-2-Matlab-Files" class="headerlink" title="0.2.2 Matlab Files"></a>0.2.2 Matlab Files</h3><p>我们可以从 Matlab 中加载通用的矩阵文件。例如下面的例子是从 Matlab 的矩阵文件中加载复数矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load matlab mat file</span></span><br><span class="line">trainDataRaw = loadmat(<span class="string">&quot;data/PA_data_train.mat&quot;</span>)</span><br><span class="line">testDataRaw = loadmat(<span class="string">&quot;data/PA_data_test.mat&quot;</span>)</span><br><span class="line">train_input = trainDataRaw[<span class="string">&#x27;paInput&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">train_output = trainDataRaw[<span class="string">&#x27;paOutput&#x27;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(train_input) == <span class="built_in">len</span>(train_output), <span class="string">&quot;The size of input vector should be equal to the output one.&quot;</span></span><br><span class="line"></span><br><span class="line">test_input = testDataRaw[<span class="string">&#x27;paInput&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">test_output = testDataRaw[<span class="string">&#x27;paOutput&#x27;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(test_input) == <span class="built_in">len</span>(test_output), <span class="string">&quot;The size of input vector should be equal to the output one.&quot;</span></span><br><span class="line"></span><br><span class="line">trainData = [(<span class="built_in">complex</span>(train_input[k]), <span class="built_in">complex</span>(train_output[k])) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train_input))]</span><br><span class="line">testData = [(<span class="built_in">complex</span>(test_input[k]), <span class="built_in">complex</span>(test_output[k])) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_input))]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[INFO] Train data loaded: <span class="subst">&#123;<span class="built_in">len</span>(trainData)&#125;</span> groups (input, output)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[INFO] Test data loaded: <span class="subst">&#123;<span class="built_in">len</span>(testData)&#125;</span> groups (input, output)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preparing for the module of the data</span></span><br><span class="line">trainModule = [(<span class="built_in">abs</span>(i[<span class="number">0</span>]), <span class="built_in">abs</span>(i[<span class="number">1</span>])) <span class="keyword">for</span> i <span class="keyword">in</span> trainData]</span><br><span class="line">testModule = [(<span class="built_in">abs</span>(i[<span class="number">0</span>]), <span class="built_in">abs</span>(i[<span class="number">1</span>])) <span class="keyword">for</span> i <span class="keyword">in</span> testData]</span><br></pre></td></tr></table></figure><h3 id="0-2-3-Distance-between-points"><a href="#0-2-3-Distance-between-points" class="headerlink" title="0.2.3 Distance between points"></a>0.2.3 Distance between points</h3><p>Scipy 求两点间距如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> pdist, cdist, squareform</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义了一组二维点</span></span><br><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">0</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算点集中两两间的欧几里得距离，输出为矩阵</span></span><br><span class="line"><span class="comment"># 第 i 行、第 j 列表示点集中第 i 个点和第 j 个点的欧式距离</span></span><br><span class="line"><span class="comment"># [[ 0.          1.41421356  2.23606798]</span></span><br><span class="line"><span class="comment">#  [ 1.41421356  0.          1.        ]</span></span><br><span class="line"><span class="comment">#  [ 2.23606798  1.          0.        ]]</span></span><br><span class="line">d = squareform(pdist(x, <span class="string">&#x27;euclidean&#x27;</span>))</span><br><span class="line"><span class="comment"># 等价于</span></span><br><span class="line">e = cdist(x, x, <span class="string">&#x27;euclidean&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure><p>还有一个 <code>cdist</code> 求的是两个点集间的距离（指定两个点集，而不像上面是在一个点集内），自动输出为矩阵形式。</p><h2 id="0-3-Matplotlib"><a href="#0-3-Matplotlib" class="headerlink" title="0.3 Matplotlib"></a>0.3 Matplotlib</h2><p>介绍一些模板用法。</p><h3 id="0-3-1-并列折线图"><a href="#0-3-1-并列折线图" class="headerlink" title="0.3.1 并列折线图"></a>0.3.1 并列折线图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = [<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;8&#x27;</span>]</span><br><span class="line"></span><br><span class="line">fig, axe = plt.subplots()</span><br><span class="line"></span><br><span class="line">prim = [<span class="number">27138166.8</span>, <span class="number">9152368.0</span>, <span class="number">7887584.4</span>, <span class="number">5203370.6</span>]</span><br><span class="line">opt = [<span class="number">45508900.4</span>, <span class="number">91379932.6</span>, <span class="number">179513808.6</span>, <span class="number">289122555.6</span>]</span><br><span class="line"></span><br><span class="line">axe.scatter(x, prim, c=<span class="string">&#x27;#219ebc&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">axe.scatter(x, opt, c=<span class="string">&#x27;#feb705&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line"><span class="comment"># axe.scatter(x, ratio_3, c=&#x27;#fa8600&#x27;, marker=&#x27;^&#x27;)</span></span><br><span class="line">axe.plot(x, prim, <span class="string">&#x27;-&#x27;</span>, c=<span class="string">&#x27;#219ebc&#x27;</span>, label=<span class="string">&quot;Primitive&quot;</span>)</span><br><span class="line">axe.plot(x, opt, <span class="string">&#x27;-&#x27;</span>, c=<span class="string">&#x27;#feb705&#x27;</span>, label=<span class="string">&quot;Optimized&quot;</span>)</span><br><span class="line"><span class="comment"># axe.plot(x, ratio_3, &#x27;-&#x27;, c=&#x27;#fa8600&#x27;, label=&quot;Workload 3&quot;)</span></span><br><span class="line"></span><br><span class="line">axe.set_xticks(x)</span><br><span class="line">axe.legend()</span><br><span class="line">axe.minorticks_on()</span><br><span class="line">axe.grid()</span><br><span class="line"></span><br><span class="line"><span class="comment"># axe.set_ylim([0.3, 1.2])</span></span><br><span class="line">axe.set_xlabel(<span class="string">&#x27;Thread Number&#x27;</span>)</span><br><span class="line">axe.set_ylabel(<span class="string">&#x27;Average Throughput (ops/s)&#x27;</span>)</span><br><span class="line">axe.set_title(<span class="string">&#x27;Average Throughput - Thread Number Relationship Plot&#x27;</span>)</span><br><span class="line"></span><br><span class="line">axe.ticklabel_format(style=<span class="string">&#x27;sci&#x27;</span>, scilimits=(-<span class="number">1</span>, <span class="number">2</span>), axis=<span class="string">&#x27;y&#x27;</span>, useMathText=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="py_imgs/throughput.png" width="400px" /></p><h3 id="0-3-2-并列直方图"><a href="#0-3-2-并列直方图" class="headerlink" title="0.3.2 并列直方图"></a>0.3.2 并列直方图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">labels = [<span class="string">&#x27;Tiny&#x27;</span>, <span class="string">&#x27;Small&#x27;</span>, <span class="string">&#x27;Medium&#x27;</span>, <span class="string">&#x27;Large&#x27;</span>]</span><br><span class="line">x_labels = labels</span><br><span class="line">width = <span class="number">0.2</span></span><br><span class="line">x = np.arange(<span class="built_in">len</span>(labels))</span><br><span class="line"></span><br><span class="line">qs = [<span class="number">5458</span>, <span class="number">36660</span>, <span class="number">155667</span>, <span class="number">1341294</span>]</span><br><span class="line">ls = [<span class="number">27247</span>, <span class="number">287699</span>, <span class="number">1146510</span>, <span class="number">10195125</span>]</span><br><span class="line">qs_r = [<span class="number">6469</span>, <span class="number">45830</span>, <span class="number">192725</span>, <span class="number">2354645</span>]</span><br><span class="line"></span><br><span class="line">fig, axe = plt.subplots()</span><br><span class="line"></span><br><span class="line">axe.minorticks_on()</span><br><span class="line">axe.grid()</span><br><span class="line"></span><br><span class="line">axe.bar(x - width - <span class="number">0.05</span>, qs, width, color=<span class="string">&#x27;#2a9d8c&#x27;</span>, label=<span class="string">&#x27;Quick Select&#x27;</span>, zorder=<span class="number">10</span>,</span><br><span class="line">        edgecolor=<span class="string">&#x27;black&#x27;</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">axe.bar(x, ls, width, label=<span class="string">&#x27;Linear Select ($Q=5$)&#x27;</span>, color=<span class="string">&#x27;#e9c46b&#x27;</span>, zorder=<span class="number">10</span>,</span><br><span class="line">        edgecolor=<span class="string">&#x27;black&#x27;</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">axe.bar(x + width + <span class="number">0.05</span>, qs_r, width, color=<span class="string">&#x27;#e66f51&#x27;</span>, label=<span class="string">&#x27;Quick Select (random pivot)&#x27;</span>, zorder=<span class="number">10</span>,</span><br><span class="line">        edgecolor=<span class="string">&#x27;black&#x27;</span>, linewidth=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">axe.set_xlabel(<span class="string">&#x27;Data Scale&#x27;</span>)</span><br><span class="line">axe.set_ylabel(<span class="string">&#x27;Operation Latency (ns)&#x27;</span>)</span><br><span class="line">axe.set_xticks(x)</span><br><span class="line">axe.set_xticklabels(x_labels)</span><br><span class="line"></span><br><span class="line">axe.set_yscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">axe.legend()</span><br><span class="line"></span><br><span class="line">axe.set_title(<span class="string">&#x27;Operation Latency for Unordered Data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="py_imgs/1-2.png" width="400px" /></p><h3 id="0-3-3-堆叠条形图"><a href="#0-3-3-堆叠条形图" class="headerlink" title="0.3.3 堆叠条形图"></a>0.3.3 堆叠条形图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">&#x27;Short (80 B)&#x27;</span>, <span class="string">&#x27;Middle (7.5 KB)&#x27;</span>, <span class="string">&#x27;Large (740 KB)&#x27;</span>]</span><br><span class="line">x_labels = labels</span><br><span class="line">width = <span class="number">0.2</span></span><br><span class="line">x = np.arange(<span class="built_in">len</span>(labels))</span><br><span class="line"></span><br><span class="line">uncompressed = [<span class="number">80</span>, <span class="number">7500</span>, <span class="number">740_000</span>]</span><br><span class="line">uncompressed_ratio = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">single = [<span class="number">50</span>, <span class="number">4000</span>, <span class="number">392_000</span>]</span><br><span class="line">single_dict = [<span class="number">185</span>, <span class="number">477</span>, <span class="number">508</span>]</span><br><span class="line">single_ratio = [single[i] / uncompressed[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels))]</span><br><span class="line">single_dict_ratio = [single_dict[i] / uncompressed[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels))]</span><br><span class="line"></span><br><span class="line">multi = [<span class="number">48</span>, <span class="number">3900</span>, <span class="number">383_000</span>]</span><br><span class="line">multi_dict = [<span class="number">212</span>, <span class="number">504</span>, <span class="number">528</span>]</span><br><span class="line">multi_ratio = [multi[i] / uncompressed[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels))]</span><br><span class="line">multi_dict_ratio = [multi_dict[i] / uncompressed[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels))]</span><br><span class="line"></span><br><span class="line">fig, axe = plt.subplots()</span><br><span class="line"></span><br><span class="line">axe.minorticks_on()</span><br><span class="line">axe.grid()</span><br><span class="line"></span><br><span class="line">axe.bar(x - width - <span class="number">0.05</span>, uncompressed_ratio, width, color=<span class="string">&#x27;gray&#x27;</span>,label=<span class="string">&#x27;Uncompressed&#x27;</span>, zorder=<span class="number">10</span>)</span><br><span class="line">axe.bar(x, single_ratio, width, label=<span class="string">&#x27;Single&#x27;</span>, color=<span class="string">&#x27;#56baf8&#x27;</span>, zorder=<span class="number">10</span>)</span><br><span class="line">axe.bar(x + width + <span class="number">0.05</span>, multi_ratio, width, color=<span class="string">&#x27;#f8566a&#x27;</span>, label=<span class="string">&#x27;Multiple&#x27;</span>, zorder=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">axe.bar(x, single_dict_ratio, width, bottom=single_ratio, color=<span class="string">&#x27;#9fd8fb&#x27;</span>, label=<span class="string">&#x27;Single Dict&#x27;</span>, zorder=<span class="number">10</span>)</span><br><span class="line">axe.bar(x + width + <span class="number">0.05</span>, multi_dict_ratio, width, color=<span class="string">&#x27;#fb9fab&#x27;</span>, bottom=multi_ratio, label=<span class="string">&#x27;Multiple Dict&#x27;</span>, zorder=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">axe.set_xlabel(<span class="string">&#x27;File Type&#x27;</span>)</span><br><span class="line">axe.set_ylabel(<span class="string">&#x27;Compress Ratio&#x27;</span>)</span><br><span class="line">axe.set_xticks(x)</span><br><span class="line">axe.set_xticklabels(x_labels)</span><br><span class="line">axe.legend()</span><br><span class="line"></span><br><span class="line">axe.set_title(<span class="string">&#x27;The Compressed Ratio Comparison&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="py_imgs/comp2.png" width="400px" /></p><h3 id="0-3-4-折线-直方组合图-amp-对数坐标"><a href="#0-3-4-折线-直方组合图-amp-对数坐标" class="headerlink" title="0.3.4 折线-直方组合图 &amp; 对数坐标"></a>0.3.4 折线-直方组合图 &amp; 对数坐标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">&#x27;Short (80 B)&#x27;</span>, <span class="string">&#x27;Middle (7.5 KB)&#x27;</span>, <span class="string">&#x27;Large (740 KB)&#x27;</span>]</span><br><span class="line">x_labels = labels</span><br><span class="line">width = <span class="number">0.2</span></span><br><span class="line">x = np.arange(<span class="built_in">len</span>(labels))</span><br><span class="line"></span><br><span class="line">uncompressed = [<span class="number">80</span>, <span class="number">7500</span>, <span class="number">740_000</span>]</span><br><span class="line"></span><br><span class="line">single = [<span class="number">50</span>, <span class="number">4000</span>, <span class="number">392_000</span>]</span><br><span class="line">single_dict = [<span class="number">185</span>, <span class="number">477</span>, <span class="number">508</span>]</span><br><span class="line"></span><br><span class="line">single_dict_ratio_in_comp = [single_dict[i] / (single_dict[i] + single[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels))]</span><br><span class="line"></span><br><span class="line">multi = [<span class="number">48</span>, <span class="number">3900</span>, <span class="number">383_000</span>]</span><br><span class="line">multi_dict = [<span class="number">212</span>, <span class="number">504</span>, <span class="number">528</span>]</span><br><span class="line"></span><br><span class="line">multi_dict_ratio_in_comp = [multi_dict[i] / (multi_dict[i] + multi[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels))]</span><br><span class="line"></span><br><span class="line">single_total_ratio = [(single[i] + single_dict[i]) / uncompressed[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels))]</span><br><span class="line">multi_total_ratio = [(multi[i] + multi_dict[i]) / uncompressed[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels))]</span><br><span class="line"></span><br><span class="line">fig, axe = plt.subplots()</span><br><span class="line">axe2 = axe.twinx()</span><br><span class="line"></span><br><span class="line">axe.bar(x - width / <span class="number">2</span> - <span class="number">0.03</span>, single_dict_ratio_in_comp, width, color=<span class="string">&#x27;#56baf8&#x27;</span>, label=<span class="string">&quot;Single&quot;</span>)</span><br><span class="line">axe.bar(x + width / <span class="number">2</span> + <span class="number">0.03</span>, multi_dict_ratio_in_comp, width, color=<span class="string">&#x27;#f8566a&#x27;</span>, label=<span class="string">&quot;Multiple&quot;</span>)</span><br><span class="line"></span><br><span class="line">axe.set_xticks(x)</span><br><span class="line">axe.set_xticklabels(x_labels)</span><br><span class="line"></span><br><span class="line">axe.set_xlabel(<span class="string">&#x27;File Type&#x27;</span>)</span><br><span class="line">axe.set_ylabel(<span class="string">&#x27;Dictionary Size Ratio&#x27;</span>)</span><br><span class="line"></span><br><span class="line">axe.minorticks_on()</span><br><span class="line">axe.grid()</span><br><span class="line">axe.set_yscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">axe.legend()</span><br><span class="line"></span><br><span class="line">axe2.scatter(x, single_total_ratio, c=<span class="string">&#x27;k&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">axe2.scatter(x, multi_total_ratio, c=<span class="string">&#x27;k&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">axe2.plot(x, single_total_ratio, <span class="string">&#x27;g--&#x27;</span>, label=<span class="string">&quot;Single in Total&quot;</span>)</span><br><span class="line">axe2.plot(x, multi_total_ratio, <span class="string">&#x27;y-&#x27;</span>, label=<span class="string">&quot;Multiple in Total&quot;</span>)</span><br><span class="line">axe2.legend(loc=(<span class="number">0.66</span>, <span class="number">0.7</span>))</span><br><span class="line"></span><br><span class="line">axe2.set_ylabel(<span class="string">&#x27;Total Compressed Ratio&#x27;</span>)</span><br><span class="line"></span><br><span class="line">axe.set_title(<span class="string">&#x27;Dict Size Ratio &amp; Compressed Ratio Relationship&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="py_imgs/dict2.png" width="400px" /></p><h3 id="0-3-3-样条曲线和插值拟合"><a href="#0-3-3-样条曲线和插值拟合" class="headerlink" title="0.3.3 样条曲线和插值拟合"></a>0.3.3 样条曲线和插值拟合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># File: fit_interp_1d.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> interpolate</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> curve_fit</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Union</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Interpolate1DGen</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x: <span class="type">List</span>[<span class="built_in">float</span>], y: <span class="type">List</span>[<span class="built_in">float</span>], point_cnt: <span class="built_in">int</span></span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        self.point_cnt = point_cnt</span><br><span class="line">        self.sorted_x = <span class="built_in">sorted</span>(self.x)</span><br><span class="line">        self.sorted_y = <span class="built_in">sorted</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">interpolate_polynomial</span>(<span class="params">self, kind: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="built_in">int</span>]</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Polynomial interpolate for 1D data.</span></span><br><span class="line"><span class="string">        :param kind: &#x27;nearest&#x27;, &#x27;zero&#x27;, &#x27;linear&#x27;, &#x27;slinear&#x27;, &#x27;quadratic&#x27;(2), &#x27;cubic&#x27;(3), 4, 5, ...</span></span><br><span class="line"><span class="string">        :return: tuple(interpolated_x, interpolated_y)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        ans_x = np.linspace(</span><br><span class="line">            self.sorted_x[<span class="number">0</span>],</span><br><span class="line">            self.sorted_x[<span class="built_in">len</span>(self.x)-<span class="number">1</span>],</span><br><span class="line">            self.point_cnt</span><br><span class="line">        )</span><br><span class="line">        f_linear = interpolate.interp1d(</span><br><span class="line">            self.x, self.y, kind=kind</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> ans_x, f_linear(ans_x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">interpolate_B_spline</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        B-spline interpolate for 1D data.</span></span><br><span class="line"><span class="string">        **WARNING**: Data sort and non-duplicated will be needed.</span></span><br><span class="line"><span class="string">        :return: tuple(interpolated_x, interpolated_y)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        ans_x = np.linspace(</span><br><span class="line">            self.sorted_x[<span class="number">0</span>],</span><br><span class="line">            self.sorted_x[<span class="built_in">len</span>(self.x)-<span class="number">1</span>],</span><br><span class="line">            self.point_cnt</span><br><span class="line">        )</span><br><span class="line">        tck = interpolate.splrep(self.sorted_x, self.sorted_y)</span><br><span class="line">        y_bSpline = interpolate.splev(ans_x, tck)</span><br><span class="line">        <span class="keyword">return</span> ans_x, y_bSpline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FitLine1DGen</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x: <span class="type">List</span>[<span class="built_in">float</span>], y: <span class="type">List</span>[<span class="built_in">float</span>], point_cnt: <span class="built_in">int</span></span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        self.point_cnt = point_cnt</span><br><span class="line">        self.sorted_x = <span class="built_in">sorted</span>(self.x)</span><br><span class="line">        self.sorted_y = <span class="built_in">sorted</span>(self.y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># S-shape curve fit.</span></span><br><span class="line">    <span class="comment"># Or logistic model.</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x, lp, x0, k, b</span>):</span><br><span class="line">        y = lp / (<span class="number">1</span> + np.exp(-k * (x - x0))) + b</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit_polynomial_curve</span>(<span class="params">self, n: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Polynomial fitting for 1D data.</span></span><br><span class="line"><span class="string">        :param n: (int) the max power of the polynomial.</span></span><br><span class="line"><span class="string">        :return: tuple(fitting_curve_x, fitting_curve_y)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        params = np.polyfit(self.x, self.y, n)</span><br><span class="line">        ans_x = np.linspace(</span><br><span class="line">            self.sorted_x[<span class="number">0</span>],</span><br><span class="line">            self.sorted_x[<span class="built_in">len</span>(self.x)-<span class="number">1</span>],</span><br><span class="line">            self.point_cnt</span><br><span class="line">        )</span><br><span class="line">        ans_y = np.polyval(params, ans_x)</span><br><span class="line">        <span class="keyword">return</span> ans_x, ans_y</span><br><span class="line"></span><br><span class="line">    <span class="comment"># logistic fit</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit_S_curve</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        S Curve fitting for 1D data.</span></span><br><span class="line"><span class="string">        :return: tuple(fitting_curve_x, fitting_curve_y)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># standardize data:</span></span><br><span class="line">        x_min = self.sorted_x[<span class="number">0</span>]</span><br><span class="line">        y_min = self.sorted_y[<span class="number">0</span>]</span><br><span class="line">        x_max = self.sorted_x[<span class="built_in">len</span>(self.x)-<span class="number">1</span>]</span><br><span class="line">        y_max = self.sorted_y[<span class="built_in">len</span>(self.y)-<span class="number">1</span>]</span><br><span class="line">        x_zoomed = (np.array(self.x) - x_min) / (x_max - x_min)</span><br><span class="line">        y_zoomed = (np.array(self.y) - y_min) / (y_max - y_min)</span><br><span class="line">        <span class="comment"># A mandatory initial guess</span></span><br><span class="line">        <span class="comment"># [max(yData), median(xData), 1, min(yData)]</span></span><br><span class="line">        p_guess = [</span><br><span class="line">            <span class="built_in">max</span>(y_zoomed), np.median(x_zoomed),</span><br><span class="line">            <span class="number">1</span>, <span class="built_in">min</span>(y_zoomed)</span><br><span class="line">        ]</span><br><span class="line">        popt, pcov = curve_fit(</span><br><span class="line">            self.sigmoid, x_zoomed, y_zoomed,</span><br><span class="line">            p_guess</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[INFO] curve_fit: popt = &quot;</span>, popt)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[INFO] curve_fit: pcov = &quot;</span>, pcov)</span><br><span class="line"></span><br><span class="line">        fit_x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, self.point_cnt)</span><br><span class="line">        fit_y = self.sigmoid(fit_x, popt[<span class="number">0</span>], popt[<span class="number">1</span>], popt[<span class="number">2</span>], popt[<span class="number">3</span>])</span><br><span class="line">        ans_x = fit_x * (x_max - x_min) + x_min</span><br><span class="line">        ans_y = fit_y * (y_max - y_min) + y_min</span><br><span class="line">        <span class="keyword">return</span> ans_x, ans_y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fit_interp_1d <span class="keyword">import</span> Interpolate1DGen</span><br><span class="line"></span><br><span class="line">element_test_list = [<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">500</span>, <span class="number">1000</span>]</span><br><span class="line">p_test_list = [<span class="number">1</span> / <span class="number">2</span>, <span class="number">1</span> / math.e, <span class="number">1</span> / <span class="number">4</span>, <span class="number">1</span> / <span class="number">8</span>, <span class="number">1</span> / <span class="number">16</span>]</span><br><span class="line">ans = [[<span class="number">8.75</span>, <span class="number">7.45</span>, <span class="number">5.48</span>, <span class="number">5.3</span>, <span class="number">4.92</span>],</span><br><span class="line">       [<span class="number">8.98</span>, <span class="number">7.67</span>, <span class="number">6.19</span>, <span class="number">5.58</span>, <span class="number">6.89</span>],</span><br><span class="line">       [<span class="number">9.72</span>, <span class="number">8.25</span>, <span class="number">6.35</span>, <span class="number">5.53</span>, <span class="number">10.33</span>],</span><br><span class="line">       [<span class="number">11.67</span>, <span class="number">9.22</span>, <span class="number">8.39</span>, <span class="number">8.81</span>, <span class="number">9.52</span>],</span><br><span class="line">       [<span class="number">12.83</span>, <span class="number">10.17</span>, <span class="number">8.56</span>, <span class="number">9.26</span>, <span class="number">10.71</span>]]</span><br><span class="line"></span><br><span class="line">style_list = [<span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;-.&#x27;</span>, <span class="string">&#x27;--&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;:&#x27;</span>]</span><br><span class="line">color_list = [<span class="string">&#x27;#04acf4&#x27;</span>, <span class="string">&#x27;#ff5722&#x27;</span>, <span class="string">&#x27;#ffeb3b&#x27;</span>, <span class="string">&#x27;#4caf50&#x27;</span>, <span class="string">&#x27;#9c27b0&#x27;</span>]</span><br><span class="line">inter_sample_cnt = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> idx, elem <span class="keyword">in</span> <span class="built_in">enumerate</span>(element_test_list):</span><br><span class="line">    aqd = ans[idx]</span><br><span class="line">    ans_p, ans_aqd = Interpolate1DGen(</span><br><span class="line">        p_test_list, aqd, inter_sample_cnt</span><br><span class="line">    ).interpolate_polynomial(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plt.scatter(ans_p, ans_aqd, color=&#x27;k&#x27;, marker=&#x27;x&#x27;)</span></span><br><span class="line">    plt.plot(ans_p, ans_aqd, color=color_list[idx], linestyle=style_list[idx])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Probability $p$&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Average Query Distance (AQD)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;The curves for AQD-probability&#x27;</span>)</span><br><span class="line">plt.minorticks_on()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend(</span><br><span class="line">    labels=(</span><br><span class="line">        <span class="string">&#x27;element: 50&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;element: 100&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;element: 200&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;element: 500&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;element: 1000&#x27;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="py_imgs/AQD_curves.png" width="400px" /></p><h3 id="0-3-4-3D-曲面"><a href="#0-3-4-3D-曲面" class="headerlink" title="0.3.4 3D 曲面"></a>0.3.4 3D 曲面</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mu0 = <span class="number">4</span> * math.pi * <span class="number">10</span> ** (-<span class="number">3</span>)</span><br><span class="line">N = <span class="number">310</span></span><br><span class="line">R = <span class="number">0.14</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># i unit: A</span></span><br><span class="line"><span class="comment"># B unit: 1 Gauss = 10 ** -4 Tesla</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">I2B</span>(<span class="params">i: <span class="built_in">float</span>, x: <span class="built_in">float</span></span>):</span><br><span class="line">    p1 = mu0 * N * math.<span class="built_in">pow</span>(R, <span class="number">2</span>) * i /(<span class="number">2</span> * math.<span class="built_in">pow</span>(math.<span class="built_in">pow</span>(R, <span class="number">2</span>) + math.<span class="built_in">pow</span>(R / <span class="number">2</span> + x, <span class="number">2</span>), <span class="number">3</span> / <span class="number">2</span>))</span><br><span class="line">    p2 = mu0 * N * math.<span class="built_in">pow</span>(R, <span class="number">2</span>) * i /(<span class="number">2</span> * math.<span class="built_in">pow</span>(math.<span class="built_in">pow</span>(R, <span class="number">2</span>) + math.<span class="built_in">pow</span>(R / <span class="number">2</span> - x, <span class="number">2</span>), <span class="number">3</span> / <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> p1 + p2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">theoB</span>(<span class="params">th_B0: <span class="built_in">float</span>, x: <span class="built_in">float</span></span>):</span><br><span class="line">    <span class="keyword">return</span> th_B0 * math.<span class="built_in">pow</span>(<span class="number">5</span>, <span class="number">3</span> / <span class="number">2</span>) / <span class="number">16</span> * (<span class="number">1</span> / math.<span class="built_in">pow</span>(<span class="number">1</span> + math.<span class="built_in">pow</span>(<span class="number">0.5</span> + x / R, <span class="number">2</span>), <span class="number">3</span> / <span class="number">2</span>) + <span class="number">1</span> / math.<span class="built_in">pow</span>(<span class="number">1</span> + math.<span class="built_in">pow</span>(<span class="number">0.5</span> - x / R, <span class="number">2</span>), <span class="number">3</span> / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    p_cnt = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    axe = plt.axes(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">    plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 用来正常显示负号</span></span><br><span class="line">    xx = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">    yy = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">    mesh_val = [</span><br><span class="line">        [<span class="number">1.025</span>, <span class="number">1.024</span>, <span class="number">1.024</span>, <span class="number">1.024</span>, <span class="number">1.023</span>, <span class="number">1.020</span>, <span class="number">1.016</span>],  <span class="comment"># y = 0</span></span><br><span class="line">        [<span class="number">1.025</span>, <span class="number">1.024</span>, <span class="number">1.024</span>, <span class="number">1.024</span>, <span class="number">1.023</span>, <span class="number">1.020</span>, <span class="number">1.016</span>],  <span class="comment"># y = 0.05R</span></span><br><span class="line">        [<span class="number">1.024</span>, <span class="number">1.027</span>, <span class="number">1.024</span>, <span class="number">1.024</span>, <span class="number">1.023</span>, <span class="number">1.022</span>, <span class="number">1.018</span>],  <span class="comment"># y = 0.10R</span></span><br><span class="line">        [<span class="number">1.024</span>, <span class="number">1.024</span>, <span class="number">1.024</span>, <span class="number">1.024</span>, <span class="number">1.025</span>, <span class="number">1.024</span>, <span class="number">1.021</span>],  <span class="comment"># y = 0.15R</span></span><br><span class="line">        [<span class="number">1.023</span>, <span class="number">1.023</span>, <span class="number">1.024</span>, <span class="number">1.025</span>, <span class="number">1.027</span>, <span class="number">1.026</span>, <span class="number">1.024</span>],  <span class="comment"># y = 0.20R</span></span><br><span class="line">        [<span class="number">1.021</span>, <span class="number">1.021</span>, <span class="number">1.023</span>, <span class="number">1.026</span>, <span class="number">1.028</span>, <span class="number">1.030</span>, <span class="number">1.030</span>],  <span class="comment"># y = 0.25R</span></span><br><span class="line">        [<span class="number">1.018</span>, <span class="number">1.019</span>, <span class="number">1.021</span>, <span class="number">1.027</span>, <span class="number">1.031</span>, <span class="number">1.033</span>, <span class="number">1.037</span>]   <span class="comment"># y = 0.30R</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">U2B</span>(<span class="params">u: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="keyword">return</span> (u - <span class="number">4.61538</span> * math.<span class="built_in">pow</span>(<span class="number">10</span>, -<span class="number">4</span>)) / <span class="number">0.25396</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Z_func</span>(<span class="params">x: <span class="built_in">int</span>, y: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="keyword">return</span> mesh_val[x][y]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(mesh_val)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(mesh_val[<span class="number">0</span>])):</span><br><span class="line">            mesh_val[i][j] = U2B(mesh_val[i][j])</span><br><span class="line"></span><br><span class="line">    rawX, rawY = np.meshgrid(xx, yy)</span><br><span class="line"></span><br><span class="line">    ap_all = np.vectorize(Z_func)</span><br><span class="line">    Z = ap_all(rawX, rawY)</span><br><span class="line">    <span class="built_in">print</span>(Z)</span><br><span class="line"></span><br><span class="line">    toReal = np.vectorize(<span class="keyword">lambda</span> x: x * <span class="number">0.05</span> * R)</span><br><span class="line">    rxx = toReal(xx)</span><br><span class="line">    ryy = toReal(yy)</span><br><span class="line">    X, Y = np.meshgrid(rxx, ryy)</span><br><span class="line"></span><br><span class="line">    axe.plot_surface(X, Y, Z, cmap=<span class="string">&#x27;jet&#x27;</span>)</span><br><span class="line">    axe.plot_wireframe(X, Y, Z, color=<span class="string">&#x27;k&#x27;</span>, linewidth=<span class="number">0.3</span>)</span><br><span class="line">    <span class="comment"># axe.minorticks_on()</span></span><br><span class="line">    axe.set_xlabel(<span class="string">&#x27;X position ($m$)&#x27;</span>)</span><br><span class="line">    axe.set_ylabel(<span class="string">&#x27;Y position ($m$)&#x27;</span>)</span><br><span class="line">    axe.set_zlabel(<span class="string">&#x27;Magnetic field density B ($G$)&#x27;</span>)</span><br><span class="line">    axe.set_title(<span class="string">&#x27;Space magnetic field distribution&#x27;</span>)</span><br><span class="line">    axe.grid(which=<span class="string">&quot;major&quot;</span>, alpha=<span class="number">0.6</span>)</span><br><span class="line">    axe.grid(which=<span class="string">&quot;minor&quot;</span>, alpha=<span class="number">0.3</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="py_imgs/AQD.png" width="400px" /></p><h3 id="0-3-5-输出图片"><a href="#0-3-5-输出图片" class="headerlink" title="0.3.5 输出图片"></a>0.3.5 输出图片</h3><p>除了使用 OpenCV 库的 <code>imshow</code>，Matplotlib 也支持输出图片：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> cv2 <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = imread(<span class="string">&#x27;assets/cat.jpg&#x27;</span>)</span><br><span class="line">img_tinted = img * [<span class="number">1</span>, <span class="number">0.95</span>, <span class="number">0.9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the original image</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the tinted image</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A slight gotcha with imshow is that it might give strange results</span></span><br><span class="line"><span class="comment"># if presented with data that is not uint8. To work around this, we</span></span><br><span class="line"><span class="comment"># explicitly cast the image to uint8 before displaying it.</span></span><br><span class="line">plt.imshow(np.uint8(img_tinted))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;0-1-NumPy&quot;&gt;&lt;a href=&quot;#0-1-NumPy&quot; class=&quot;headerlink&quot; title=&quot;0.1 NumPy&quot;&gt;&lt;/a&gt;0.1 NumPy&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Numpy&lt;/code&gt; 库是 Python 科学计算的核心。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="Programming" scheme="https://blog.sjtuxhw.top/tags/Programming/"/>
    
    <category term="Math" scheme="https://blog.sjtuxhw.top/tags/Math/"/>
    
    <category term="Python" scheme="https://blog.sjtuxhw.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>OpenHarmony Hilog 架构趣读</title>
    <link href="https://blog.sjtuxhw.top/technical/hilog-paper/"/>
    <id>https://blog.sjtuxhw.top/technical/hilog-paper/</id>
    <published>2024-10-29T05:14:04.000Z</published>
    <updated>2025-07-27T10:28:17.616Z</updated>
    
    <content type="html"><![CDATA[<p>最近看到一篇讨论 OpenHarmony Hilog 日志子系统的设计的论文，遂进行了一番阅读。该论文发表在软件学报上。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol><li>分析当今主流日志系统的技术架构和优缺点；</li><li>基于 <code>OpenHarmony</code> 操作系统的异构设备互联特性，设计 <code>HiLog</code> 日志系统模型规范；</li><li>设计并实现第 1 个面向 <code>OpenHarmony</code> 的日志系统 <code>HiLog</code>, 并贡献到 <code>OpenHarmony</code> 主线；</li><li>对 <code>HiLog</code> 日志系统的关键指标进行测试和对比试验；</li></ol><p>实现的 <code>HiLog</code> 具有以下特征：</p><ul><li>基础性能：日志写入阶段吞吐量分别为 1 500 KB/s 和 700 KB/s，吞吐量相对 Android Log 提升 114%；</li><li>日志持久化：压缩率 3.5%，丢包率 0.6%；</li><li>数据安全、流量控制等等新型实用能力；</li></ul><h2 id="背景概述"><a href="#背景概述" class="headerlink" title="背景概述"></a>背景概述</h2><p>地位：在计算机系统中，日志作为一种基于时间序列的数据，记录了在操作系统中发生的事件或其他软件运行的事件。</p><p>作用：</p><ul><li>实用价值：系统开发和运维人员需要通过日志对程序中存在的问题进行定位和分析，提高工作效率；</li><li>商业价值：日志记录了大量用户行为习惯信息，这些信息通过大数据分析可用于了解用户需求，作为改进产品或孵化新的商业项目的依据；</li></ul><p>目前产业界的日志系统：Android Log、FTrace、NanoLog、Log4j2 等等；</p><p><code>OpenHarmony</code> 日志系统需要具备的功能：生成、过滤、记录和消息分析的能力。</p><ul><li>多进程日志读写：<code>OpenHarmony</code> 是支持多进程并发的操作系统，其日志系统需要具备从多进程收集日志的能力；</li><li>实时日志读写：作为操作系统的高效调试辅助工具，日志系统需要具备<strong><u>事件发生-日志输出</u></strong>的实时响应能力；</li><li>多内核适配：<code>OpenHarmony</code> 是多内核的操作系统，其日志系统需要具备多种内核适配能力；</li></ul><p>经过分析，目前产业界的日志系统均不适合 <code>OpenHarmony</code>：</p><ul><li><p><code>Log4j2</code> 单进程日志架构；</p><ul><li>借助 CAS 实现缓冲区加解锁，降低日志读写接口的延时；</li><li>缓存行填充解决伪共享问题，隔离更新操作，进一步提升运行效率；</li><li>CAS 方法的 CPU 开销较大（也就是适用于单进程的日志，不适用多进程并发的状况），且存在日志缓冲区修改的 ABA 问题 (ABA problem)；</li></ul></li><li><p><code>NanoLog</code> 虽然日志写入效率很高（大量数据操作以二进制形式完成），但是：</p><ul><li>其读取需要复杂的后处理机制（反序列化、格式化、排序等）；</li><li>采用空间换时间的策略，内存消耗大；</li></ul><p>因此，不能满足 OS 调试所需实时读日志需求；</p></li><li><p><code>FTrace</code> 日志系统仅适用于内核日志读写，使用就和 Linux 强耦合，不适用于多内核的 <code>OpenHarmony</code>；</p><ul><li>每个 CPU 上维护一个缓冲区，因此读写时延低（与前述日志系统“为每个操作系统维护唯一缓冲区”的设计理念不同）；</li><li>Page 结构组织数据，单 Page 上记录一个时间戳，Page 内日志记录相对时间差，节约记录时间所需的存储空间；</li></ul></li><li><p>Android Log（5.0 后）满足内核解耦、多进程、实时读写的需求，如下图：</p><p><img src="imgs/android-log.png" width="400px" /></p><ul><li>日志写：IPC 采用原生 Socket + <code>/proc/kmsg</code> 内核日志；</li><li>日志缓存：<code>logd</code> 用户态 list buffer；</li><li>日志读：<code>logcat</code> 使用 Socket 读；</li></ul><p>但是有 4 个关键问题：</p><ul><li><strong><u>吞吐量不足</u></strong>：负载超过吞吐量将会导致严重的日志丢包问题；</li><li><strong><u>缺乏资源分配机制</u></strong>：没有对日志资源的使用进行合理分配或约束，写日志进程间可能出现资源竞争；</li><li><strong><u>缺乏数据安全能力</u></strong>：未提供相应的敏感数据保护功能, 任何读权限日志用户均可阅读全部日志信息；</li><li><strong><u>面向轻量设备的兼容性差</u></strong>：没有特别面向资源受限设备进行兼容设计。Android Log 在用户态维护独立缓冲区（list buffer），保存包括 Linux 内核日志在内的所有日志数据, 因此需要消耗大量的内存资源；</li></ul></li><li><p>Fuchsia OS 中的日志系统：Socket 通信（类似 Android Log），不过使用链表作为缓冲区（有效利用碎片化的内存空间）。但是存在<u>内存拷贝次数多、用户态内存频繁分配释放</u>的问题；</p></li></ul><p>上面的案例中，只有 Android Log 最接近要求。但它的问题也亟需优化。</p><h2 id="HiLog-日志系统模型规范"><a href="#HiLog-日志系统模型规范" class="headerlink" title="HiLog 日志系统模型规范"></a>HiLog 日志系统模型规范</h2><p>基于以上分析，本文将要设计面向 OpenHarmony 操作系统的高性能日志系统 HiLog。首先，为了明确日志系统的研发目标与技术特点，文章为 HiLog 设计了相应的模型规范。</p><p>虽然 HiLog 与 Android 日志系统有相同的基础架构，但提出了更多的场景要求，以及原则：</p><ul><li><p>性能原则：应当针对高吞吐量需求进行设计，从软件层面解决吞吐量瓶颈问题（以及引发的丢包问题）；</p></li><li><p>资源分配原则：从操作系统层面看，日志系统作为操作系统的信息记录者，不应抢占过多的系统资源。应当在设计时考虑资源分配问题；</p><ul><li>一方面在操作系统层面合理分配日志系统和其他程序占用的资源；</li><li>另一方面是在日志系统层面合理分配各个程序占用的日志资源；</li></ul></li><li><p>设备兼容性原则：OpenHarmony 操作系统即是一种面向全设备的操作系统，因此需要考虑资源受限的轻量化设备, 如蓝牙耳机、键盘、智能音箱、传感器等。</p><p><img src="imgs/oh-platform.png" width="600px" /></p><p>需要注意：减小 CPU、内存、存储空间占用。</p></li><li><p>数据安全原则：常见的隐私保护方法有匿名化、同态加密、差分隐私等等，由于它们需要基于静态的、结构相同的数据集合计算数据之间的相关性，因此难以适用：</p><ul><li>基于时间序列意味着日志数据是随时间快速更新的, 每次更新都需要重新计算数据之间的关系, 计算开销是昂贵的；</li><li>长文本缺乏字段概念, 日志语句的长短、句式各不相同 (结构不同), 难以基于规则分辨需要保护的内容；</li></ul><p>因此 HiLog 应当具备一定的日志数据安全能力, 但是同时需要保证轻量化。</p></li></ul><h2 id="HiLog-系统设计实现"><a href="#HiLog-系统设计实现" class="headerlink" title="HiLog 系统设计实现"></a>HiLog 系统设计实现</h2><h3 id="1-日志类型"><a href="#1-日志类型" class="headerlink" title="1. 日志类型"></a>1. 日志类型</h3><p>OpenHarmony 操作系统由下至上分为内核层、系统层和应用层：</p><ul><li>内核层（对应内核开发者）：可由面向标准系统的 Linux 内核或面向轻量系统的 LiteOS 内核构成；</li><li>系统层（对应系统开发者）：主要由 OpenHarmony 操作系统的各个子系统构成；</li><li>应用层（对应应用开发者）：由运行在 OpenHarmony 上的系统应用以及第三方应用构成；</li></ul><p>不同开发者关心的信息是不同的. 因此为了方便开发者区分不同层级产生的日志, HiLog 将日志分为内核日志、系统日志和应用日志 3 类, 并实现日志的<strong><u>分类管理</u></strong>。</p><h3 id="2-日志级别"><a href="#2-日志级别" class="headerlink" title="2. 日志级别"></a>2. 日志级别</h3><p>为了方便开发者和运维人员<strong><u>快速分辨系统状态的严重程度</u></strong>, 日志应当基于记录事件的重要程度划分级别。</p><p>标准需要：</p><ul><li>日志的级别数目不应过多或过少, 防止检索困难或分类标准不明；</li><li>每个日志级别应当有清晰的使用标准, 开发者在开发时不可混用；</li><li>写入时, 每条日志都应当分配到一个日志级别；</li><li>在输出时, 每个级别的日志都需要采用不同的字体或者颜色来区分；</li></ul><p>因此 HiLog 分为：</p><p><img src="imgs/hilog-level.png" width="350px" /></p><h3 id="3-日志数据结构"><a href="#3-日志数据结构" class="headerlink" title="3. 日志数据结构"></a>3. 日志数据结构</h3><p>按位紧密存储（packed），节省空间可以减少 IPC 开销和存储开销, 提高日志系统的日志吞吐量。</p><p><img src="imgs/hilog-datatype.png" width="350px" /></p><h3 id="4-日志功能"><a href="#4-日志功能" class="headerlink" title="4. 日志功能"></a>4. 日志功能</h3><ul><li><p>日志写入：包括日志生成、日志排序、日志暂存。</p><ul><li>在开发时 HiLog 的使用者通过引入 libhilog 的头文件, 使用 libhilog 提供的写日志接口编写程序, 在程序运行时 libhilog 即可生成日志；</li><li>libhilog 在生成日志过程中还提供数据保护、进程流控等辅助能力；</li><li>在标准 HiLog 中, hilogd 收集来自各个 libhilog 的日志信息, 按时间进行排序, 并进入缓冲区暂存. 在轻量 HiLog 中, 日志缓冲区是 LiteOS 的 <code>kernel_log_buffer</code>, 相应的日志排序和暂存能力由 <code>kernel_log_buffer</code> 实现；</li></ul></li><li><p>日志输出：包括日志打印、日志持久化。</p><ul><li>读取暂存的日志写入到标准输出 (<code>stdout</code>), 并且支持通过辅助信息等特征进行筛选；</li><li>将暂存的日志写入文件, 并进一步地提供压缩功能；</li></ul><p>可以通过 hilogtool 命令行工具执行日志打印、持久化等输出工作；</p></li><li><p>日志系统控制：包括数据安全、进程流控、业务流控、缓冲区以及持久化的配置；</p><ul><li>例如, 当操作系统内存空间紧张, 可以缩小日志缓冲区空间, 为其他程序让出更多内存；又例如, 当操作系统 CPU 负载较高, 可以降低流量控制阈值, 减少 HiLog 日志处理消耗的 CPU 资源；</li></ul></li></ul><h3 id="5-架构-amp-模块设计"><a href="#5-架构-amp-模块设计" class="headerlink" title="5. 架构 &amp; 模块设计"></a>5. 架构 &amp; 模块设计</h3><p><img src="imgs/hilog-arch.png" width="500px" /></p><ul><li>标准 Hilog（L2-L5）：维护守护进程 hilogd 实现高性能的日志缓冲区管理；</li><li>轻量 Hilog（L1）：直接将日志写入内核的缓冲区中；</li></ul><p>其中：</p><ul><li><p><code>libhilog</code>：提供头文件 &amp; 动态链接库。一方面提供静态写日志接口, 另一方面负责运行时日志生成。附加：</p><ul><li>写日志接口的敏感数据标识, 实现数据安全；</li><li>基于进程的日志流控机制, 实现对所有进程日志写入资源的合理分配；</li></ul><p>轻量 Hilog：添加敏感数据标识和流控后, 将日志直接写入内核缓冲区；</p><p>标准 Hilog：直接发送至 <code>hilogd</code> 模块；</p></li><li><p><code>hilogtool</code>：提供读日志能力。一方面提供与操作系统 Shell 交互的能力, 另一方面负责执行读日志任务。</p><ul><li>开发者通过 Shell 命令控制日志打印或日志持久化任务；</li><li>根据平台种类，从不同位置读取日志；</li></ul></li><li><p><code>hilogd</code>：面向 L2–L5 平台设计的高性能日志缓冲区 (<code>hilog_buffer</code>) 及其管理模块；</p><ul><li>与系统的其他模块是解耦的（IPC 交互）；</li><li>提供日志监听、排序和存储的功能, 其运行时具备系统守护进程的特性；</li></ul></li></ul><p>图中“内核缓冲区”含义不同：</p><ul><li>轻量 Hilog：指 LiteOS 内核的内核日志缓冲区, 负责暂存全量的日志信息；</li><li>标准 Hilog：指 Linux 的内核日志缓冲区, hilogd 将会读取其中的日志信息到 <code>hilog_buffer</code>, 保证 <code>hilog_buffer</code> 中拥有系统的全量日志信息；</li></ul><h3 id="6-HiLog-日志系统-IPC"><a href="#6-HiLog-日志系统-IPC" class="headerlink" title="6. HiLog 日志系统 IPC"></a>6. HiLog 日志系统 IPC</h3><p><img src="imgs/hilog-ipc.png" width="550px" /></p><ul><li><p><code>socket_input</code> 服务端：采用 I/O Multiplxing，而不是多线程策略。因为下面的原因导致单独线程处理会浪费资源：</p><ul><li>日志的写入存在并发特征；</li><li>每个进程在每个时间段产生的日志数量是不定的, 且每一条日志的长度 (字节数) 也是不等的, 因此日志的写入数据量存在时间分布不均匀特征；</li></ul></li><li><p><code>socket_input</code> 客户端：采用非阻塞 IO 模型 (non-blocking input/output)，异步传输；由于服务端 I/O Multiplxing 不能保证及时处理，因此同步方式会使进程阻塞。</p></li><li><p><code>socket_output</code> 客户端/服务端均采用阻塞 IO (blocking input/output) 模型构建；</p><ul><li>读日志事件的数据量较大且需要确保数据到达的先后顺序；</li><li>从需求分析不会同时存在太多读日志进程, 因此阻塞 IO 不会给系统带来过大的负担；</li></ul><p>因此：</p><ul><li><p><code>hilogtool</code>, 维护各自的 <code>socket_output</code> 客户端向 <code>hilogd</code> 发送读日志请求；</p></li><li><p><code>hilogd</code> 对于每一个客户端创建一个线程操作 <code>socket_output</code> 服务端；</p></li></ul></li></ul><p>而对于轻量 HiLog 而言，IPC 机制直接采用 <code>ioctl</code>；</p><h3 id="6-HiLog-日志数据安全"><a href="#6-HiLog-日志数据安全" class="headerlink" title="6. HiLog 日志数据安全"></a>6. HiLog 日志数据安全</h3><p>为了平衡安全性和性能开销, 在设计 HiLog 的数据安全能力时<strong><u>重点考虑了变量的安全问题</u></strong>。仅基于静态的源码分析难以捕捉是否有敏感数据会被日志系统记录, 因此<strong><u>变量是敏感数据泄露的重要风险因素</u></strong>。</p><p>开发者指定变量的敏感标识，HiLog 通过识别这些标识来提供数据安全能力。</p><p>敏感数据标识分为 2 种, 分别是公开标识 <code>&#123;public&#125;</code> 和隐藏标识 <code>&#123;private&#125;</code>，例如 <code>&quot;%&#123;public&#125;s&quot;</code>，<code>&quot;%&#123;private&#125;s&quot;</code>；</p><p>若修饰 <code>&#123;private&#125;</code>, 在开启数据安全能力的情况下, libhilog 会以字符串 <code>&quot;&lt;private&gt;&quot;</code> 替换原参数后, 再将对应日志发送到 <code>hilog_buffer</code>；</p><p><img src="imgs/hilog-safety-rule.png" width="550px" /></p><h3 id="7-HiLog-日志流量控制"><a href="#7-HiLog-日志流量控制" class="headerlink" title="7. HiLog 日志流量控制"></a>7. HiLog 日志流量控制</h3><p>作为实现系统资源合理分配的手段, HiLog 提供日志流量控制 (简称流控) 机制, 避<strong>免部分进程日志流量过大造成的系统负载过高和日志丢包问题</strong>。</p><p>流量控制原理：</p><p><img src="imgs/hilog-flow-control.png" width="600px" /></p><ul><li>设置流量阈值 $q$，每个时间片段 Δt 内统计日志流量, 当某个时间片段内的日志流量超出阈值 q 时, 按照默认配额或者进程白名单设置的配额进行控制, 对超出配额的日志进行抛弃；</li><li>这种控制方法会在 进程端（<code>libhilog</code>）和业务端（<code>hilogd</code>）同时开展，可以平衡 IPC 资源的使用并降低性能开销；</li><li>除了从进程端进行流控，HiLog 可以跨进程针对同一业务流控。OH 使用 <code>Domain</code>（领域标识）将进程归类，具备相同 Domain 的进程被归纳为同一业务，然后在业务粒度上进行流控。</li></ul><p>注：轻量 HiLog 不存在 <code>hilogd</code>，不存在业务流控，只有进程流控。实际上本身轻量级设备无法运行大量进程，因此这么做本身没有问题。</p><h3 id="8-HiLog-日志缓冲区管理"><a href="#8-HiLog-日志缓冲区管理" class="headerlink" title="8. HiLog 日志缓冲区管理"></a>8. HiLog 日志缓冲区管理</h3><p>对于标准 HiLog 而言，使用如图<strong><u>双循环链表</u></strong>作为缓冲区 <code>hilog_buffer</code> 的数据结构：</p><p><img src="imgs/hilog-buffer.png" width="400px" /></p><ul><li>高效利用碎片化的内存空间；</li><li>有效降低日志的排序、插入、读取等操作时指针需要跳转的链表节点数目；</li></ul><p>其特性如下：</p><ol><li><p>时间戳有序（为了给开发和维护人员提供符合逻辑的信息）：<code>hilog_buffer</code> 中的数据按照日志数据的时间戳排序。这个特性需要编码时保证，因为 OH 是分时操作系统，随机的延时会导致日志产生时间顺序与到达缓冲区的时间顺序不一致，进而增大日志理解难度。因此提供两种排序方案：</p><ul><li>先排序：日志在写入日志缓冲区时进行排序；</li><li>后排序：从缓冲区读取日志进行输出时排序；</li></ul><p>由于 HiLog 单生产者、多消费者模型，因此采用 “先排序” 的方案。</p><blockquote><p>原因如下：</p><ul><li>“先排序”方案执行一次排序即可解决顺序问题。并且由于缓冲区内日志有序, 排序是较为简单的, 只需将新到日志的时间戳依次与缓冲区日志的时间戳进行比较 (由新到旧) 然后插入即可；</li><li>“后排序”需要每个消费者都进行排序, 由于缓冲区内日志无序, 消费者需要遍历一次链表才能排序一条日志, 效率较低；</li></ul></blockquote></li><li><p>读写指针：<code>hilog_buffer</code> 包含 3 类读写指针成员：</p><ul><li>写指针 $w$：指向当前最新日志结点；</li><li>公共读指针 $r$：指向当前最旧的日志节点；</li><li>读者指针 $r_i$：位于 $w$ 和 $r$ 间若干份，以供不同的日志读需求；</li></ul></li><li><p>单生产者、多消费者模型：</p><ul><li>在 <code>hilogd</code> 中只运行一个生产者线程。插入日志时扫描日志应该处于的时间区间并移动 $w$ 插入（可能在 $w$ 当前指向结点前或后）；如果新插入的日志记录在 $r$ 的前面，则将 $r$ 移动到该结点。最终 $w$ 需要复位至时间最新的结点；</li><li>每个消费者独占 $r_i$​ 指针，起始时将读指针 $r_i$ 指向公共读指针 $r$ 所指节点, 接下来操作 $r_i$ 依次读取后续节点；</li></ul></li><li><p><code>hilog_buffer</code> 线程同步：仅处理生产者和消费者间的并发同步问题就行（读者间不存在同步问题）。</p><ul><li>对 “消费者跳转读指针” 和 “生产者调整链表结构” 这两类过程加锁即可；</li></ul><p>考虑两种锁：线程互斥锁（<code>mutex</code>）和 CAS（Compare And Swap）；</p><blockquote><p>mutex:  CPU 占用低、不存在 ABA Problem；缺点在于存在 domain switch 开销；</p><p>CAS:  无需 domain switch；单需要轮询锁状态, CPU 消耗大, 并且存在 ABA 风险；</p><p>综合考虑：</p><ul><li>资源消耗：在单生产者、多消费者的场景下，多个 client 轮询 CAS 可能对移动终端设备不友好；</li><li>临界区执行时间较长：使用 mutex 带来的 domain switch 开销不显著。</li></ul></blockquote></li><li><p>缓冲区容量可定制：缓冲区容量上限可配置。当 hilog_buffer 容量已达上限时, 插入数据会导致时间戳最早的一部分数据被删除, 以存储最新的数据；总体过程：</p><ol><li>将公用读指针 $r$ 指向<strong>剩余数据（不包括将要被删除的结点）</strong>中最旧的节点；</li><li>遍历检查读者列表中每个读者的读指针是否指向将被删除的节点，如果是，则移至 $r$；</li><li>最终释放要被删除的结点；</li></ol></li></ol><h3 id="9-HiLog-日志系统持久化-amp-压缩"><a href="#9-HiLog-日志系统持久化-amp-压缩" class="headerlink" title="9. HiLog 日志系统持久化 &amp; 压缩"></a>9. HiLog 日志系统持久化 &amp; 压缩</h3><p>日志持久化：</p><p>使用一般的日志轮替机制。指定日志文件的总数和每个日志文件的大小，日志系统的内存部分大小与一个日志文件大小匹配。</p><p>当正在写入的文件超过规定阈值后创建新日志文件，当日志文件数量超过规定阈值时删除最旧的日志文件；</p><p>日志压缩：</p><p>提供两种不同的日志压缩方法, 一种是日志流压缩（从日志缓冲区读取日志, 接下来将日志作为比特流输入压缩算法接口。使用 zlib 流压缩算法库），另一种是日志文件压缩（主要面向小流量的写日志场景，先创建临时日志文件，后续压缩并删除）。</p><blockquote><p>当日志流量较小时, 如果采用流压缩方法, 压缩算法的输出数据量达到单文件大小阈值需要很长的时间, 期间一旦出现系统崩溃或断电问题, 就会导致内存中的日志数据丢失。</p></blockquote><h2 id="HiLog-日志系统实验分析"><a href="#HiLog-日志系统实验分析" class="headerlink" title="HiLog 日志系统实验分析"></a>HiLog 日志系统实验分析</h2><p>基本性能、流控性能、持久化性能、设备兼容性分析、数据安全能力分析。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>完成了前述的几项基本要求、遵循了前述的设计原则；</p><p>同时也反映出 <code>HiLog</code> 的一些问题与改进空间：</p><ol><li><p>业界对于日志系统的数据安全的研究较少, HiLog 的轻量化数据安全能力是对于日志数据安全问题的初步探索；</p></li><li><p>OpenHarmony 作为分布式操作系统, 原生支持分布式能力。</p><p><strong><u>然而目前 HiLog 尚不具备从多设备统一收集日志并进行管理的能力</u></strong>. 这种缺陷对于分布式能力的开发和调试造成了一定的不便, 具备优化的空间；</p><p>构造分布式日志系统有两个重要的问题需要解决, 其一是设备间高速、高稳定的连接问题, 其二是多设备的时钟同步问题。</p><p>对于第 1 个问题, 可以等待 OpenHarmony 的软总线 (SoftBus) 技术成熟后，利用 SoftBus 作为稳定高速的日志传输的通道；</p><p>对于第 2 个问题, 可以考虑基于精确时间协议 (precision time protocol, PTP) 实现无线局域网内的多设备时钟同步；</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近看到一篇讨论 OpenHarmony Hilog 日志子系统的设计的论文，遂进行了一番阅读。该论文发表在软件学报上。&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;ol&gt;
</summary>
      
    
    
    
    <category term="technical" scheme="https://blog.sjtuxhw.top/categories/technical/"/>
    
    
    <category term="HarmonyOS" scheme="https://blog.sjtuxhw.top/tags/HarmonyOS/"/>
    
    <category term="OS" scheme="https://blog.sjtuxhw.top/tags/OS/"/>
    
    <category term="Paper" scheme="https://blog.sjtuxhw.top/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Makefile 快速上手 (again)</title>
    <link href="https://blog.sjtuxhw.top/review/makefile-again/"/>
    <id>https://blog.sjtuxhw.top/review/makefile-again/</id>
    <published>2024-10-11T02:05:34.000Z</published>
    <updated>2024-10-31T02:19:47.509Z</updated>
    
    <content type="html"><![CDATA[<p>说来惭愧，之前笔者还认为 Makefile 这种工具已经过时，只需要学 CMake 就行。</p><p>但最近在写 boot loader 时遇到了一些问题：我既不是在编译可执行文件，也不是在编译库，这样 CMake 就显得比较无力了，因为总是用 <code>add_custom_*</code> 也不是办法，非常臃肿——毕竟不是在管理一个 C/C++ 应用的项目嘛。所以决定再整理一下 Makefile 的写法。</p><p>本文充当一个 Makefile cheat sheet 的作用，自己有点遗忘的时候回来查一查。</p><hr><h2 id="Define-a-Target"><a href="#Define-a-Target" class="headerlink" title="Define a Target"></a>Define a Target</h2><p>在 Makefile 中定义一个可以构建的 target：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">&lt;target&gt;: dependency1 dependency2 ... dependency3</span></span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandM</span><br></pre></td></tr></table></figure><p>这样可以使用 <code>make &lt;target&gt;</code> 来执行它。</p><p>注意哦，make 会认为 <code>&lt;target&gt;</code> 是一个需要构建的目标文件名。最终按照 commands 生成的文件会被命名为 <code>target</code>；</p><h2 id="Use-Variables"><a href="#Use-Variables" class="headerlink" title="Use Variables"></a>Use Variables</h2><p>Makefile 中定义变量也很方便：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">variable_name = file1 file2 ... fileN</span><br><span class="line"><span class="comment"># 注：:= 和 += 和 ?= 不经常用，不放在这里了</span></span><br><span class="line"><span class="comment"># 感兴趣自行查阅，或者查看之前介绍 cmake 的文章</span></span><br></pre></td></tr></table></figure><p>然后和 shell 类似直接用 <code>$()</code> 包裹使用：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">myTarget : <span class="variable">$(var1)</span></span><br><span class="line">    some-shell-command <span class="variable">$(var2)</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$(var3)</span> : dep1 dep2 ... depN</span><br><span class="line">    command1</span><br><span class="line">    ...</span><br><span class="line">    commandM</span><br></pre></td></tr></table></figure><p>Makefile 中还可以使用<strong><u>环境变量</u></strong>，和普通变量用起来一样。你还可以通过执行 make 时的 <code>-e</code> 参数来 override 对应的环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -e name=value myTarget</span><br></pre></td></tr></table></figure><h2 id="Use-Shell-Results"><a href="#Use-Shell-Results" class="headerlink" title="Use Shell Results"></a>Use Shell Results</h2><p>Makefile 还可以直接使用 shell 的输出结果，和变量一样用 <code>$()</code> 包裹：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">CC=gcc</span><br><span class="line">AS=$&#123;CC&#125; -c</span><br><span class="line">LD=ld</span><br><span class="line">PROJ_NAME=main.bin</span><br><span class="line"></span><br><span class="line"><span class="section">all: $&#123;PROJ_NAME&#125; log</span></span><br><span class="line"></span><br><span class="line"><span class="section">log:</span></span><br><span class="line">    <span class="comment"># 在字符串里和命令里都能用</span></span><br><span class="line">    echo <span class="string">&quot;build at: $(shell date --iso)&quot;</span> &gt; <span class="variable">$(<span class="built_in">shell</span> data --iso)</span>.log</span><br><span class="line"></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">    rm -f *.o *.bin</span><br><span class="line"></span><br><span class="line"><span class="section">PROJ_NAME:</span></span><br><span class="line">    <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><h2 id="Phony-Commands"><a href="#Phony-Commands" class="headerlink" title="Phony Commands"></a>Phony Commands</h2><p>Makefile 不仅仅可以用于管理项目的编译流程，还能定制一些自动化的指令。例如一个没有任何 dependencies 的 target 就可以被 make 直接执行其中的指令：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">clean:</span></span><br><span class="line">    rm -f *.o *.a</span><br><span class="line">    echo <span class="string">&quot;Finished.&quot;</span> &gt; my.log</span><br></pre></td></tr></table></figure><p>但由于 make <u>按照当前 target 文件是否存在、依赖的 dependencies 的时间戳来判断增量执行</u>，假设你创建了一个名为 <code>clean</code> 的文件，很可能 make 就不会再执行上面的指令了：因为 make 认为构建的目标文件已经构建完成了，并没有把它作为一组指令看待。</p><p>为了区分指令组，以及真正的 target 目标文件，make 允许在 Makefile 中使用伪指令 <code>.PHONY</code>，以此来标识这个 target 仅仅是一组指令，每次调用时执行它就行。例如：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.PHONY clean rebuild    <span class="comment"># 可以指定多个</span></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">    rm -f *.o *.a</span><br><span class="line">    echo <span class="string">&quot;Finished.&quot;</span> &gt; my.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指令间也可以相互依赖，这无论是管理项目编译，还是其他自动化用途，都很方便</span></span><br><span class="line"><span class="section">rebuild: clean</span></span><br><span class="line">    <span class="comment"># Do something else</span></span><br></pre></td></tr></table></figure><h2 id="Automatic-variables"><a href="#Automatic-variables" class="headerlink" title="Automatic variables"></a>Automatic variables</h2><p>有的时候在写 commands 的时候需要用到 <code>target</code> 或者 <code>dependencies</code> 中的名字，但是不想重复一遍，因为存在耦合，下次想改名的时候就要一个一个改，不利于维护。于是 Makefile 预定义了一组变量符号：</p><ul><li><code>$@</code>：当前规则中 <code>target</code> 的名称；</li><li><code>$^</code>：当前规则中所有 <code>dependencies</code> 的名称；</li><li><code>$&lt;</code>：当前规则中第一个 <code>dependencies</code> 的名称；</li><li><code>$?</code>：当前规则中时间戳比 <code>target</code> 更新的所有 <code>dependencies</code> 的名称组成的变量；</li></ul><h2 id="Implicit-Rules"><a href="#Implicit-Rules" class="headerlink" title="Implicit Rules"></a>Implicit Rules</h2><p>如果你使用 Makefile 不是用来管理编译项目的话，本节就不用看啦。</p><p>由于 make 一开始是为管理编译 C 语言而设计的，所以它对 C 语言有些 “偏爱”，包含了很多 “隐晦规则”，这让很多人在阅读 Makefile 的时候可能感到困惑。</p><p>这就像一些约定俗成的 magic，下面向你展示一个：</p><p>对于所有以 <code>.o</code> 扩展名结尾的 target：</p><ul><li>默认依赖于同名的 <code>.c</code> 文件（如果找不到，则 fallback 到 <code>.cc / .cpp</code>）；</li><li>如果依赖于 <code>.c</code>（C 程序），则默认使用 command：<code>$(CC) -c $(CPPFLAGS) $(CFLAGS) $^ -o $@</code>；</li><li>如果依赖于 <code>.cc/.cpp</code>（C++ 程序），则默认使用 command：<code>$(CXX) -c $(CPPFLAGS) $(CXXFLAGS) $^ -o $@</code>；</li></ul><p>然后你就会看见很简洁的 Makefile：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CC = gcc <span class="comment"># Flag for implicit rules</span></span><br><span class="line">CFLAGS = -g <span class="comment"># Flag for implicit rules. Turn on debug info</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用了隐晦规则 #1: blah 会使用默认的 linker 和 LDFLAGS 被链接</span></span><br><span class="line"><span class="comment"># 使用了隐晦规则 #2: 会自动生成一个名为 blah.o 的规则，使用默认 command，并依赖于 blah.c</span></span><br><span class="line"><span class="comment"># 使用了隐晦规则 #3: 编译时会使用 CFLAGS/CPPFLAGS 作为编译时 options</span></span><br><span class="line"><span class="section">blah: blah.o</span></span><br><span class="line"></span><br><span class="line"><span class="section">blah.c:</span></span><br><span class="line">    echo <span class="string">&quot;int main() &#123; return 0; &#125;&quot;</span> &gt; blah.c</span><br><span class="line"></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">    rm -f blah*</span><br></pre></td></tr></table></figure><h2 id="Static-Pattern-Rules"><a href="#Static-Pattern-Rules" class="headerlink" title="Static Pattern Rules"></a>Static Pattern Rules</h2><p>有的时候，我想利用隐晦规则少写一点东西，但是我又不是在编译 C/C++ 程序（例如做汇编 / 管理其他语言的项目），怎么办？</p><p>你可以用这个语法：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">targets...: target-pattern: prereq-patterns ...</span></span><br><span class="line">   commands</span><br></pre></td></tr></table></figure><p>这相当于自己定义了一套规则，让所有匹配 <code>prereq-patterns</code> 的 dependencies 在执行 commands 后输出为 target-pattern。</p><blockquote><p>官方的说法是：</p><p>The essence is that the given <code>target</code> is matched by the <code>target-pattern</code> (via a <code>%</code> wildcard). Whatever was matched is called the <em>stem</em>. The stem is then substituted into the <code>prereq-pattern</code>, to generate the target’s prereqs.</p><p>其本质是，给定的 <code>target</code> 与 <code>target-pattern</code>（通过 <code>%</code> 通配符）相匹配。匹配到的内容称为 <em>stem</em>。然后，将 stem 替换为<code>prereq-pattern</code>，生成 <code>target</code> 的 dependencies。</p></blockquote><p>例如，如果我不想编译 C/C++ 程序，只是想汇编一批文件，那么可以这么做：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">AS=$&#123;CC&#125; -c</span><br><span class="line">deps = main.o print.o print_hex.o</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 target 中需要的所有 *.o 的名字对应到 *.s 的依赖上</span></span><br><span class="line"><span class="section">$&#123;deps&#125;: %.o: %.s</span></span><br><span class="line">    $&#123;AS&#125; -o <span class="variable">$@</span> <span class="variable">$^</span></span><br></pre></td></tr></table></figure><h2 id="More…"><a href="#More…" class="headerlink" title="More… ?"></a>More… ?</h2><p>好了，上面的用法已经能涵盖 90% 的 Makefile 的用途了</p><p>如果你还希望更详细、更“刁钻” 的用法，就应该去查官方文档啦。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;说来惭愧，之前笔者还认为 Makefile 这种工具已经过时，只需要学 CMake 就行。&lt;/p&gt;
&lt;p&gt;但最近在写 boot loader 时遇到了一些问题：我既不是在编译可执行文件，也不是在编译库，这样 CMake 就显得比较无力了，因为总是用 &lt;code&gt;add_cu</summary>
      
    
    
    
    <category term="review" scheme="https://blog.sjtuxhw.top/categories/review/"/>
    
    
    <category term="GNU" scheme="https://blog.sjtuxhw.top/tags/GNU/"/>
    
    <category term="Programming" scheme="https://blog.sjtuxhw.top/tags/Programming/"/>
    
    <category term="make" scheme="https://blog.sjtuxhw.top/tags/make/"/>
    
  </entry>
  
</feed>
