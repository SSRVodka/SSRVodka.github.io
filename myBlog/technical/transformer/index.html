<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer 论文精读 + 代码实现 | SSRVodka's blog</title><meta name="author" content="SSRVodka,xhwpro@gmail.com"><meta name="copyright" content="SSRVodka"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="笔记温习一下经典的 Transformer 架构的论文，结合代码实现和解读。">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer 论文精读 + 代码实现">
<meta property="og:url" content="https://blog.sjtuxhw.top/technical/transformer/index.html">
<meta property="og:site_name" content="SSRVodka&#39;s blog">
<meta property="og:description" content="笔记温习一下经典的 Transformer 架构的论文，结合代码实现和解读。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.sjtuxhw.top/cover_imgs/transformer.jpg">
<meta property="article:published_time" content="2025-07-20T15:15:10.000Z">
<meta property="article:modified_time" content="2025-07-30T13:14:09.876Z">
<meta property="article:author" content="SSRVodka">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="Paper">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.sjtuxhw.top/cover_imgs/transformer.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://blog.sjtuxhw.top/technical/transformer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="/css/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功 ヾ(≧∇≦*)ゝ',
    error: '复制失败 (#`皿´)',
    noSupport: '浏览器不支持 ╮(╯_╰)╭'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"已切换为繁体中文","cht_to_chs":"已切换为简体中文","day_to_night":"已切换为深色模式","night_to_day":"已切换为浅色模式","bgLight":"#333","bgDark":"#1f1f1f","position":"top-center"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer 论文精读 + 代码实现',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/css/mouseConfig.css"/><link rel="stylesheet" href="/css/rightmenu.css"/><link rel="stylesheet" href="/css/custom_music.css"/><link rel="stylesheet" href="/css/addFonts.css"/><link rel="stylesheet" href="/css/titleFonts.css"/><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="SSRVodka's blog" type="application/atom+xml">
</head><body><div id="loading-box"><div id="main-loading-bg"><div class="truckWrapper"><div class="truckBody"><svg class="trucksvg" viewBox="0 0 198 93" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M135 22.5H177.264C178.295 22.5 179.22 23.133 179.594 24.0939L192.33 56.8443C192.442 57.1332 192.5 57.4404 192.5 57.7504V89C192.5 90.3807 191.381 91.5 190 91.5H135C133.619 91.5 132.5 90.3807 132.5 89V25C132.5 23.6193 133.619 22.5 135 22.5Z" fill="currentColor" stroke="#282828" stroke-width="3"></path><path d="M146 33.5H181.741C182.779 33.5 183.709 34.1415 184.078 35.112L190.538 52.112C191.16 53.748 189.951 55.5 188.201 55.5H146C144.619 55.5 143.5 54.3807 143.5 53V36C143.5 34.6193 144.619 33.5 146 33.5Z" fill="#7D7C7C" stroke="#282828" stroke-width="3"></path><path d="M150 65C150 65.39 149.763 65.8656 149.127 66.2893C148.499 66.7083 147.573 67 146.5 67C145.427 67 144.501 66.7083 143.873 66.2893C143.237 65.8656 143 65.39 143 65C143 64.61 143.237 64.1344 143.873 63.7107C144.501 63.2917 145.427 63 146.5 63C147.573 63 148.499 63.2917 149.127 63.7107C149.763 64.1344 150 64.61 150 65Z" fill="#282828" stroke="#282828" stroke-width="2"></path><rect x="187" y="63" width="5" height="7" rx="1" fill="#FFFCAB" stroke="#282828" stroke-width="2"></rect><rect x="193" y="81" width="4" height="11" rx="1" fill="#282828" stroke="#282828" stroke-width="2"></rect><rect x="6.5" y="1.5" width="121" height="90" rx="2.5" fill="#DFDFDF" stroke="#282828" stroke-width="3"></rect><rect x="1" y="84" width="6" height="4" rx="2" fill="#DFDFDF" stroke="#282828" stroke-width="2"></rect></svg></div><div class="truckTires"><svg class="tiresvg" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="15" cy="15" r="13.5" fill="#282828" stroke="#282828" stroke-width="3"></circle><circle cx="15" cy="15" r="7" fill="#DFDFDF"></circle></svg><svg class="tiresvg" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="15" cy="15" r="13.5" fill="#282828" stroke="#282828" stroke-width="3"></circle><circle cx="15" cy="15" r="7" fill="#DFDFDF"></circle></svg></div><div class="road"></div><svg class="lampPost" fill="currentColor" version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 453.459 453.459" xml:space="preserve"><path d="M252.882,0c-37.781,0-68.686,29.953-70.245,67.358h-6.917v8.954c-26.109,2.163-45.463,10.011-45.463,19.366h9.993 c-1.65,5.146-2.507,10.54-2.507,16.017c0,28.956,23.558,52.514,52.514,52.514c28.956,0,52.514-23.558,52.514-52.514 c0-5.478-0.856-10.872-2.506-16.017h9.992c0-9.354-19.352-17.204-45.463-19.366v-8.954h-6.149C200.189,38.779,223.924,16,252.882,16 c29.952,0,54.32,24.368,54.32,54.32c0,28.774-11.078,37.009-25.105,47.437c-17.444,12.968-37.216,27.667-37.216,78.884v113.914 h-0.797c-5.068,0-9.174,4.108-9.174,9.177c0,2.844,1.293,5.383,3.321,7.066c-3.432,27.933-26.851,95.744-8.226,115.459v11.202h45.75 v-11.202c18.625-19.715-4.794-87.527-8.227-115.459c2.029-1.683,3.322-4.223,3.322-7.066c0-5.068-4.107-9.177-9.176-9.177h-0.795 V196.641c0-43.174,14.942-54.283,30.762-66.043c14.793-10.997,31.559-23.461,31.559-60.277C323.202,31.545,291.656,0,252.882,0z M232.77,111.694c0,23.442-19.071,42.514-42.514,42.514c-23.442,0-42.514-19.072-42.514-42.514c0-5.531,1.078-10.957,3.141-16.017 h78.747C231.693,100.736,232.77,106.162,232.77,111.694z"></path></svg></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
      setTimeout(() => {
        $loadingBox.style.display = 'none'
      }, 800)
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()
</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.ico" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">78</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://notes.sjtuxhw.top"><i class="fa-fw fa-solid fa-pen-to-square"></i><span> Notes</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-gamepad"></i><span> ACG-Lab</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ACGLab/MikuTap/"><i class="fa-fw fas fa-music"></i><span> MikuTap</span></a></li><li><a class="site-page child" href="/ACGLab/Live2D/"><i class="fa-fw fa-solid fa-face-kiss-wink-heart"></i><span> Live2D</span></a></li><li><a class="site-page child" href="/ACGLab/Folio-2019/"><i class="fa-fw fa-solid fa-car-side"></i><span> Folio-2019</span></a></li><li><a class="site-page child" href="/ACGLab/Cube/"><i class="fa-fw fa-solid fa-cube"></i><span> Cube</span></a></li><li><a class="site-page child" href="/ACGLab/TowerBlocks/"><i class="fa-fw fa-solid fa-gopuram"></i><span> TBlocks</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/friend-links/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-camera"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.travellings.cn/go.html"><i class="fa-fw fa-solid fa-train-subway"></i><span> Travelling</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.sjtuxhw.top/cover_imgs/transformer.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head_icon.png" alt="Logo"><span class="site-name">SSRVodka's blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformer 论文精读 + 代码实现</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://notes.sjtuxhw.top"><i class="fa-fw fa-solid fa-pen-to-square"></i><span> Notes</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-gamepad"></i><span> ACG-Lab</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ACGLab/MikuTap/"><i class="fa-fw fas fa-music"></i><span> MikuTap</span></a></li><li><a class="site-page child" href="/ACGLab/Live2D/"><i class="fa-fw fa-solid fa-face-kiss-wink-heart"></i><span> Live2D</span></a></li><li><a class="site-page child" href="/ACGLab/Folio-2019/"><i class="fa-fw fa-solid fa-car-side"></i><span> Folio-2019</span></a></li><li><a class="site-page child" href="/ACGLab/Cube/"><i class="fa-fw fa-solid fa-cube"></i><span> Cube</span></a></li><li><a class="site-page child" href="/ACGLab/TowerBlocks/"><i class="fa-fw fa-solid fa-gopuram"></i><span> TBlocks</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/friend-links/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-camera"></i><span> Gallery</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.travellings.cn/go.html"><i class="fa-fw fa-solid fa-train-subway"></i><span> Travelling</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformer 论文精读 + 代码实现</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-20T15:15:10.000Z" title="发表于 2025-07-20 23:15:10">2025-07-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-30T13:14:09.876Z" title="更新于 2025-07-30 21:14:09">2025-07-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/technical/">technical</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">11.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>45分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/technical/transformer/#post-comment"><span class="waline-comment-count" data-path="/technical/transformer/"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>笔记温习一下经典的 Transformer 架构的论文，结合<a href="#代码实现及详解">代码实现和解读</a>。</p>
<span id="more"></span>
<h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><ul>
<li><p>循环神经网络、卷积神经网络的演化过程、结构、代表性的模型；</p>
</li>
<li><p>传统的注意力机制（attention）已经在很多场合下成为序列/转录模型的不可分割的一部分，因为无论两个词语语义的依赖在输入/输出序列中距离多远，都能建模依赖关系。但是这种传统的注意力机制仍然没有用在 recurrent 网络中。</p>
</li>
<li><p>自注意力机制（self-attention）是通过关联单个序列中的的不同位置，来计算这个序列的 hidden representation。自注意力机制在此前被成功应用与阅读理解、抽象总结等任务中；</p>
</li>
<li><p>另外有工作表明，基于循环注意力机制（recurrent attention）的端到端记忆网络（end-to-end memory networks），它并没有采用传统 RNN 的序列对齐循环（sequence-aligned recurrence）的计算方法，仍然能在简单语言问答、语言建模等任务上取得比较好的效果；</p>
<blockquote>
<p>循环注意力机制：一种将注意力机制与循环神经网络（RNN）相结合的技术，常见的有 Recurrent Attention Model（RAM）和 Recurrent Attention Convolutional Neural Network（RA - CNN）等模型；</p>
<p>序列对齐循环（sequence-aligned recurrence）：是一种与循环神经网络（RNN）相关的计算方式。通常沿输入和输出序列的符号位置进行因子计算（Recurrent models typically factor computation along the symbol positions of the input and output sequences），将位置与计算时间中的步骤对齐，根据前一个隐藏状态  $h_{t-1}$ 和位置 $t$  的输入生成新的隐藏状态 $h_{t}$。这种计算方式具有内在的序列性，导致训练示例中的并行化难以实现，在处理长序列时，由于内存限制会影响跨示例的批处理效率。</p>
</blockquote>
</li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul>
<li><p>RNN 和 GRU/LSTM 之类的模型已经在语言序列建模（尤其是序列到序列，或者称为 “转录模型”，transduction models）、机器翻译等领域达到了 SOTA 级别的效果；</p>
</li>
<li><p>Recurrent 类型的模型由于采用的是 sequence-aligned recurrence 的计算方法，极大阻碍了计算的并行化，尤其是在序列很长的情况下；</p>
<ul>
<li>虽然目前的工作进行了 factorization tricks 以及条件计算（后者还增强了模型的 performance）来优化性能，但是 Recurrent 网络串行计算的根源问题仍然无法解决；</li>
</ul>
</li>
<li><p>CNN 架构的模型如 ByteNet/ConvS2S 等使用 CNN 作为 basic building block，可以并行计算所有输入输出位置的 hidden representations 数据，但是输入输出间任意位置需要进行的计算量会随着位置距离增长而增长（ByteNet 是线性的，ConvS2S 是对数的）。</p>
<p>但这也会导致模型难以学习到较远距离的两个位置之间的依赖关系。</p>
</li>
</ul>
<p>基于上述背景，这个工作提出了 Transformer 模型架构，<strong>直接避开了 sequence-aligned recurrence 的做法</strong>，仅依靠注意力机制来构建一个输入/输出间的全局依赖。</p>
<p>目前 Transformer 也是第一个仅依靠自注意力机制（而不是使用 sequence-aligned recurrence 或者卷积的方法）来计算输入输出序列 representations 的转录模型。</p>
<p>这个架构的重要好处之一是可以尽可能地利用并行化的计算资源。另外 Transformer 还解决了 CNN 模型在解决序列长距离依赖时的高额时间开销问题：常数时间！</p>
<blockquote>
<p>但同时由于引入了平均注意力加权的位置参数（averaging attention-weighted positions），代价是 reduced effective resolution（丢失有效分辨率）。本文通过引入<strong><u>多头注意力机制</u></strong>来缓解这一点。</p>
</blockquote>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="imgs/transformer.png" width="320px" /></p>
<p>研究表明大部分的有竞争力的序列转录模型都有一个 encoder-decoder 结构。Transformer 也不例外：</p>
<ul>
<li>encoder（上图左框）将符号表示的输入序列 $(x_1,x_2,\ldots,x_n)$ 映射到序列连续表示（a sequence of continuous representations）$z=(z_1,\ldots,z_n)$；</li>
<li>给定序列的连续表示 $z$，decoder（上图右框）就能每次生成一个输出序列 $(y_1,\ldots,y_m)$ 的一个元素，并且每一步模型都是<strong><u>自回归的</u></strong>（self-regressive，指前面步骤中生成的符号会作为后面序列生成的额外的输入）。</li>
</ul>
<p>Transformer 总体就是遵循上述的架构设计，使用堆叠的 self-attention 块、由全连接层组成的 encoder 和 decoder，搭建出上图的结构。</p>
<h3 id="Encoder-和-Decoder-设计"><a href="#Encoder-和-Decoder-设计" class="headerlink" title="Encoder 和 Decoder 设计"></a>Encoder 和 Decoder 设计</h3><p>encoder 由 $N=6$ 的完全相同的层组成（参见上图示意），每个 layer 有两个 sub-layers：多头注意力机制，以及逐位的前馈全连接网络（position-wise fully connected feed-forward network）。</p>
<p>每个 sub-layers 周围引入残差连接块、正则化层，即每个 sub-layers 输出为 $\text{LayerNorm}(x+\text{Sublayer(x)})$，其中 $\text{Sublayer}$ 是 sub-layers 中实现的函数。</p>
<blockquote>
<p>为了容易实现残差连接块，模型的所有 sub-layers，包括 embedding layers 的输出维度都是 $d=512$；</p>
</blockquote>
<p>decoder 同样由 $N=6$ 的完全相同的层堆叠而成。不过其中的 sub-layers 有 3 个，除了 encoder 中有的两个以外，又加了一个 masked 多头注意力层（以及同样的残差连接-正则化层），用于处理输入的之前的输出序列（自回归嘛）。</p>
<blockquote>
<p>为什么处理输入的 output embedding 的多头注意力层有 mask 呢？</p>
<p>主要考虑到<strong>防止模型在训练时“作弊”（Peeking Ahead），即防止模型利用当前要预测位置之后的信息（未来信息）来预测当前的位置</strong>。</p>
<p>这个问题就像把 validation set 直接作为 training set 一样，这会严重影响训练效果。</p>
<p>目的就是让模型在预测 $y_t$ 时不会“看到” $y_{t+1}$ 以及以后的信息。</p>
</blockquote>
<h3 id="Attention-设计以及创新点"><a href="#Attention-设计以及创新点" class="headerlink" title="Attention 设计以及创新点"></a>Attention 设计以及创新点</h3><p>一个注意力函数实际上能被描述为 <code>&#123;a query, a set&#123;k: v&#125;&#125; -&gt; an output</code> 的映射。</p>
<p>其中查询（query）、键（key）、值（value）、输出（output，即注意力分数）都是向量。</p>
<p>而输出实质上就是值（values）的加权和，其中这些“权重” 是由一个 “适配性函数”（compatibility function）计算出的 <strong><u>这个查询 query 与对应键 key 的匹配的程度</u></strong>。</p>
<p>文章中介绍的这个算法就是 QKV 算法，注意力机制的<strong><u>一种高效的实现形式</u></strong>。</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>读者这里可能会好奇，为什么将 query 和 key 的匹配程度作为权重加权到 value 上就能得到注意力分数，且这个做法是有效的？也就是说：为什么 QKV 算法、以及注意力机制是有效的？</p>
<p>其实关键在于它模拟了人类认知中一个核心过程：<strong>选择性聚焦</strong>。它允许模型在处理信息时，<strong>动态地、有选择性地</strong>将有限的“认知资源”集中在输入信息中最相关、最重要的部分上，而忽略或弱化不相关的部分。</p>
<p>因此说，注意力机制的核心思想就是“动态、内容相关的信息选择”，就是让模型具备这种<strong>动态聚焦</strong>的能力。</p>
<p>它让模型在处理某个特定元素（<code>Query</code>）时，能够“有意识地”去“看”其他元素（<code>Key</code>），并根据它们与当前元素的相关性（<code>Query-Key</code> 匹配度）来决定从这些元素中提取多少信息（加权 <code>Value</code>）。</p>
<p>它也因此突破了固定编码的局限。做个比较：</p>
<ul>
<li><p>传统的神经网络层（如全连接层、CNN、RNN）在编码一个元素（如一个词、一个像素）时，主要依赖于其<strong>固定的上下文窗口</strong>或<strong>预定义的位置关系</strong>（如 CNN 的卷积核、RNN 的时序依赖）。</p>
<p>这种固定方式在处理长距离依赖、理解复杂关系或需要<strong>全局上下文</strong>信息时效率低下或效果不佳。</p>
</li>
<li><p>相比之下，注意力机制允许模型在处理序列中任何一个位置时，都能<strong>直接访问并评估序列中所有其他位置的信息</strong>，并根据<strong>内容的相关性</strong>（而非固定的位置或距离）来决定依赖程度。</p>
</li>
</ul>
<p>上述思考的有效性也被实验结果所证明。</p>
</blockquote>
<p>根据我们上面的注意力机制的定义，我们只需要设计一个 compatibility function 不就能完成注意力的计算了吗！我们记 compatibility function 为 $f_c$，那么</p>
<script type="math/tex; mode=display">
\text{attention score}=f_c(q, k)\cdot v</script><p>这里 $f_c$ 算出的结果是一个关系矩阵 $R_{ij}=(r)_{ij}$ 表示 $q_i$ 与 $k_j$ 的匹配程度，最后矩阵向量点积表示求加权和，得到对应的注意力分数。</p>
<p>这里因为我们想以匹配程度作为参考，给 $v$ 做个权重，因此希望满足：</p>
<ul>
<li>计算结果的元素求和为 1（<strong>归一化与概率解释性</strong>）；</li>
<li>并且希望<strong>显著放大</strong>最高分数与其他分数之间的<strong>相对差异</strong>实现 “强聚焦” 的效果（<strong>突出显著项与抑制不相关项</strong>）；</li>
<li>还希望利于神经网络的后续梯度的计算（<strong>梯度计算的优化</strong>）；</li>
</ul>
<p>因此 softmax 完美符合上述要求（本身输出归一化、可解释性强、非线性指数放大效应、容易计算导数），我们修改为下面的公式更为准确：</p>
<script type="math/tex; mode=display">
\text{attention score}=\text{softmax}(f_c(q, k))\cdot v</script><p>我们将 $f_c(q,k)$ 称为对齐分数，它的每个元素就是对应的、未归一化的 “query 和对应 key 的匹配程度”。</p>
<h4 id="创新点-1：缩放点积注意力-Scaled-Dot-Product-Attention"><a href="#创新点-1：缩放点积注意力-Scaled-Dot-Product-Attention" class="headerlink" title="创新点 1：缩放点积注意力 (Scaled Dot-Product Attention)"></a>创新点 1：缩放点积注意力 (Scaled Dot-Product Attention)</h4><p>对于适配性函数的具体定义，文章介绍了一种 “缩放点积” 的定义，即 $f_c(q,k)=\dfrac{1}{\sqrt{d_k}}\cdot q^T\cdot k$)，其中$q$ 和 $k$ 向量均为 $d_k$ 维。</p>
<p>因为计算机中一般需要批量并行计算，因此我们一般将输入向量堆叠成矩阵，具体计算起来会比上面单个向量的计算复杂一些：输入包含同样维度 $d_k$ 的 query 和 keys 向量（分别记为 $q_i$ 和 $k_i$），以及一个维度 $d_v$ 的 values 向量 $v_i$，输出对齐分数，计算方法：将一个 $q_i$ 与所有 $k_j$ 点积，每个都除以 $\sqrt{d_k}$，得到 $q_i$ 和 $k_j$ 的对齐分数。</p>
<p>例如 $q_i$（第 $i$ 个 query 向量）和 $k_j$（第 $j$ 个 key 向量）的关于缩放点积的对齐分数：</p>
<script type="math/tex; mode=display">
e_{ij}=\dfrac{q_i^T\cdot k_j}{\sqrt{d_k}}</script><p>最终注意力分数计算过程等价于下面的矩阵式：</p>
<script type="math/tex; mode=display">
\text{Attention}(Q,K,V)=\text{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V</script><p>我们将使用 “缩放点积” 作为适配性函数的注意力机制称为 “缩放点积注意力”。</p>
<blockquote>
<p>[!NOTE]</p>
<p>文章中也提到，并不是一开始就知道要用缩放点积函数作为 compatibility function，作者实际上先考虑的是常用的两种函数：加性、点积（分别对应加性注意力、点积注意力）。</p>
<p>加性函数是使用一个含有单层隐藏层的前馈神经网络，公式（$\tanh$ 是激活函数）：</p>
<script type="math/tex; mode=display">
f_c(q,k)=v^T\tanh(W_q\cdot q+W_k\cdot k)</script><p>虽然理论上，上述加性函数和点积函数的复杂度相当，但实际计算机计算起来点积函数的时间和空间消耗都更好一些，因为后者可以利用被高度优化的矩阵乘法计算代码。</p>
<p>不过文章指出，使用点积函数时，在 $d_k$ 很大的情况下，效果不如加性函数，作者推测可能是<strong><u>点积结果过大导致 Softmax 梯度消失</u></strong>，因此在点积后添加了一个 $\dfrac{1}{\sqrt{d_k}}$ 的缩放，这才提出了缩放点积。</p>
<p>原文：We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has  extremely small gradients [4]. To counteract this effect, we scale the dot products by $\dfrac{1}{\sqrt{d_k}}$.</p>
</blockquote>
<h4 id="创新点-2：多头注意力机制"><a href="#创新点-2：多头注意力机制" class="headerlink" title="创新点 2：多头注意力机制"></a>创新点 2：多头注意力机制</h4><p>文章注意到，与其使用单个的注意力函数来处理 $d_{\text{model}}$ 个的 $q,k,v$，不如将它们投影（线性变换）到 $h$ 个不同方向（分别是 $d_k,d_k,d_v$ 维的线性空间），然后对它们并行地求注意力分数，每个方向都能得到 $d_v$ 维输出值（注意力分数）。</p>
<p>作者指出这样做的作用是，<strong><u>增强模型捕捉不同子空间信息的能力</u></strong>。</p>
<blockquote>
<p>原文：Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
</blockquote>
<p>这样的多头注意力计算更加繁琐一些，我们直接展示结论（$d_\text{model}$ 组数据同时计算的矩阵计算式）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{MultiHead}(Q,K,V)&=\text{Concat}(\text{head}_1,\ldots,\text{head}_h)W^O\\
\text{where head}_i&=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)
\end{aligned}</script><p>其中 $W_i^Q\in\mathbf{R}^{d_{\text{model}}\times d_k},\space W_i^K\in\mathbf{R}^{d_{\text{model}}\times d_k},W_i^V\in\mathbf{R}^{d_{\text{model}}\times d_v}$ 以及 $W^O\in\mathbf{R}^{hd_v\times d_\text{model}}$ 均为用于投影的超参数矩阵。</p>
<p>本文的工作实验时取的值 $h=8,d_k=d_v=\dfrac{1}{h}d_{\text{model}}=64$。</p>
<p>Transformer 架构也充分利用了注意力机制，例如：</p>
<ul>
<li><p><strong><u>Encoder 的每一个自注意力层</u></strong>；</p>
<ul>
<li><p>输入：注意这里 <code>Q = K = V = 该层前一层的输出</code>；</p>
<blockquote>
<p>queries、keys 和 value 全部是一样的，这也是为什么称它为 “自注意力”；</p>
</blockquote>
</li>
<li><p>目的：让输入序列中的<strong>每个词（位置）</strong> 能够关注到输入序列（自身）中<strong>所有其他词（位置）</strong>，捕捉词与词之间的依赖关系（无论距离远近）；</p>
</li>
<li><p>特点：</p>
<ul>
<li>全局上下文：每个词的表示都融合了整个输入序列的信息；</li>
<li>并行计算：因为 <code>Q</code>, <code>K</code>, <code>V</code> 都来自同一序列的上一层输出，且计算不依赖顺序，整个层的计算可以高度并行化；</li>
<li>多头注意力；</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><u>Decoder 的掩码自注意力层</u></strong>；</p>
<ul>
<li>输入：仍然是 <code>Q = K = V = 该层前一层的输出</code>；</li>
<li>目的：让输出序列中<strong>正在预测的位置 <code>i</code></strong> 能够关注到<strong>已经生成的输出序列中在它之前的所有位置（<code>1</code> 到 <code>i-1</code>）</strong>，但<strong>不能</strong>看到它自身 (<code>i</code>) 或它之后 (<code>i+1</code> 到 <code>m</code>) 的位置。简言之，<strong>保持自回归（Autoregressive）特性，避免信息泄露（作弊）</strong>；</li>
<li>特点：<ul>
<li>掩码：在计算 <code>Q</code>（位置 <code>i</code>）与所有 <code>K</code>（位置 <code>j</code>）的点积后、进行 softmax 之前，会将 <code>j &gt; i</code> 的位置对应的点积结果设置为一个非常大的负数（如 <code>-10^9</code> 或 <code>-inf</code>）。这样，经过 softmax 后，这些未来位置的权重就几乎为 0；</li>
<li>其他同 encoder 的自注意力层；</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><u>Encoder-Decoder 注意力层</u>（encoder-decoder attention layers）</strong>：</p>
<ul>
<li><p>输入：注意这个时候与前面的不太一样：</p>
<ul>
<li><strong>查询（Q）</strong>：来自<strong>解码器前一层的输出</strong>（即 Masked Self-Attention 子层的输出，代表了当前预测位置 <code>i</code> 及之前的信息）。</li>
<li><strong>键（K）</strong> 和 <strong>值（V）</strong>：来自<strong>编码器的最终输出</strong>（即最后一层编码器的输出，代表了整个输入序列的编码信息）；</li>
</ul>
<blockquote>
<p>Query（来自 Decoder）会有意识地查询、注意来自不同序列的 Key 和 Value（来自 Encoder），</p>
<p>因此和自注意力相对，也称 <strong><u>Cross Attention</u></strong>；</p>
</blockquote>
</li>
<li>目的：让输出序列中<strong>正在预测的位置 <code>i</code></strong> 能够关注到<strong>整个输入序列的所有位置（<code>1</code> 到 <code>n</code>）</strong>。这是经典的“源-目标”注意力机制，让解码器在生成目标词时，能够动态地聚焦于输入序列中最相关的部分（<u>类似于传统 Seq2Seq + Attention 模型中的注意力</u>）；</li>
<li>特点：<ul>
<li>源-目标对齐： 核心作用是根据当前目标状态，在源端信息中找到最相关的上下文。这是翻译、摘要等任务的关键；</li>
<li>信息桥梁： 这是连接编码器和解码器信息的主要通道；</li>
<li>多头注意力；</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="创新点-3：逐位的前馈神经网络"><a href="#创新点-3：逐位的前馈神经网络" class="headerlink" title="创新点 3：逐位的前馈神经网络"></a>创新点 3：逐位的前馈神经网络</h4><p>之前介绍架构时指出每两个 sub-layers 中一个是逐位的全连接前馈神经网络，采用一个 ReLU 激活函数和两层线性全连接层：</p>
<script type="math/tex; mode=display">
\text{SubLayer}_\text{FFN}(x)=\max(0,\space xW_1+b_1)W_2+b_2</script><p>其中输入输出的维度都为 $d_\text{model}=512$，两个线性层中间的维度是 $d_{ff}=2048$；</p>
<p>此外，文章指出这部分还可以用 kernel size 为 1 的两个卷积层来代替。</p>
<h4 id="创新点-4-Embeddings-and-Softmax-的使用"><a href="#创新点-4-Embeddings-and-Softmax-的使用" class="headerlink" title="创新点 4:   Embeddings and Softmax 的使用"></a>创新点 4:   Embeddings and Softmax 的使用</h4><p>和一般的序列转录模型一样，Transformer 使用 embedding 的方法将输入/输出的 tokens 转为 $d_\text{model}$ 维度的向量，并且使用 linear transformation 层和 softmax 层将 decoder 输出转换为预测的 next-token 概率。</p>
<p>在本文构建的模型中，作者将两个 embedding layers（input/output embedding layers，参见上图架构中的粉红色块）和 pre-softmax linear transformation（参见上图架构中 decoder 输出的第一个 Linear 块）共用了相同的参数权重矩阵，不过在 embedding layers 中，这些权重还会被乘以 $\sqrt{d_\text{model}}$；</p>
<h4 id="创新点-5：位置编码-Positional-Encoding"><a href="#创新点-5：位置编码-Positional-Encoding" class="headerlink" title="创新点 5：位置编码 Positional Encoding"></a>创新点 5：位置编码 Positional Encoding</h4><p>因为 Transformer 架构模型不含有 sequence-aligned recurrence 计算方法，也不含有卷积操作，所以，为了让模型利用并感知到序列的具体顺序信息，作者还在 input/output embedding 传给 encoder/decoder 前注入了 tokens 在序列中相对或绝对的位置信息，这被称为 “<strong>位置编码</strong>”。</p>
<p>位置编码和 embedding vectors 的维度都是 $d_\text{model}$ 维，因此可以直接相加起来。</p>
<p>一般位置编码可以通过学习获得，也可以事先给定，本文中选取了不同频率的正弦/余弦函数作为位置编码信息（这也是为什么上图架构图把 position encoding 部分画成了示波器的形状）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
PE_{(pos,2i)}&=\sin(\dfrac{pos}{10000^{2i/d_\text{model}}})\\
PE_{(pos,2i+1)}&=\cos(\dfrac{pos}{10000^{2i/d_\text{model}}})
\end{aligned}</script><p>其中 $pos$ 表示 token 在 sequence 中的位置，$i$ 表示的是一个 embedding vector 的维度索引（共 $d_\text{model}$ 维）。因此每个位置 $pos$ 对应一个 $d_\text{model}$ 维向量，<strong>偶数列</strong>用正弦函数，<strong>奇数列</strong>用余弦函数。</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>为什么需要 “让模型感知到序列的具体顺序信息”（设计动机）？为什么这么设计（这么设计的原因）？</p>
<p>设计动机简单来说就一个：<strong><u>弥补 self-attention 的位置不变性的问题</u></strong>。</p>
<p>我们数学上注意到，自注意力机制（Self-Attention）本身是具有<strong><u>置换不变性</u></strong>的（Permutation Invariant）。即：若打乱输入序列顺序，输出不变（仅依赖词之间的相似度）。</p>
<p>但是同时我们又需要模型必须感知序列顺序，例如掌握语义的差别：“猫追狗”不等于“狗追猫”。</p>
<p>那么这样设计的原因也是和数学特性有关：<strong><u>对任意固定偏移量 $k$，$PE_{pos+k}$ 可表示为 $PE_{pos}$ 的线性变换</u></strong>。证明：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
\sin(\omega_i(pos+k))\\\cos(\omega_i(pos+k))
\end{bmatrix}
=
\begin{bmatrix}
\cos(\omega_ik)&\sin(\omega_ik)\\-\sin(\omega_ik)&\cos(\omega_ik)
\end{bmatrix}
\begin{bmatrix}
\sin(\omega_i(pos))\\\cos(\omega_i(pos))
\end{bmatrix}</script><p>其中 $w_i=\dfrac{1}{10000^{2i/d_\text{model}}}$，这意味着 $i$ 越大，位置编码的“频率” 越高，越倾向于捕获全局位置信息（长距离依赖），反之倾向于捕获局部位置信息（相邻词关系）。</p>
<p>这样的特性可以让模型轻松地学习相对位置。</p>
<p>除了数学特性，这么还有其他的两个方面的考虑：</p>
<ul>
<li><p>外推性（Extrapolation）：正弦/余弦函数的<strong>周期性</strong>允许模型泛化到比训练时更长的序列（如测试时遇到更长的句子），相比较下，可学习的位置嵌入（Learned Positional Embedding）则难以泛化到未见过的位置；</p>
<blockquote>
<p>原文：We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p>
</blockquote>
</li>
<li><p><strong>值域有界</strong>：<code>[−1,1]</code>，与词嵌入（通常归一化）兼容。</p>
</li>
</ul>
</blockquote>
<h2 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h2><p>文章基于包含了 45 万词语对的 WMT 2014 English-German 数据集训练，句子被 encoded 为  byte-pair encoding（参考了 CoRR 的工作）。</p>
<p>使用的梯度下降的优化器是自适应学习率的 Adam optimizer（$\beta_1=0.9,\beta_2=0.98,\varepsilon=10^{-9}$），并且自定义控制学习率的策略：先预留 $warmup_steps=4000$ 这些 steps 来让学习率线性增长，后面在以平方根反比的速度减小学习率：</p>
<script type="math/tex; mode=display">
l=d^{-0.5}_\text{model}\cdot\min(\text{step\_num}^{-0.5},\text{step\_num}\cdot\text{warmup\_steps}^{-1.5})</script><p>文章训练过程中使用了 3 种正则化方法：</p>
<ul>
<li><p>Normal Dropout：和一般的神经网络一样，我们会在比较长的网络中添加一些 dropout 进行正则化，起到防止过拟合等作用，没什么新鲜的不作介绍。</p>
</li>
<li><p>Residual Dropout：对每个 sub-layer 的输出采用了残差连接（在输入下一层 sub-layer 以及归一化前）。另外，在向 embeddings 加 positional encodings 时也用了 dropout；实验用的 base model 的  dropout rate 取 0.1；</p>
</li>
<li>Label Smoothing：训练过程中，使用 smoothing rate 取 $\varepsilon=0.1$；</li>
</ul>
<blockquote>
<p>知识补充：什么是标签平滑（label smoothing）？</p>
<p>在传统的分类任务（如机器翻译的词预测）中，标签通常采用 <strong>one-hot 编码</strong>（正确词的概率=1，其他词=0）。但问题是这会使模型过度自信（overconfident），强制将正确词概率推至 1，其他词压至 0。容易导致过拟合，降低泛化能力。</p>
<p>标签平滑的做法是，将正确词的目标概率设为 $1-\varepsilon$，并将剩余概率 $\varepsilon$ <strong>均匀分配</strong>给所有其他词（共 $V$ 个词）：</p>
<script type="math/tex; mode=display">
P=\left\{\begin{aligned}
1-\varepsilon&,\space\text{if correct}\\
\dfrac{\varepsilon}{V-1}&,\space\text{otherwise}
\end{aligned}\right.</script><p>那么为什么说 label smoothing 会损害 perplexity（模型困惑度）呢？回顾 perplexity 定义（交叉熵的指数），<strong><u>perplexity 值越低表示模型预测的越准确、越不太可能有很多不确定的用词选择</u></strong>：</p>
<script type="math/tex; mode=display">
\text{perplexity}=\exp(-\dfrac{1}{N}\sum\limits_{i=1}^N\log P(w_i|\text{context}))</script><p>而 label smoothing 会让<strong>目标分布更“平滑”</strong>，显然会提升模型的 perplexity；</p>
<p>但它也在另一个方便提升了模型的泛化性：</p>
<ul>
<li>模型不会对训练数据中的噪声或特定模式过度敏感；</li>
<li>并且减少了 over-confident 的可能，在测试时对模糊边界（如近义词）更鲁棒；</li>
</ul>
</blockquote>
<h2 id="效果简述"><a href="#效果简述" class="headerlink" title="效果简述"></a>效果简述</h2><p>文章先对于 “为什么选择自注意力机制” 做了一些理论上的比较：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="imgs/layer-comp.png" width="650px" /></p>
<p>综合了整体复杂度、串行计算效率，以及关键路径长度比较，self-attention 在保证 performance（原文：could yield more interpretable models）的同时确保高效：Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.</p>
<p>然后摆出了和其他 Seq2Seq 模型的优越 performance：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="imgs/perf.png" width="550px" /></p>
<h2 id="语境生词"><a href="#语境生词" class="headerlink" title="语境生词"></a>语境生词</h2><p>transduction model：转录模型，指输入序列、生成序列的模型；</p>
<p>eschew：避开，规避了；</p>
<p>counteract with：将…（不良效果）中和/抵销了。</p>
<h2 id="代码实现及详解"><a href="#代码实现及详解" class="headerlink" title="代码实现及详解"></a>代码实现及详解</h2><blockquote>
<p>本章前置知识：PyTorch 的基本使用，至少需要了解怎么用 PyTorch 搭建线性分类器/CNN 这样的模型。</p>
<p>代码中所有重要的部分均已用 <code>Note</code> 在注释中注明。</p>
</blockquote>
<h3 id="Input-Encoder-Layer"><a href="#Input-Encoder-Layer" class="headerlink" title="Input Encoder Layer"></a>Input Encoder Layer</h3><p>input encoder layer：</p>
<ul>
<li>根据论文原文，取 $d_\text{model}=hd_k=512$，因此输出的 token embedding 维数为 512；</li>
<li>另外根据原文，在 embedding layers 中还会将所有权重乘以 $\sqrt{d_\text{model}}$（参见创新点 4）；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InputEmbeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, vocab_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a Transformer input embedding layer</span></span><br><span class="line"><span class="string">        :param d_model: the dimension of an embedding vector</span></span><br><span class="line"><span class="string">        :param vocab_size: the full size of the vocabulary</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.vocab_sz = vocab_size</span><br><span class="line">        self.embedding = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size,</span><br><span class="line">            embedding_dim=d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.embedding(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<p>如上代码，PyTorch 内置的 <code>nn.Embedding</code> 足够进行词嵌入的运算。</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><ul>
<li>根据原文，注入位置信息的方法就是将同维度的 $PE$ 与 input/output encoding 相加；</li>
<li>计算公式已由原文给出；</li>
<li>注意在 这里添加了一个 dropout layer（这里是为了提升泛化性，要与 Layer Normalization 区分开），dropout rate 为 0.1；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, seq_len: <span class="built_in">int</span>, p_dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a Transformer position encoding layer</span></span><br><span class="line"><span class="string">        :param d_model: the dimension of an embedding vector</span></span><br><span class="line"><span class="string">        :param seq_len: the max length of the tokens for the input sequence</span></span><br><span class="line"><span class="string">        :param p_dropout: the dropout rate for the current layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.seq_len = seq_len</span><br><span class="line">        self.dropout = nn.Dropout(p_dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># PE_&#123;(pos,2i)&#125;=\sin(\dfrac&#123;pos&#125;&#123;10000^&#123;2i/d_\text&#123;model&#125;&#125;&#125;)</span></span><br><span class="line">        <span class="comment"># PE_&#123;(pos,2i+1)&#125;=\cos(\dfrac&#123;pos&#125;&#123;10000^&#123;2i/d_\text&#123;model&#125;&#125;&#125;)</span></span><br><span class="line">        pe = torch.zeros((seq_len, d_model))</span><br><span class="line">        <span class="comment"># construct Tensor(seq_len, 1) from (seq_len,)</span></span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, seq_len, dtype=torch.float64).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        divisor_part = torch.exp(</span><br><span class="line">            torch.arange(<span class="number">0</span>, seq_len, <span class="number">2</span>, dtype=torch.float64) / self.d_model * (-math.log(<span class="number">10000.0</span>)))</span><br><span class="line">        <span class="comment"># for all the seq_len (each token), for even/odd dimension</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos * divisor_part)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos * divisor_part)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: here the size of the position encoding vector is (seq_len, d_model),</span></span><br><span class="line">        <span class="comment"># which cannot deal with batch input embeddings (multiple sequences).</span></span><br><span class="line">        <span class="comment"># We should generate (1, seq_len, d_model) for **batch processing**</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: If you want to save a variable in a nn.Module which is not a learnable parameter,</span></span><br><span class="line">        <span class="comment"># then you need to register it as a buffer so that PyTorch will save it for you.</span></span><br><span class="line">        <span class="comment"># And next time (e.g., training/inference stage) you can load it from the model checkpoint file!</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply position encoding to input sequence</span></span><br><span class="line"><span class="string">        :param x: a tensor with shape(batch, seq_len, d_model) including all the embeddings in batched sequences</span></span><br><span class="line"><span class="string">        :return: PE(x)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># use pe buffer here</span></span><br><span class="line">        <span class="comment"># Note: truncate buffer self.pe to fit sequence length</span></span><br><span class="line">        <span class="comment"># Note: position encoding is fixed and is not learnable.</span></span><br><span class="line">        <span class="comment"># So we should tell PyTorch using &#x27;requires_grad_(False)&#x27;.</span></span><br><span class="line">        x = x + (self.pe[:, :x.shape(<span class="number">1</span>), :]).requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h3 id="正则化层（架构中的-Add-amp-Norm-的-“Norm”）"><a href="#正则化层（架构中的-Add-amp-Norm-的-“Norm”）" class="headerlink" title="正则化层（架构中的 Add &amp; Norm 的 “Norm”）"></a>正则化层（架构中的 <code>Add &amp; Norm</code> 的 “<code>Norm</code>”）</h3><p>先看正则化层，即文中的 $\text{LayerNorm}$（Layer Normalization），它就是对一个 <strong>单个样本（Token）在某一层的所有特征维度</strong>（沿 $d_\text{model}$）上进行归一化。</p>
<p>必要性：我们知道因为这些 embedding representation 每个特征维度可能相差很大（几个数量级），例如一个 token 的 embedding vector 可能是这样的：<code>[0.001, 10000.103, -9999999.113]</code>（比较极端），这不利于后续梯度计算和收敛。因此 Layer Normalization 是为了<strong>稳定深层网络训练</strong>（ 缓解梯度问题）、<strong>加速收敛</strong>（减少内部协变量偏移）的考量才设计的。</p>
<p>这个 Layer Normalization 的归一化方法是非常科学的，除了一般的归一化处理，还会进行<strong><u>仿射变换</u></strong>。</p>
<blockquote>
<p>这里仿射变换的必要性？</p>
<p>它让模型能够学习在归一化之后，<strong><u>是否以及如何恢复某些特征维度的原始重要性或偏差</u></strong>。如果没有它们，归一化可能会破坏网络已经学习到的一些表示能力。</p>
</blockquote>
<p>因此现在步骤如下：</p>
<ol>
<li><p>一般归一化：$\hat{h}=\dfrac{h-\mu}{\sqrt{\sigma^2+\varepsilon}}$，其中 $\varepsilon$ 是保证数值稳定性的极小数（老生常谈了），$\mu=\dfrac{1}{d_\text{model}}\sum\limits_{i=1}^{d_\text{model}}h_i$，$\sigma^2=\dfrac{1}{d_\text{model}}\sum\limits_{i=1}^{d_\text{model}}(h_i-\mu)^2$；</p>
</li>
<li><p>可学习的仿射变换（<strong><u>注意超参数 $\gamma$ 和 $\beta$ 都是可学习的</u></strong>，前者负责<strong>缩放</strong>归一化后的值，后者负责<strong>平移</strong>/偏移归一化后的值）：</p>
<script type="math/tex; mode=display">
y=\gamma\cdot\hat{h}+\beta</script></li>
</ol>
<blockquote>
<p>[!NOTE]</p>
<p>这里可以与一般的 Batch Normalization 做个区分。</p>
<p><strong>Batch Norm</strong>： 对一个 Batch 内所有样本的 <strong>同一特征维度</strong> 计算均值和方差进行归一化。它依赖于 Batch Size 和序列长度（需要填充对齐），对 Batch Size 敏感，且在 RNN/Transformer 这类序列模型上应用较麻烦。</p>
<p>而 <strong>Layer Norm</strong>： 对 <strong>单个样本的所有特征维度</strong> 计算均值和方差进行归一化。它与 Batch Size 无关，天然适合处理不同长度的序列输入（每个 Token 独立归一化），可以针对每个样本、每个位置独立计算统计量，非常适合处理长度可变的序列数据（如句子），避免了 Batch Norm 在序列数据上的局限性（需要填充对齐、依赖 Batch Size 统计量）。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNormalization</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, eps: <span class="built_in">float</span> = <span class="number">1e-9</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.eps = eps</span><br><span class="line">        <span class="comment"># Note: use Parameter for learnable parameters in nn.Module</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(<span class="number">1</span>))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply layer normalization to current input (LayerNorm)</span></span><br><span class="line"><span class="string">        :param x: the tensor output from every sub-layer</span></span><br><span class="line"><span class="string">        :return: LayerNorm(x)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># &#x27;mean&#x27;/&#x27;std&#x27; along the dimension in each token (indexing elements of an embedding)</span></span><br><span class="line">        <span class="comment"># Note: &#x27;mean&#x27;/&#x27;std&#x27; always cancels the dimension it applies.</span></span><br><span class="line">        <span class="comment"># Here we need to keep it for further calculation, otherwise we will need to un-squeeze</span></span><br><span class="line">        mu = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        sigma = x.std(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gamma * (x - mu) / (sigma + self.eps) + self.beta</span><br></pre></td></tr></table></figure>
<p>至于残差连接块（residual dropout），我们将在后文介绍。</p>
<h3 id="逐位的全连接前馈神经网络"><a href="#逐位的全连接前馈神经网络" class="headerlink" title="逐位的全连接前馈神经网络"></a>逐位的全连接前馈神经网络</h3><p>计算公式和网络结构论文已经给出，参见 “创新点 3”。</p>
<p>这里建议实现时添加在激活函数后添加一个 dropout layer 进行正则化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FFNBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, d_ff: <span class="built_in">int</span>, ff_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a 2-layer fully connected feed-forward network with activation function ReLU.</span></span><br><span class="line"><span class="string">        :param d_model: input and output size</span></span><br><span class="line"><span class="string">        :param d_ff: hidden layer size</span></span><br><span class="line"><span class="string">        :param ff_dropout: dropout rate for this network</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_ff = d_ff</span><br><span class="line">        self.linear_1 = nn.Linear(in_features=d_model, out_features=d_ff)</span><br><span class="line">        <span class="comment"># Note: we add dropout layer here to do normalization</span></span><br><span class="line">        self.dropout = nn.Dropout(ff_dropout)</span><br><span class="line">        self.linear_2 = nn.Linear(in_features=d_ff, out_features=d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply this FFN to the input tensor.</span></span><br><span class="line"><span class="string">        (batch, seq_len, d_model) -&gt; (batch, seq_len, d_ff) -&gt; (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">        :param x: the input tensor</span></span><br><span class="line"><span class="string">        :return: SubLayer_&#123;FFN&#125;(x)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.linear_2(</span><br><span class="line">            self.dropout(</span><br><span class="line">                torch.relu(</span><br><span class="line">                    self.linear_1(x))))</span><br></pre></td></tr></table></figure>
<h3 id="多头自注意力块、掩码多头自注意力块、Encoder-Decoder-注意力块"><a href="#多头自注意力块、掩码多头自注意力块、Encoder-Decoder-注意力块" class="headerlink" title="多头自注意力块、掩码多头自注意力块、Encoder-Decoder 注意力块"></a>多头自注意力块、掩码多头自注意力块、Encoder-Decoder 注意力块</h3><p>这里如果看论文的公式会发现比较复杂，尤其涉及分块和 batch，很容易混淆 tensor shape。这里建议在架构图上表明每一步的 shape/size 的变化情况。</p>
<p>如果是 Encoder 的多头自注意力块（或者 Decoder 的掩码多头自注意力块），$Q=K=V=\text{Input}$，而 Encoder-Decoder 注意力则是 $Q=\text{Decoder Input},\space K,V=\text{Encoder Input}$。它们只是计算传入的参数有所不同。另外关于 Mask 我们等会考虑。</p>
<p>为了方便分析，我们现在省去 batch 的维度，因此对每一个 sequence（token）的 embedding vector 而言，多头注意力层的计算过程如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="imgs/mha-shapes.png" /></p>
<p>现在我们逐步介绍。输入 shape 为 $(\text{seq-len},d_\text{model})$，注意无论是哪种 attention，在这个论文的实现中都是这个 shape；</p>
<ul>
<li><p>先计算多头 $\text{head}_i$：</p>
<ol>
<li><p>先定义可训练的模型超参数矩阵 $W^Q,W^K,W^V$，它们不能改变 $Q,K,V$ 的形状，所以大小显然都是 $d_\text{model}\times d_\text{model}$；</p>
</li>
<li><p>直接计算 $QW^Q,KW^K,VW^V$，然后将它们沿着 $d_\text{model}$（垂直于 embedding 特征维度）方向，平均拆成 $h$ 份，每份记为 $QW_i^Q,KW_i^K,VW_i^V$；</p>
<p>因为拆成 $h$ 份，因此大小都是 $\text{seq-len}\times \dfrac{d_\text{model}}{h}=\text{seq-len}\times d_k$（$d_k,d_v$ 的定义，$d_k=d_v=\dfrac{d_\text{model}}{h}$）；</p>
<blockquote>
<p>这一步相当于将 $Q,K,V$ 投影到 $h$ 个不同子空间；</p>
</blockquote>
</li>
<li><p>对拆好的每一份计算一次缩放点积注意力：$\text{head}_i=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$；</p>
</li>
</ol>
</li>
<li><p>现在 $\text{head}_i$ 包含了 $h$ 个不同子空间的 representation 的注意力信息，我们最后将它简单地拼起来，最后进行一个线性变换（$\times W^O$）。注意拼起来的 $\text{head}_i$ 的大小 $\text{seq-len}\times (h\cdot d_v)$，而要保证输入输出的 size 一致，因此线性变换 $W^O$ 张量大小需要 $(h\cdot d_v)\times d_\text{model}$。</p>
</li>
<li><p>最后的最后，和 position encoding、FFN 一样，我们需要添加一个 dropout layer 来正则化。</p>
</li>
</ul>
<p>另外，为了代码的可重用性，我们应该在类中定义一个可以计算 mask 的注意力公式，同时可以计算含有掩码的多头注意力块。如果需要 mask，那么应该在计算 $\text{head}_i$ 时（$QW_i^Q\times KW_i^K$ 完成后、softmax 计算前）针对对齐分数进行 mask。</p>
<blockquote>
<p>[!TIP]</p>
<p>这里有个比较有意思的处理方式，上面的超参数矩阵（$W^Q,W^K,W^V,W^O$）可以直接用无偏移的 <code>nn.Linear</code>（不包含激活函数的线性网络）表示，因为后者在数学上的表达式就是这样。注意 input feature 和 output feature 对应矩阵的长宽（利用 broadcast）。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, h: <span class="built_in">int</span>, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build a multi-head attention block with source mask (optional)</span></span><br><span class="line"><span class="string">        :param d_model: the dimension of an embedding vector</span></span><br><span class="line"><span class="string">        :param h: the number of the heads</span></span><br><span class="line"><span class="string">        :param p_dropout: dropout rate for this network</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.h = h</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_k = d_model // h     <span class="comment"># Use floor div</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span>, <span class="string">&quot;d_model is not divisible by h&quot;</span></span><br><span class="line"></span><br><span class="line">        self.w_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.w_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.w_v = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: h * d_v == d_model. so (h*d_v, d_model) == (d_model, d_model)</span></span><br><span class="line">        self.w_o = nn.Linear(d_model, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(p_dropout)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask, dropout</span>) -&gt; (torch.Tensor, torch.Tensor):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Helper for calculating MHA scores</span></span><br><span class="line"><span class="string">        :param query: Query tensor with batch and heads stacked (batch, h, seq_len, d_k)</span></span><br><span class="line"><span class="string">        :param key: Key tensor with the same shape of Query</span></span><br><span class="line"><span class="string">        :param value: Value tensor with shape (batch, h, seq_len, d_v), where `d_v == d_k`</span></span><br><span class="line"><span class="string">        :param mask: (optional) Source mask with shape (1, 1, seq_len, seq_len). Meaning: (i,j) =&gt; j for i</span></span><br><span class="line"><span class="string">        :param dropout: (optional) drop out network</span></span><br><span class="line"><span class="string">        :return: (split attention_scores, softmax-processed alignment scores)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        d_k = query.shape[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: we need to switch dimension seq_len and d_k to do multiplication</span></span><br><span class="line">        <span class="comment"># [IMPORTANT] the shape of aligned_scores:</span></span><br><span class="line">        <span class="comment"># (batch, h, seq_len, d_k) x (batch, h, d_k, seq_len) -&gt; (batch, h, seq_len, seq_len)</span></span><br><span class="line">        aligned_scores: torch.Tensor = (query @ key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            aligned_scores.masked_fill_(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        aligned_scores = aligned_scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            aligned_scores = dropout(aligned_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch, h, seq_len, seq_len) x (batch, h, seq_len, d_k) -&gt; (batch, h, seq_len, d_k)</span></span><br><span class="line">        <span class="keyword">return</span> aligned_scores @ value, aligned_scores</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Calculate MHA scores for input Q/K/V</span></span><br><span class="line"><span class="string">        :param q: Query tensor with shape (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">        :param k: Key tensor with the same shape of Query</span></span><br><span class="line"><span class="string">        :param v: Value tensor with the same shape of Key</span></span><br><span class="line"><span class="string">        :param mask: source mask () for the alignment scores</span></span><br><span class="line"><span class="string">        :return: Output tensor with shape (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        q_prime: torch.Tensor = self.w_q(q)</span><br><span class="line">        k_prime: torch.Tensor = self.w_k(k)</span><br><span class="line">        v_prime: torch.Tensor = self.w_v(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split q_prime, k_prime and v_prime into h pieces.</span></span><br><span class="line">        <span class="comment"># And we use different dimension to indicate partition!</span></span><br><span class="line">        <span class="comment"># Reshape (without creating new memory area):</span></span><br><span class="line">        <span class="comment"># (batch, seq_len, d_model) -&gt; (batch, seq_len, h, d_k)</span></span><br><span class="line">        <span class="comment"># [IMPORTANT] 注意：这里需要整理出 seq_len x d_k 相邻维度方便后续计算，因此应该交换 h 和 seq_len 的维度</span></span><br><span class="line">        <span class="comment"># (batch, seq_len, h, d_k) -&gt; (batch, h, seq_len, d_k)</span></span><br><span class="line">        q_split = q_prime.view((q_prime.shape[<span class="number">0</span>], q_prime.shape[<span class="number">1</span>], self.h, self.d_k)).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        k_split = k_prime.view((k_prime.shape[<span class="number">0</span>], k_prime.shape[<span class="number">1</span>], self.h, self.d_k)).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        v_split = v_prime.view((v_prime.shape[<span class="number">0</span>], v_prime.shape[<span class="number">1</span>], self.h, self.d_k)).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shape of split_mha_scores: (batch, h, seq_len, d_v)</span></span><br><span class="line">        <span class="comment"># shape of alignment_scores: (batch, h, seq_len, seq_len)</span></span><br><span class="line">        split_mha_scores, alignment_scores = MultiHeadAttentionBlock.attention(</span><br><span class="line">            q_split, k_split, v_split, mask, self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># concat the split MHA scores and multiply by w_o (d_k == d_v)</span></span><br><span class="line">        <span class="comment"># concat: (batch, h, seq_len, d_v) -&gt; (batch, seq_len, h, d_v) -&gt; (batch, seq_len, h*d_v)</span></span><br><span class="line">        <span class="comment"># [IMPORTANT] do contiguous() here to declare memory copy explicitly</span></span><br><span class="line">        mha_scores = (split_mha_scores</span><br><span class="line">                      .transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">                      .view((split_mha_scores.shape[<span class="number">0</span>], -<span class="number">1</span>, self.h * self.d_k)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># multiply: (batch, seq_len, h*d_v) x (h*d_v, d_model) -(broadcast)-&gt; (batch, seq_len, d_model)</span></span><br><span class="line">        <span class="keyword">return</span> self.w_o(mha_scores)</span><br></pre></td></tr></table></figure>
<p>代码有些复杂，不过我用 <strong><code>[IMPORTANT]</code></strong> 标注出了 3 处比较重要、困难的部分，我们单独分析。</p>
<p>先看 <code>q_split = q_prime.view((q_prime.shape[0], q_prime.shape[1], self.h, self.d_k)).transpose(1, 2)</code> 这部分，<code>view</code> 是创建了一个 stride 不同的新的 tensor 对象，但是与 <code>q_prime</code> 共用数据内存（引用式 reshape），这个比较好理解。</p>
<p>但是为什么需要 <code>transponse</code> 将 $h$ 维度和 $d_k$ 交换呢？</p>
<p>这主要考虑到计算 attention score 时需要让 <code>seq_len</code> 和 $d_k$ 在相邻的维度上，方便后续计算。</p>
<p>然后再看 <code>attention</code> 计算函数的 <code>aligned_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)</code>，对应的是点积缩放 compatibility function $\dfrac{QK^T}{\sqrt{d_k}}$。</p>
<p>这里想搞清楚 shape 比较困难：为什么 <code>(batch, h, seq_len, d_k) x (batch, h, d_k, seq_len)</code> 得到形状 <code>(batch, h, seq_len, seq_len)</code>？</p>
<p>这主要是高维张量相乘特性，记住即可。如果是 <code>query.transpose(-2,-1) @ key</code> 那么 shape 就是 $(\text{batch}, h, d_k, d_k)$ 了。</p>
<p>你可以用简单的情况试一试，建立直觉：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a 的形状是 (3, 1, 2, 3), b 的形状是 (1, 1, 2, 3)</span></span><br><span class="line">a = torch.tensor([[[[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>]]], [[[<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span>], [<span class="number">4</span>,<span class="number">6</span>,<span class="number">6</span>]]], [[[<span class="number">5</span>,<span class="number">7</span>,<span class="number">7</span>], [<span class="number">6</span>,<span class="number">9</span>,<span class="number">9</span>]]]])</span><br><span class="line">b = torch.tensor([[[[<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]]]])</span><br><span class="line">c = a @ b.transpose(-<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">d = a.transpose(-<span class="number">2</span>, -<span class="number">1</span>) @ b</span><br><span class="line"><span class="comment"># e = a @ b 维度不匹配</span></span><br><span class="line"><span class="built_in">print</span>(a.shape, b.shape, c.shape, d.shape)</span><br></pre></td></tr></table></figure>
<p>最后来看这段将 MHA 的多头合并起来的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mha_scores = (split_mha_scores</span><br><span class="line">              .transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">              .view((split_mha_scores.shape[<span class="number">0</span>], -<span class="number">1</span>, self.h * self.d_k)))</span><br></pre></td></tr></table></figure>
<p><code>.transpose(1, 2)</code> 是将之前为了计算方便而交换的 $h$ 和 <code>seq_len</code> 维度再换回来，准备合并。</p>
<p><code>.contiguous()</code> 是显式地进行 tensor 内存 copy，让 stride 对应的底层数据结构是连续的，方便后续 <code>view</code> reshape 和其他操作。</p>
<blockquote>
<p>[!TIP]</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><code>transpose()</code></th>
<th><code>transpose_()</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>是否原地修改</strong></td>
<td>❌ 返回新张量</td>
<td>✅ 修改原始张量</td>
</tr>
<tr>
<td><strong>内存共享</strong></td>
<td>✅ 与原始张量共享内存</td>
<td>✅ 同一张量内存地址不变</td>
</tr>
<tr>
<td><strong>连续性</strong></td>
<td>❌ 结果是非连续的</td>
<td>❌ 结果是非连续的</td>
</tr>
<tr>
<td><strong>内存复制时机</strong></td>
<td>仅在需要连续张量时触发（如 <code>contiguous()</code>）</td>
<td>同左</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<p>最后的 <code>.view()</code> 最终进行符合要求的合并操作。</p>
<h3 id="残差连接块（Add-amp-Norm-中的-“Add”）"><a href="#残差连接块（Add-amp-Norm-中的-“Add”）" class="headerlink" title="残差连接块（Add &amp; Norm 中的 “Add”）"></a>残差连接块（<code>Add &amp; Norm</code> 中的 “<code>Add</code>”）</h3><p>前面说了正则化层的定义，现在我们看残差连接层。正如论文的表达式：$\text{LayerNorm}(x+\text{SubLayer}(x))$，残差连接就是 $x+\text{SubLayer}(x)$，也就是上面架构图中将上一层的 input 拉过来的箭头。</p>
<p>为了方便起见，我们这里代码中的残差连接层的定义直接和正则化层写在了一起（调用关系），因为它们总是一同出现。</p>
<p>另外比较好玩的是，很多 Transformer Implementation 实际上是这么计算的：$x+\text{SubLayer}(\text{LayerNorm}(x))$，然后在 $\text{SubLayer}$ 计算完后再添加一个 dropout layer，最后再和 $x$ 残差连接起来。可能这样的工程效果更好？注：下面的代码也是这种和论文不一样的计算方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConnection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build residual connection layer (Add &amp; Norm)</span></span><br><span class="line"><span class="string">        :param p_dropout: the normal dropout rate for this layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p_dropout)</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sub_layer</span>):</span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sub_layer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<h3 id="Encoder-amp-Decoder-Block"><a href="#Encoder-amp-Decoder-Block" class="headerlink" title="Encoder &amp; Decoder Block"></a>Encoder &amp; Decoder Block</h3><p>先考虑 Encoder。现在我们要将之前已经定义的模块组合起来成为一个 Transformer Encoder（参见架构图），我们分别定义 <code>EncoderBlock</code>（包含 MHA、FFN、两个 Add &amp; Norm），以及 <code>Encoder</code>（$N\times$ Encoder Block，论文中 $N=6$）。</p>
<p>注意，因为前面的多头注意力块实现的时候我们为了可复用性添加了 Mask 参数，所以这里在构造 Encoder Attention 的时候还需要代一个参数方便复用（尽管当前模型用不到）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mha: MultiHeadAttentionBlock, ffn: FFNBlock, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build a single encoder block with 1x MHA, 2x Residual Connections, 1x FFN.</span></span><br><span class="line"><span class="string">        :param mha: Multi-head attention block instance</span></span><br><span class="line"><span class="string">        :param ffn: Position-wise feed-forward network instance</span></span><br><span class="line"><span class="string">        :param p_dropout: the dropout rate for each of the residual connection block</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.mha_layer = mha</span><br><span class="line">        self.ffn_layer = ffn</span><br><span class="line">        self.residual_connections = nn.ModuleList([ResidualConnection(p_dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, src_mask</span>):</span><br><span class="line">        <span class="comment"># Note: sub_layer in ResidualConnection only has one argument,</span></span><br><span class="line">        <span class="comment"># but `forward` in MultiHeadAttentionBlock has 4 parameters (self excluded)</span></span><br><span class="line">        <span class="comment"># So we need to use lambda expr to construct function with one parameter</span></span><br><span class="line">        x = self.residual_connections[<span class="number">0</span>](x, <span class="keyword">lambda</span> i: self.mha_layer(i, i, i, src_mask))</span><br><span class="line">        x = self.residual_connections[<span class="number">1</span>](x, self.ffn_layer)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers: nn.ModuleList</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Construct a Transformer Encoder</span></span><br><span class="line"><span class="string">        :param layers:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = layers</span><br><span class="line">        <span class="comment"># Question: is this necessary?</span></span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>还要注意一下，在组合网络的时候需要注意和定义一个网络有略微区别，主要是需要向上层呈递和注册参数，方便训练优化器识别。下面的 Tip 提示一个易错点。</p>
<blockquote>
<p>[!TIP]</p>
<p>PyTorch 在处理多个模型拼接的时候，不能用普通 Python 列表来管理网络部件，必须使用 <code>nn.ModuleList</code> 来表示（当然你不嫌麻烦的话可以一个一个重复手写）。它允许 index / for 迭代、使用 <code>List[nn.Module]</code> 初始化。</p>
<p>读者可能会问，为什么不直接用 Python 列表存放 <code>nn.Module</code> 而必须用 <code>nn.ModuleList</code> 呢？</p>
<p>很多新手都会犯这样的错误（包括笔者），不用的话可能有些问题：</p>
<ol>
<li><p><strong><u>参数注册</u></strong>问题：<code>nn.ModuleList</code> 会自动将列表中的所有子模块注册到父模块中。这意味着子模块的参数（<code>nn.Parameter</code>）会被父模块的 <code>parameters()</code> 方法识别，从而被优化器发现并更新；</p>
<p>如果只是用列表的话，优化器可能没法识别到，或者无法正确保存/加载模型（<code>state_dict</code> 会缺失这些参数）；</p>
</li>
<li><p>另一种情况是<strong><u>设备移动</u></strong>问题，当调用 <code>model.to(device)</code> 时，<code>nn.ModuleList</code> 会管理并将所有子模块及其参数会自动移动到目标设备（如 GPU）；</p>
<p>如果只用列表的话，子模块可能不会被移动，导致模型一部分在 CPU、一部分在 GPU，引发运行时错误；</p>
</li>
<li><p><strong><u>模式状态</u></strong>也可能有问题。<code>model.train()</code> 和 <code>model.eval()</code> 两种情况网络的读写行为是不一样的，用普通列表会导致更新状态错误。</p>
</li>
</ol>
<p>举个栗子🌰：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BadModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 错误的。你不炸了吗</span></span><br><span class="line">        self.layers = [</span><br><span class="line">            nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoodModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 正确的（forward 与上面的相同）</span></span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line">        ])</span><br></pre></td></tr></table></figure>
</blockquote>
<p>那么好，我们回顾一下残差连接层的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConnection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p_dropout)</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<p>回答问题：</p>
<ol>
<li><p>为什么 <code>LayerNormalization</code> 和 <code>Dropout</code> 的网络不需要 <code>nn.ModuleList</code> 来帮它注册参数？</p>
<blockquote>
<p>答：<code>nn.Module</code> 类会自动管理所有存放在<strong><u>实例属性</u></strong>中的网络（不包括 Python 原生容器类型，因为对象的内存存放方式导致）。</p>
<p>上面的网络作为 <code>self.dropout</code> 和 <code>self.norm</code> 存在，因此已经被管理；</p>
</blockquote>
</li>
<li><p>为什么传入的 <code>sublayer</code> 不需要 <code>nn.ModuleList</code> 来帮它注册参数？</p>
<blockquote>
<p>答：外部传入的 <code>sublayer</code> <strong>不是</strong> <code>ResidualConnection</code> 的组成部分，约定由创建它的父模块负责。</p>
</blockquote>
</li>
</ol>
<p>看完上面的 Tip 和两个问题后，你应该能明白 <code>nn.ModuleList</code> 或者 <code>nn.Sequential</code> 的作用了，主要是方便管理<u>不方便一个个写成实例属性的情况</u>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>场景</th>
<th>推荐方式</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>固定数量的子模块</strong></td>
<td>直接定义成实例属性</td>
<td><code>self.conv = nn.Conv2d()</code></td>
</tr>
<tr>
<td><strong>动态数量的模块集合 / 重复的模块</strong></td>
<td><code>nn.ModuleList</code></td>
<td>循环创建的 layer 列表</td>
</tr>
<tr>
<td><strong>需要按名字访问的模块集合</strong></td>
<td><code>nn.ModuleDict</code></td>
<td>通过键名访问的模块</td>
</tr>
<tr>
<td><strong>顺序执行的固定模块序列</strong>（串行计算）</td>
<td><code>nn.Sequential</code></td>
<td><code>self.seq = nn.Sequential(...)</code></td>
</tr>
</tbody>
</table>
</div>
<p>最后，Decoder 和 Encoder 非常相似，不再赘述，只要区分给 Encoder MHA 传入的 mask（source mask）和 Decoder MHA 的 mask（target mask）是不同的即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, masked_mha: MultiHeadAttentionBlock, mha: MultiHeadAttentionBlock, ffn: FFNBlock, p_dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build a single decoder block with 1x Masked MHA, 1x Cross MHA, 1x FFN, 3x Residual Connections.</span></span><br><span class="line"><span class="string">        :param masked_mha: Masked MHA instance for decoder</span></span><br><span class="line"><span class="string">        :param mha: Multi-head cross attention block instance for decoder (encoder-decoder attention)</span></span><br><span class="line"><span class="string">        :param ffn: Position-wise feed-forward network instance</span></span><br><span class="line"><span class="string">        :param p_dropout: the dropout rate for each of the residual connection block</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.masked_mha_layer = masked_mha</span><br><span class="line">        self.mha_layer = mha</span><br><span class="line">        self.ffn_layer = ffn</span><br><span class="line">        self.residual_connections = nn.ModuleList([ResidualConnection(p_dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input, decoder_input, src_mask, target_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply DecoderBlock to Transformer Encoder/Decoder inputs (with masks)</span></span><br><span class="line"><span class="string">        :param encoder_input: input embeddings from encoder</span></span><br><span class="line"><span class="string">        :param decoder_input: input from decoder</span></span><br><span class="line"><span class="string">        :param src_mask: mask for MHA of Transformer Encoder</span></span><br><span class="line"><span class="string">        :param target_mask: mask for MHA of Transformer Decoder</span></span><br><span class="line"><span class="string">        :return: tensor proceeded by one single DecoderBlock</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        decoder_input = self.residual_connections[<span class="number">0</span>](</span><br><span class="line">            decoder_input, <span class="keyword">lambda</span> i: self.masked_mha_layer(i, i, i, target_mask))</span><br><span class="line">        output = self.residual_connections[<span class="number">1</span>](</span><br><span class="line">            decoder_input, <span class="keyword">lambda</span> d: self.mha_layer(d, encoder_input, encoder_input, src_mask))</span><br><span class="line">        output = self.residual_connections[<span class="number">2</span>](output, self.ffn_layer)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layers: nn.ModuleList</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Construct a Transformer Decoder</span></span><br><span class="line"><span class="string">        :param layers: the layers of multiple DecoderBlocks</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = layers</span><br><span class="line">        self.norm = LayerNormalization()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_from_encoder, x_from_decoder, src_mask, target_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x_from_decoder = layer(x_from_encoder, x_from_decoder, src_mask, target_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x_from_decoder)</span><br></pre></td></tr></table></figure>
<h3 id="Projection-Layer-Linear"><a href="#Projection-Layer-Linear" class="headerlink" title="Projection Layer (Linear)"></a>Projection Layer (<code>Linear</code>)</h3><p>在 Transformer 架构的最后有一个 Linear 块，即 pre-softmax linear transformation（参见 “创新点 4”），它的权重是与 Embedding layers 共用的（不过没有 $\sqrt{d_\text{model}}$ 的缩放），它的作用是最终将生成的特征 tensors 映射回 vocabulary 中。</p>
<p>这里为了简便起见，我们直接设置自由的权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ProjectionLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Build a projection layer for Transformer to convert features back into vocabulary</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, vocab_size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># project: (batch, seq_len, d_model) -&gt; (batch, seq_len, vocab_size)</span></span><br><span class="line">        <span class="comment"># log-softmax: convert vocab_size dim to probabilities</span></span><br><span class="line">        <span class="keyword">return</span> torch.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Put-It-Together"><a href="#Put-It-Together" class="headerlink" title="Put It Together"></a>Put It Together</h3><p>最终我们定义一个 <code>Transformer</code> 类型，将上面的模块组合起来（以文本映射任务为例），并且为模型的参数设置初始值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, encoder: Encoder, decoder: Decoder,</span></span><br><span class="line"><span class="params">            src_embedding_layer: InputEmbeddings,</span></span><br><span class="line"><span class="params">            target_embedding_layer: InputEmbeddings,</span></span><br><span class="line"><span class="params">            src_pos_encoding_layer: PositionalEncoding,</span></span><br><span class="line"><span class="params">            target_pos_encoding_layer: PositionalEncoding,</span></span><br><span class="line"><span class="params">            proj_layer: ProjectionLayer</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.src_embed = src_embedding_layer</span><br><span class="line">        self.target_embed = target_embedding_layer</span><br><span class="line">        self.src_pos_encod = src_pos_encoding_layer</span><br><span class="line">        self.target_pos_encod = target_pos_encoding_layer</span><br><span class="line">        self.proj = proj_layer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        src = self.src_embed(src)</span><br><span class="line">        src = self.src_pos_encod(src)</span><br><span class="line">        <span class="keyword">return</span> self.encoder(src, src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, target, target_mask</span>):</span><br><span class="line">        target = self.target_embed(target)</span><br><span class="line">        target = self.target_pos_encod(target)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(target, target_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">project</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.proj(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_transformer</span>(<span class="params"></span></span><br><span class="line"><span class="params">        src_vocab_size: <span class="built_in">int</span>, target_vocab_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        input_seq_len: <span class="built_in">int</span>, output_seq_len: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        d_model: <span class="built_in">int</span> = <span class="number">512</span>,</span></span><br><span class="line"><span class="params">        n: <span class="built_in">int</span> = <span class="number">6</span>,</span></span><br><span class="line"><span class="params">        h: <span class="built_in">int</span> = <span class="number">8</span>,</span></span><br><span class="line"><span class="params">        dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        d_ff: <span class="built_in">int</span> = <span class="number">2048</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Construct a Transformer model from scratch.</span></span><br><span class="line"><span class="string">    :param src_vocab_size: size of vocabulary for the source langauge</span></span><br><span class="line"><span class="string">    :param target_vocab_size: size of vocabulary for the target langauge</span></span><br><span class="line"><span class="string">    :param input_seq_len: the approximate length of the input token sequence</span></span><br><span class="line"><span class="string">    :param output_seq_len: the approximate length of the output token sequence</span></span><br><span class="line"><span class="string">    :param d_model: the dimension of the embedding vectors (feature dimension)</span></span><br><span class="line"><span class="string">    :param n: the number of EncoderBlock/DecoderBlock in Encoder/Decoder</span></span><br><span class="line"><span class="string">    :param h: the number of the heads in MHA</span></span><br><span class="line"><span class="string">    :param dropout: dropout rate for all the dropout networks in the model</span></span><br><span class="line"><span class="string">    :param d_ff: the size for the hidden layer in all the FFNs</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    src_embed = InputEmbeddings(d_model, src_vocab_size)</span><br><span class="line">    target_embed = InputEmbeddings(d_model, target_vocab_size)</span><br><span class="line"></span><br><span class="line">    src_pos_enc = PositionalEncoding(d_model, input_seq_len, dropout)</span><br><span class="line">    target_pos_enc = PositionalEncoding(d_model, output_seq_len, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note: we cannot use list expression because</span></span><br><span class="line">    <span class="comment"># each EncoderBlock/DecoderBlock has different parameters</span></span><br><span class="line">    encoder_blocks = []</span><br><span class="line">    decoder_blocks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        encoder_mha = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line">        decoder_masked_mha = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line">        cross_mha = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line"></span><br><span class="line">        encoder_ffn = FFNBlock(d_model, d_ff, dropout)</span><br><span class="line">        decoder_ffn = FFNBlock(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line">        encoder_blocks.append(EncoderBlock(encoder_mha, encoder_ffn, dropout))</span><br><span class="line">        decoder_blocks.append(DecoderBlock(decoder_masked_mha, cross_mha, decoder_ffn, dropout))</span><br><span class="line"></span><br><span class="line">    encoder = Encoder(nn.ModuleList(encoder_blocks))</span><br><span class="line">    decoder = Decoder(nn.ModuleList(decoder_blocks))</span><br><span class="line"></span><br><span class="line">    proj_layer = ProjectionLayer(d_model, target_vocab_size)</span><br><span class="line"></span><br><span class="line">    model = Transformer(encoder, decoder, src_embed, target_embed, src_pos_enc, target_pos_enc, proj_layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 为模型参数设置服从标准分布的值</span></span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.sjtuxhw.top">SSRVodka</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.sjtuxhw.top/technical/transformer/">https://blog.sjtuxhw.top/technical/transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.sjtuxhw.top" target="_blank">SSRVodka's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/ML/">ML</a><a class="post-meta__tags" href="/tags/Paper/">Paper</a></div><div class="post-share"><div class="social-share" data-image="https://cdn.sjtuxhw.top/cover_imgs/transformer.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat_pay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/wechat_pay.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/technical/sc-paper/" title="阅读: A Hardware-Software Co-Design for Efficient Secure Containers"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/sc.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">阅读: A Hardware-Software Co-Design for Efficient Secure Containers</div></div><div class="info-2"><div class="info-item-1">这是一篇 2025 年的关于软硬协同的安全容器设计的文章。 0. Overview虚拟机级别的容器中，每个容器运行在虚拟机虚拟出的独立内核上，因此隔离性很强。但其依赖于通用虚拟机虚拟出的虚拟化硬件，与 OS 级别的容器相比，会导致不可忽略的性能开销。而在嵌套虚拟化场景下，secure container 运行在虚拟机中，这个性能的 gap 会显著地扩大。 本篇文章基于两个角度提出容器内核隔离（CKI），一个软硬协调的高效机密容器设计。  首先，Protection Keys for Supervisor（PKS）可以帮助我们构建一个新的权限级别，用于在 Host Kernel 中安全地配置多个容器内核，而不涉及 non-root ring 0（Intel 中的 Guest Kernel 所处级别）； 其次，secure container 使用的通用虚拟化技术提供很多容器实际隔离并不需要的特性，例如二阶段页表翻译，这引入了可以避免的性能开销；  因此容器内核隔离技术在跑容器内核时：  避免使用虚拟化硬件，并移除不必要的虚拟化技术（像二阶段地址翻译）。它使用 PKS...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/technical/ml-roadmap/" title="知识图谱：Machine Learning Roadmap"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/ml-roadmap.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-08</div><div class="info-item-2">知识图谱：Machine Learning Roadmap</div></div><div class="info-2"><div class="info-item-1">笔者感觉 ML 这块知识点太多，互联网上多数信息都难以结构化，尤其是一个方向的知识火起来后，每个人都写一篇博客，看的眼花缭乱。。因此笔者简单总结了一下机器学习领域的知识图谱，方便知识体系构建和回顾。 如有错误，欢迎读者勘误斧正。  </div></div></div></a><a class="pagination-related" href="/technical/embodied-3-papers-202503/" title="具身智能论文速读3篇 2025年3月"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/embodied-3-202503.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-02</div><div class="info-item-2">具身智能论文速读3篇 2025年3月</div></div><div class="info-2"><div class="info-item-1">HumanUP: Learning Getting-Up Policies for Real-World Humanoid Robots主旨：如何通过强化学习（RL）和仿真到现实（Sim-to-Real）的方法，为人形机器人开发能够从不同跌倒姿势和不同地形中自主起身的控制策略； 背景：人形机器人在实际应用中容易跌倒，而手动设计控制器来处理各种跌倒姿势和复杂地形非常困难。现有的控制器通常只能处理有限的跌倒情况，缺乏泛化能力。因此，论文提出了一种基于学习的框架，通过仿真训练生成能够在真实世界中应对多种跌倒姿势和地形的起身策略。目前的挑战有：  非周期性行为：起身任务不像行走那样有固定的周期性接触模式，接触序列需要动态调整。 丰富的接触：起身过程中，机器人不仅依靠脚部接触地面，还可能利用身体其他部位（如手臂、躯干）来施加力。 稀疏奖励：起身任务的奖励信号较为稀疏，机器人需要在长时间内做出正确的动作才能获得奖励。   论文的解决方案 HumanUP：  第一阶段（Stage...</div></div></div></a><a class="pagination-related" href="/technical/pytorch-dim/" title="如何理解 PyTorch 函数的 dim 参数"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/pth_dim.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-18</div><div class="info-item-2">如何理解 PyTorch 函数的 dim 参数</div></div><div class="info-2"><div class="info-item-1">之前很长的一段时间内，我都不太清楚如何感性地理解 PyTorch 中的 dim 参数。最近琢磨到了一个还算比较好理解的方法，故简单记录在这里。 dim 在 PyTorch 的很多函数中都可以指定，例如 sum / mode / unsqueeze / topk 等等，主要是告诉函数应该针对张量的哪个特定维度操作。 这在输入张量维度很高的时候就不那么直观了。虽说不理解问题不大，最多手写循环就能达到目的。但如果我们想尽量避免使用 python 的显式循环，或者还想要利用广播机制来更快的完成计算任务，就不得不总结一下了。  聚合类函数（减小维度数的运算，reduction operations），例如 sum / mean / max / min / mode / topk 等等；  dim 通常的语义是 “沿这个维度进行消除”，如果有指定 keepdim=True，则这个维度 size 压缩为 1；  dim 的值就对应张量 shape 的索引；  被操作的每个元素的 shape 就是 原张量的 shape 在 dim 索引之后组成的新的 shape，即...</div></div></div></a><a class="pagination-related" href="/technical/algo-in-ai/" title="Algorithms in AI"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/algo-in-ai.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-20</div><div class="info-item-2">Algorithms in AI</div></div><div class="info-2"><div class="info-item-1">Chapter 0. Intro0.1 The definition of Artificial Intelligence Think rationally -&gt; Think like people -&gt; Act like people -&gt; Act rationally.  The system maximumly achieving predefined goals. -&gt; Maximize the expected utility. (最大化预期效用)   Brains and AI  Why not reverse engineering the brains? -&gt; Not as modular as software.  But there are the lessons learned from the brain (interleave, 交织在一起):  Memory (data): Judge situations depending on the previous experiences...</div></div></div></a><a class="pagination-related" href="/technical/hilog-paper/" title="OpenHarmony Hilog 架构趣读"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/hilog-paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-29</div><div class="info-item-2">OpenHarmony Hilog 架构趣读</div></div><div class="info-2"><div class="info-item-1">最近看到一篇讨论 OpenHarmony Hilog 日志子系统的设计的论文，遂进行了一番阅读。该论文发表在软件学报上。 摘要 分析当今主流日志系统的技术架构和优缺点； 基于 OpenHarmony 操作系统的异构设备互联特性，设计 HiLog 日志系统模型规范； 设计并实现第 1 个面向 OpenHarmony 的日志系统 HiLog, 并贡献到 OpenHarmony 主线； 对 HiLog 日志系统的关键指标进行测试和对比试验；  实现的 HiLog 具有以下特征：  基础性能：日志写入阶段吞吐量分别为 1 500 KB/s 和 700 KB/s，吞吐量相对 Android Log 提升 114%； 日志持久化：压缩率 3.5%，丢包率...</div></div></div></a><a class="pagination-related" href="/technical/xpc-paper/" title="论文阅读 - XPC: Architectural Support for Secure and Efficient Cross Process Call"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/xpc-paper.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-28</div><div class="info-item-2">论文阅读 - XPC: Architectural Support for Secure and Efficient Cross Process Call</div></div><div class="info-2"><div class="info-item-1">这是一篇 2019 年的关于微内核 IPC 性能优化的文章。 摘要微内核有很多引人注目的 features，例如 安全性、容错性、模块化，以及可定制性，这些特性近期在学术界和工业界又再次掀起了一股研究热潮（including seL4, QNX and Google’s Fuchsia OS）。  Google’s Fuchsia’s kernel (called Zircon)  但是 IPC（进程间通信）作为微内核的 阿喀琉斯之踵，仍然是导致微内核 OS 总体性能较低的主要因素之一。同时 IPC 在宏内核中也扮演者很重要的角色，例如 Android Linux，其中的移动端程序会经常和用户态服务通过 IPC 通信。所以优化 IPC 自然是一个很重要的课题。 之前学界对 IPC 在软件层面的优化都绕不开 Kernel，因为 IPC 在这方面的主要开销就是 域切换（domain switch）和消息复制/重映射（message copying/remapping）；在硬件层面的优化方法主要是 给内存和能力打...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="xhw-card-content"><div class="xhw-avatar-group"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.ico" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="xhw-sticker"><img class="sticker-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/smile.avif" alt="emoji-sticker"/></div></div></div><div class="author-info-name">SSRVodka</div><div class="author-info-description">A blog to document learning and life</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">78</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SSRVodka" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:sjtuxhw12345@sjtu.edu.cn" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.sjtuxhw.top/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span><a class="service-status-badge" id="serviceStatusBadge" href="https://status.sjtuxhw.top" rel="external nofollow noreferrer" target="_blank"><span class="status-loading"></span><span class="status-text">loading...</span></a></div><div class="announcement_content">Thanks for visiting! |•'-'•) ✧</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-text">前置知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-text">模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder-%E5%92%8C-Decoder-%E8%AE%BE%E8%AE%A1"><span class="toc-text">Encoder 和 Decoder 设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention-%E8%AE%BE%E8%AE%A1%E4%BB%A5%E5%8F%8A%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-text">Attention 设计以及创新点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-1%EF%BC%9A%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B-Scaled-Dot-Product-Attention"><span class="toc-text">创新点 1：缩放点积注意力 (Scaled Dot-Product Attention)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-2%EF%BC%9A%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">创新点 2：多头注意力机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-3%EF%BC%9A%E9%80%90%E4%BD%8D%E7%9A%84%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">创新点 3：逐位的前馈神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-4-Embeddings-and-Softmax-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">创新点 4:   Embeddings and Softmax 的使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-5%EF%BC%9A%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="toc-text">创新点 5：位置编码 Positional Encoding</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="toc-text">训练方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%88%E6%9E%9C%E7%AE%80%E8%BF%B0"><span class="toc-text">效果简述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E5%A2%83%E7%94%9F%E8%AF%8D"><span class="toc-text">语境生词</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E5%8F%8A%E8%AF%A6%E8%A7%A3"><span class="toc-text">代码实现及详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Input-Encoder-Layer"><span class="toc-text">Input Encoder Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%B1%82%EF%BC%88%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84-Add-amp-Norm-%E7%9A%84-%E2%80%9CNorm%E2%80%9D%EF%BC%89"><span class="toc-text">正则化层（架构中的 Add &amp; Norm 的 “Norm”）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%90%E4%BD%8D%E7%9A%84%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">逐位的全连接前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%9D%97%E3%80%81%E6%8E%A9%E7%A0%81%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%9D%97%E3%80%81Encoder-Decoder-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%9D%97"><span class="toc-text">多头自注意力块、掩码多头自注意力块、Encoder-Decoder 注意力块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%9D%97%EF%BC%88Add-amp-Norm-%E4%B8%AD%E7%9A%84-%E2%80%9CAdd%E2%80%9D%EF%BC%89"><span class="toc-text">残差连接块（Add &amp; Norm 中的 “Add”）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder-amp-Decoder-Block"><span class="toc-text">Encoder &amp; Decoder Block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Projection-Layer-Linear"><span class="toc-text">Projection Layer (Linear)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Put-It-Together"><span class="toc-text">Put It Together</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/technical/transformer/" title="Transformer 论文精读 + 代码实现"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/transformer.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 论文精读 + 代码实现"/></a><div class="content"><a class="title" href="/technical/transformer/" title="Transformer 论文精读 + 代码实现">Transformer 论文精读 + 代码实现</a><time datetime="2025-07-20T15:15:10.000Z" title="发表于 2025-07-20 23:15:10">2025-07-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/technical/sc-paper/" title="阅读: A Hardware-Software Co-Design for Efficient Secure Containers"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/sc.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="阅读: A Hardware-Software Co-Design for Efficient Secure Containers"/></a><div class="content"><a class="title" href="/technical/sc-paper/" title="阅读: A Hardware-Software Co-Design for Efficient Secure Containers">阅读: A Hardware-Software Co-Design for Efficient Secure Containers</a><time datetime="2025-07-01T10:14:11.000Z" title="发表于 2025-07-01 18:14:11">2025-07-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/technical/ml-roadmap/" title="知识图谱：Machine Learning Roadmap"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/ml-roadmap.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="知识图谱：Machine Learning Roadmap"/></a><div class="content"><a class="title" href="/technical/ml-roadmap/" title="知识图谱：Machine Learning Roadmap">知识图谱：Machine Learning Roadmap</a><time datetime="2025-06-08T06:59:31.000Z" title="发表于 2025-06-08 14:59:31">2025-06-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/review/os-el-and-mem/" title="OS 的特权级切换与内存管理总览：以 Linux AArch64 为例"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/os-sum.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="OS 的特权级切换与内存管理总览：以 Linux AArch64 为例"/></a><div class="content"><a class="title" href="/review/os-el-and-mem/" title="OS 的特权级切换与内存管理总览：以 Linux AArch64 为例">OS 的特权级切换与内存管理总览：以 Linux AArch64 为例</a><time datetime="2025-05-30T08:49:12.000Z" title="发表于 2025-05-30 16:49:12">2025-05-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/technical/tee-lab/" title="机密计算与TEE：知识整理和试验笔记"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.sjtuxhw.top/cover_imgs/tee-lab.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机密计算与TEE：知识整理和试验笔记"/></a><div class="content"><a class="title" href="/technical/tee-lab/" title="机密计算与TEE：知识整理和试验笔记">机密计算与TEE：知识整理和试验笔记</a><time datetime="2025-04-17T15:31:36.000Z" title="发表于 2025-04-17 23:31:36">2025-04-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By SSRVodka  |  tech SJTU saves the world</div><div class="framework-info"><span>Built With love &amp; </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme By </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><div style="display:flex;flex-flow:row;justify-content:center;align-items:center;"> <a href="https://beian.miit.gov.cn" rel="external nofollow noreferrer" id="beian" target="_blank">ICP备案：沪ICP备2023012264-1号</a> <span class="footer-separator">|</span> <a style="display:flex;flex-flow:row;align-items:center;" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral" rel="external nofollow noreferrer" target="_blank"> 本网站由 <img style="margin:0 3px;" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/upCloud_logo_blue.png" title="upcloud" alt="upcloud" height="30px"> 提供CDN加速/云存储服务 </a></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();" rel="external nofollow noreferrer"><i class="fa-solid fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();" rel="external nofollow noreferrer"><i class="fa-solid fa-arrow-rotate-right"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();" rel="external nofollow noreferrer"><i class="fa-solid fa-arrow-right"></i></a><a class="rightMenu-item" id="menu-radompage" href="javascript:window.location.href = window.location.origin;" rel="external nofollow noreferrer"><i class="fa-solid fa-house"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();" rel="external nofollow noreferrer"><i class="fa-solid fa-copy"></i><span>复制</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-image"><a class="rightMenu-item" href="javascript:rmf.copyImageUrl();" rel="external nofollow noreferrer"><i class="fa-solid fa-link"></i><span>复制图片地址</span></a><a class="rightMenu-item" href="javascript:rmf.downloadImage();" rel="external nofollow noreferrer"><i class="fa-solid fa-download"></i><span>保存图片</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-link"><a class="rightMenu-item" href="javascript:rmf.copyLink();" rel="external nofollow noreferrer"><i class="fa-solid fa-link"></i><span>复制链接地址</span></a><a class="rightMenu-item" href="javascript:rmf.openLinkNewTab();" rel="external nofollow noreferrer"><i class="fa-solid fa-external-link-alt"></i><span>在新标签页打开</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:rmf.switchDarkMode();" rel="external nofollow noreferrer"><i class="fa-solid fa-circle-half-stroke"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="javascript:rmf.switchReadMode();" rel="external nofollow noreferrer"><i class="fa-solid fa-book"></i><span>阅读模式</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  let initFn = window.walineFn || null
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo
  const option = {"emoji":["https://unpkg.com/@waline/emojis@1.2.0/tw-emoji","https://unpkg.com/@waline/emojis@1.2.0/tieba"],"locale":{"reactionTitle":"你认为这篇文章怎么样？","placeholder":"给大佬递笔 XP\n[ Akismet AI Filter 🤖 ON ]"},"reaction":["https://unpkg.com/@waline/emojis@1.2.0/qq/qq_thumbsup.gif","https://unpkg.com/@waline/emojis@1.2.0/qq/qq_thumbsdown.gif","https://unpkg.com/@waline/emojis@1.2.0/qq/qq_antic.gif","https://unpkg.com/@waline/emojis@1.2.0/qq/qq_cool.gif","https://unpkg.com/@waline/emojis@1.2.0/qq/qq_sleepy.gif","https://unpkg.com/@waline/emojis@1.2.0/qq/qq_emm.gif"]}

  const destroyWaline = ele => ele.destroy()

  const initWaline = (Fn, el = document, path = window.location.pathname) => {
    const waline = Fn({
      el: el.querySelector('#waline-wrap'),
      serverURL: 'https://waline.sjtuxhw.top/',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      comment: true,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    if (isShuoshuo) {
      window.shuoshuoComment.destroyWaline = () => {
        destroyWaline(waline)
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const loadWaline = (el, path) => {
    if (initFn) initWaline(initFn, el, path)
    else {
      btf.getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css')
        .then(() => import('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js'))
        .then(({ init }) => {
          initFn = init || Waline.init
          initWaline(initFn, el, path)
          window.walineFn = initFn
        })
    }
  }

  if (isShuoshuo) {
    'Waline' === 'Waline'
      ? window.shuoshuoComment = { loadComment: loadWaline } 
      : window.loadOtherComment = loadWaline
    return
  }

  if ('Waline' === 'Waline' || !false) {
    if (false) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script></div><script src="/js/rightmenu.js"></script><script src="/js/mourn.js"></script><script src="/js/status_badge.js"></script><script defer src="/js/console_welcome.js"></script><script async data-pjax src="/js/bsz.build-20250729.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>